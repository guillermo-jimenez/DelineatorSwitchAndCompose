# -*- coding: utf-8 -*-

import numpy as np
import numpy.random
import keras
import csv
import keras.layers
from utils.disambiguator import select_kernel_initializer
from utils.modules import StemModule
from utils.modules import AtrousMiddleModule
from utils.modules import LevelModule
from utils.modules import PoolingModule
from utils.modules import OutputModule

class FlatNet():
    '''Model generator'''

    def __init__(self, config):
        '''Initialization'''
        
        assert config.m_name in ('vanilla', 'residual', 'xception')

        # Parameters
        self.sig_shape      = (config.window,config.in_ch)
        self.m_name         = config.m_name
        self.m_repetitions  = config.m_repetitions
        self.ms_upsampling  = config.ms_upsampling
        self.hyperdense     = config.hyperdense
        self.atrous_conv    = config.atrous_conv
        self.maxpool        = config.maxpool
        self.out_ch         = config.out_ch
        self.start_ch       = config.start_ch
        self.dropout_rate   = config.dropout_rate
        self.kernel_size    = config.kernel_size
        self.depth          = config.depth
        self.inc_rate       = config.inc_rate
        self.kernel_init    = select_kernel_initializer(config.kernel_init)

        # Declutter code -> essentially the same information in every single function call
        self.constant_info  = (self.kernel_size, self.dropout_rate, self.kernel_init)

    def create_model(self):
        # Define network's input
        inp = keras.Input(shape=self.sig_shape)

        ######################### Stem ##########################
        # Stem before U-Net
        stem = StemModule(self.m_name)(inp, int(self.start_ch), self.kernel_size, kernel_init=self.kernel_init)

        # Convolutional modules:
        encoder_module   = [[] for i in range(self.depth-1)]
        embedding_module = []
        decoder_module   = [[] for i in range(self.depth-1)]

        # Concatenations of previous levels (if hyperdense):
        concat           = [[] for i in range(self.depth)]

        # Transitions up and down (if hyperdense):
        encoder_transit  = [[] for i in range(self.depth-1)]
        embedding_transit= []
        decoder_transit  = [[] for i in range(self.depth-1)]


        ###################### Entry Flow #######################
        # Introduce the "stem" variable as input for the first element of the encoder
        encoder_module[0].append(stem)

        # Case the architecture is hyperdense
        if self.hyperdense:
            # For every level in the U-NET
            for i in range(len(encoder_module)):
                # Downsample the resulting convolution of all previous levels for vertical propagation
                # encoder_transit[i] += [MaxAvgPool1d(pool_size=2**(i-j), padding='same')(encoder_module[j][-1]) for j in range(0,i-1)]
                encoder_transit[i] += [PoolingModule(self.m_name)(encoder_module[j][-1], int(self.inc_rate**(i-j))*self.start_ch, self.kernel_size, self.maxpool, strides=(int(self.inc_rate**(i-j)),), kernel_init=self.kernel_init) for j in range(0,i-1)]

                # Add the contributions of the transition modules to the concatenation list
                concat[i] += encoder_transit[i]

                # Repetitions of the entry modules
                for j in range(self.m_repetitions):
                    concat[i] += [encoder_module[i][-1]]
                    try:
                        if len(concat[i]) != 1: # In case we're in the middle of the dense block
                            encoder_module[i].append(LevelModule(self.m_name)(keras.layers.Concatenate()(concat[i]), int(self.inc_rate**i)*self.start_ch, self.kernel_size, kernel_init=self.kernel_init))
                        else: # In case there's only one element
                            encoder_module[i].append(LevelModule(self.m_name)(concat[i][-1], int(self.inc_rate**i)*self.start_ch, self.kernel_size, kernel_init=self.kernel_init))
                    except:
                        raise IOError("")

                concat[i] += [encoder_module[i][-1]]

                # Pooling Module
                if i != len(encoder_module)-1:
                    # encoder_module[i+1].append(MaxAvgPool1d()(encoder_module[i][-1]))
                    encoder_module[i+1].append(PoolingModule(self.m_name)(encoder_module[i][-1], int(self.inc_rate**(i+1))*self.start_ch, self.kernel_size, self.maxpool, kernel_init=self.kernel_init))


            ###################### Middle Flow #######################
            # Pooling module to the embedding
            embedding_module.append(PoolingModule(self.m_name)(encoder_module[-1][-1], int(self.inc_rate**(i+1))*self.start_ch, self.kernel_size, self.maxpool, kernel_init=self.kernel_init))
            
            # Downsample the resulting convolution of all previous levels for vertical propagation
            embedding_transit += [PoolingModule(self.m_name)(encoder_module[j][-1], int(self.inc_rate**(self.depth-j-1))*self.start_ch, self.kernel_size, self.maxpool, strides=(int(self.inc_rate**(self.depth-j-1)),), kernel_init=self.kernel_init) for j in range(0,self.depth-2)]
            
            # Add the contributions of the transition modules to the concatenation list
            concat[-1] += embedding_transit

            # Middle Flow
            for j in range(self.m_repetitions):
                concat[-1] += [embedding_module[-1]]

                if (j == self.m_repetitions - 1) and (self.atrous_conv): # Apply atrous convolution
                    if len(concat[-1]) != 1: # If number of repetitions is higher than one
                        embedding_module.append(AtrousMiddleModule(self.m_name)(keras.layers.Concatenate()(concat[-1]), int(self.inc_rate**(self.depth-1))*self.start_ch, self.kernel_size, kernel_init=self.kernel_init))
                    else: # In case there's only one element
                        embedding_module.append(AtrousMiddleModule(self.m_name)(embedding_module[-1], int(self.inc_rate**(self.depth-1))*self.start_ch, self.kernel_size, kernel_init=self.kernel_init))
                    concat[-1].append(embedding_module[-1])
                else:
                    if len(concat[-1]) != 1: # If number of repetitions is higher than one
                        embedding_module.append(LevelModule(self.m_name)(keras.layers.Concatenate()(concat[-1]), int(self.inc_rate**(self.depth-1))*self.start_ch, self.kernel_size, kernel_init=self.kernel_init))
                    else: # In case there's only one element
                        embedding_module.append(LevelModule(self.m_name)(concat[-1][-1], int(self.inc_rate**(self.depth-1))*self.start_ch, self.kernel_size, kernel_init=self.kernel_init))



            ####################### Exit Flow #######################
            # The input of the decoder path is the last layer of the encoder
            decoder_module[-1].append(keras.layers.UpSampling1D()(embedding_module[-1]))

            # Iterate over decoder levels in a backwards manner
            for i in range(len(decoder_module))[::-1]:
                # Upsample the resulting convolution of the embedded level for vertical propagation:
                if ((self.depth-1)-i) > 1: decoder_transit[i] += [keras.layers.UpSampling1D(size=2**((self.depth-1)-i))(embedding_module[-1])]

                # Upsample the resulting convolution of all previous decoder levels for vertical propagation:
                decoder_transit[i] += [keras.layers.UpSampling1D(size=2**(j-i))(decoder_module[j][-1]) for j in range(self.depth-2,i+1,-1)]

                # Accumulate skipped connection of U-Net (direct link from last element of encoder module)
                concat[i] += [encoder_module[i][-1]]

                # Accumulate the contributions of the upwards transition modules to the concatenation list
                concat[i] += decoder_transit[i]

                # Repetitions of the exit modules
                for j in range(self.m_repetitions):
                    # Propagation of previous levels
                    concat[i] += [decoder_module[i][-1]]

                    # Execution of exit module
                    if len(concat[i]) != 1: # In case we're in the middle of the dense block
                        decoder_module[i].append(LevelModule(self.m_name)(keras.layers.Concatenate()(concat[i]), int(self.inc_rate**i)*self.start_ch, self.kernel_size, kernel_init=self.kernel_init))
                    else: # In case there's only one element
                        decoder_module[i].append(LevelModule(self.m_name)(concat[i][-1], int(self.inc_rate**i)*self.start_ch, self.kernel_size, kernel_init=self.kernel_init))

                # Upsampling Layer
                if i != 0:
                    decoder_module[i-1].append(keras.layers.UpSampling1D()(decoder_module[i][-1]))
                                               
            concat[0] += [decoder_module[0][-1]]

            # Perform last convolution
            out = OutputModule(self.m_name)(keras.layers.Concatenate()(concat[0]), int(self.out_ch), 1, kernel_init=self.kernel_init)

        else: # If not hyperdense
            # Entry flow/encoder
            for i in range(len(encoder_module)):
                # Repetitions of the entry modules
                for j in range(self.m_repetitions):
                    encoder_module[i].append(LevelModule(self.m_name)(encoder_module[i][-1], int(self.inc_rate**i)*self.start_ch, self.kernel_size, kernel_init=self.kernel_init))
                    
                # Pooling Module
                if i != len(encoder_module)-1:
                    # encoder_module[i+1].append(MaxAvgPool1d()(encoder_module[i][-1]))
                    encoder_module[i+1].append(PoolingModule(self.m_name)(encoder_module[i][-1], int(self.inc_rate**(i+1))*self.start_ch, self.kernel_size, self.maxpool, kernel_init=self.kernel_init))

            # embedding_module.append(MaxAvgPool1d()(encoder_module[-1][-1]))
            embedding_module.append(PoolingModule(self.m_name)(encoder_module[-1][-1], int(self.inc_rate**(i+1))*self.start_ch, self.kernel_size, self.maxpool, kernel_init=self.kernel_init))

            # Middle Flow
            for j in range(self.m_repetitions):
                if (j == self.m_repetitions - 1) and (self.atrous_conv):
                    embedding_module.append(AtrousMiddleModule(self.m_name)(embedding_module[-1], int(self.inc_rate**(self.depth-1))*self.start_ch, self.kernel_size, kernel_init=self.kernel_init))
                else:
                    embedding_module.append(      LevelModule(self.m_name)(embedding_module[-1], int(self.inc_rate**(self.depth-1))*self.start_ch, self.kernel_size, kernel_init=self.kernel_init))

            ####################### Exit Flow #######################
            # The input of the decoder path is the last layer of the encoder
            decoder_module[-1].append(keras.layers.UpSampling1D()(embedding_module[-1]))
                                      
            # Iterate over decoder levels in a backwards manner
            for i in range(len(decoder_module))[::-1]:
                # Concatenate skipped connection
                decoder_module[i].append(keras.layers.Concatenate()([encoder_module[i][-1],decoder_module[i][-1]]))

                ################# Multiscale Upsampling #################
                if (i == 0) and (self.ms_upsampling):
                    msUp  = [decoder_module[i][-1]] # Include last upsampling from previus iteration

                    # Scale 0 does not upsample, as there is nothing to upsample. Scale 1 has been included in the last upsampling layer of the exit flow
                    for j in range(2,self.depth-1):
                        msUp += [keras.layers.UpSampling1D(size=2**j)(decoder_module[j][-1])]
                                 

                    decoder_module[i].append(keras.layers.Concatenate()(msUp))
                
                # Repetitions of the entry modules
                for j in range(self.m_repetitions):
                    # Propagation of previous levels
                    decoder_module[i].append(LevelModule(self.m_name)(decoder_module[i][-1], int(self.inc_rate**i)*self.start_ch, self.kernel_size, kernel_init=self.kernel_init))
                    
                # Upsampling Layer
                if i != 0:
                    decoder_module[i-1].append(keras.layers.UpSampling1D()(decoder_module[i][-1]))

            out = OutputModule(self.m_name)(decoder_module[0][-1], int(self.out_ch), 1, kernel_init=self.kernel_init)

        return keras.Model(inputs=inp, outputs=out)


    def Export(self, path):
        with open(path,'wb') as f:
            printDict                                               = dict()
            printDict['Input_SignalShape:                       ']  = self.sig_shape
            printDict['Architecture_IsHyperDense:               ']  = self.hyperdense
            printDict['Architecture_Depth:                      ']  = self.depth
            printDict['Architecture_OutputChannels:             ']  = self.out_ch
            printDict['Architecture_StartingChannels:           ']  = self.start_ch
            printDict['Architecture_ChannelIncrementRate:       ']  = self.inc_rate
            printDict['Architecture_HasMaxPool:                 ']  = self.maxpool
            printDict['Architecture_KernelInitializer:          ']  = self.kernel_init
            printDict['Module_Name:                             ']  = self.m_name
            printDict['Module_Repetitions:                      ']  = self.m_repetitions
            printDict['Module_KernelSize:                       ']  = self.kernel_size

            w = csv.writer(f)
            w.writerows(printDict.items())



