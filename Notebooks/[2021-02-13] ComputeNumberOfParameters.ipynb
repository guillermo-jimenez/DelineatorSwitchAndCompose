{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import skimage\n",
    "import skimage.segmentation\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import math\n",
    "import shutil\n",
    "import pathlib\n",
    "import glob\n",
    "import shutil\n",
    "import uuid\n",
    "import random\n",
    "import platform\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import wfdb\n",
    "import json\n",
    "import tqdm\n",
    "import dill\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats\n",
    "import cv2\n",
    "\n",
    "import src.data\n",
    "import src.metrics\n",
    "import sak\n",
    "import sak.signal.wavelet\n",
    "import sak.data\n",
    "import sak.data.augmentation\n",
    "import sak.data.preprocessing\n",
    "import sak.visualization\n",
    "import sak.visualization.signal\n",
    "import sak.torch\n",
    "import sak.torch.nn\n",
    "import sak.torch.nn as nn\n",
    "import sak.torch.nn\n",
    "import sak.torch.train\n",
    "import sak.torch.data\n",
    "import sak.torch.models\n",
    "import sak.torch.models.lego\n",
    "import sak.torch.models.variational\n",
    "import sak.torch.models.classification\n",
    "\n",
    "from sak.signal import StandardHeader\n",
    "\n",
    "def predict_mask(signal, N, stride, model, thr_dice, ptg_voting = 0.25, batch_size = 16):\n",
    "    # Data structure for computing the segmentation\n",
    "    windowed_signal = skimage.util.view_as_windows(signal,(window_size,1),(stride,1))\n",
    "\n",
    "    # Flat batch shape\n",
    "    new_shape = (windowed_signal.shape[0]*windowed_signal.shape[1],*windowed_signal.shape[2:])\n",
    "    windowed_signal = np.reshape(windowed_signal,new_shape)\n",
    "\n",
    "    # Exchange channel position\n",
    "    windowed_signal = np.swapaxes(windowed_signal,1,2)\n",
    "\n",
    "    # Output structures\n",
    "    windowed_mask = np.zeros((windowed_signal.shape[0],3,windowed_signal.shape[-1]),dtype=float)\n",
    "\n",
    "    # Compute segmentation for all leads independently\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,windowed_signal.shape[0],batch_size):\n",
    "            inputs = {\"x\": torch.tensor(windowed_signal[i:i+batch_size]).cuda().float()}\n",
    "            windowed_mask[i:i+batch_size] = models[fold].cuda()(inputs)[\"sigmoid\"].cpu().detach().numpy() > thr_dice\n",
    "\n",
    "    # Retrieve mask as 1D\n",
    "    counter = np.zeros((signal.shape[0]), dtype=int)\n",
    "    segmentation = np.zeros((3,signal.shape[0]))\n",
    "\n",
    "    for i in range(0,windowed_mask.shape[0],12):\n",
    "        counter[(i//12)*stride:(i//12)*stride+window_size] += 1\n",
    "        segmentation[:,(i//12)*stride:(i//12)*stride+window_size] += windowed_mask[i:i+12].sum(0)\n",
    "    segmentation = ((segmentation/counter) >= (signal.shape[-1]*ptg_voting))\n",
    "\n",
    "    return segmentation\n",
    "\n",
    "def smooth(x: np.ndarray, window_size: int, conv_mode: str = 'same'):\n",
    "    x = np.pad(np.copy(x),(window_size,window_size),'edge')\n",
    "    window = np.hamming(window_size)/(window_size//2)\n",
    "    x = np.convolve(x, window, mode=conv_mode)\n",
    "    x = x[window_size:-window_size]\n",
    "    return x\n",
    "\n",
    "def get_ground_truth(basedir: str, database: str):\n",
    "    if database == 'ludb':\n",
    "        # Load data\n",
    "        P = sak.load_data(os.path.join(basedir,\"ludb\",\"P.csv\"))\n",
    "        QRS = sak.load_data(os.path.join(basedir,\"ludb\",\"QRS.csv\"))\n",
    "        T = sak.load_data(os.path.join(basedir,\"ludb\",\"T.csv\"))\n",
    "    elif database == 'zhejiang':\n",
    "        # Load data\n",
    "        P = sak.load_data(os.path.join(basedir,\"ZhejiangDB\",\"P.csv\"))\n",
    "        QRS = sak.load_data(os.path.join(basedir,\"ZhejiangDB\",\"QRS.csv\"))\n",
    "        T = sak.load_data(os.path.join(basedir,\"ZhejiangDB\",\"T.csv\"))\n",
    "        \n",
    "    # Divide into onsets/offsets\n",
    "    Pon    = {k: P[k][0::2] for k in P}\n",
    "    Poff   = {k: P[k][1::2] for k in P}\n",
    "    QRSon  = {k: QRS[k][0::2] for k in QRS}\n",
    "    QRSoff = {k: QRS[k][1::2] for k in QRS}\n",
    "    Ton    = {k: T[k][0::2] for k in T}\n",
    "    Toff   = {k: T[k][1::2] for k in T}\n",
    "\n",
    "    # Generate validity\n",
    "    validity = {\n",
    "        k: [\n",
    "            np.min(np.concatenate((P.get(k,[+np.inf]),QRS.get(k,[+np.inf]),T.get(k,[+np.inf])))),\n",
    "            np.max(np.concatenate((P.get(k,[-np.inf]),QRS.get(k,[-np.inf]),T.get(k,[-np.inf])))),\n",
    "        ] for k in QRS\n",
    "    }\n",
    "    return Pon,Poff,QRSon,QRSoff,Ton,Toff,validity\n",
    " \n",
    "def get_file_list(basedir: str, database: str):\n",
    "    if database == 'ludb':\n",
    "        files = glob.glob(os.path.join(basedir,'ludb','*.dat'))\n",
    "    elif database == 'zhejiang':\n",
    "        files = glob.glob(os.path.join(basedir,'ZhejiangDB','RAW','*.csv'))\n",
    "    return files\n",
    " \n",
    "def get_sample(file: str, database: str) -> Tuple[float, np.ndarray]:\n",
    "    if database == 'ludb':\n",
    "        (signal, header) = wfdb.rdsamp(os.path.splitext(file)[0])\n",
    "        fs = header['fs']\n",
    "    elif database == 'zhejiang':\n",
    "        signal = pd.read_csv(file).values\n",
    "        fs = 1000.\n",
    "    return signal, fs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../list_files.txt') as f:\n",
    "    list_files = f.read().splitlines()\n",
    "    \n",
    "basedir=os.path.expanduser('~/DADES/DADES/Delineator/')\n",
    "model_type='model_best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "109it [00:03, 34.98it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,model_name in tqdm.tqdm(enumerate(list_files)):\n",
    "    path = os.path.join(basedir,'TrainedModels',model_name,'fold_1',f'{model_type}.model')\n",
    "    savepath = os.path.join(basedir,'TrainedModels',model_name,'parameters.txt')\n",
    "    if not os.path.isfile(path):\n",
    "        continue\n",
    "    model = torch.load(path,pickle_module=dill)\n",
    "    \n",
    "    # Compute the number of parameters\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    \n",
    "    # Save as file\n",
    "    if not os.path.isfile(savepath):\n",
    "        with open(savepath,\"w\") as f:\n",
    "            f.writelines([str(params)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPC",
   "language": "python",
   "name": "hpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
