{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import skimage\n",
    "import skimage.segmentation\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import math\n",
    "import shutil\n",
    "import pathlib\n",
    "import glob\n",
    "import shutil\n",
    "import uuid\n",
    "import random\n",
    "import platform\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import wfdb\n",
    "import json\n",
    "import tqdm\n",
    "import dill\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import src.data\n",
    "import src.metrics\n",
    "import utils\n",
    "import utils.wavelet\n",
    "import utils.data\n",
    "import utils.data.augmentation\n",
    "import utils.visualization\n",
    "import utils.visualization.plot\n",
    "import utils.torch\n",
    "import utils.torch.nn\n",
    "import utils.torch.nn as nn\n",
    "import utils.torch.loss\n",
    "import utils.torch.train\n",
    "import utils.torch.data\n",
    "import utils.torch.preprocessing\n",
    "import utils.torch.models\n",
    "import utils.torch.models.lego\n",
    "import utils.torch.models.variational\n",
    "import utils.torch.models.classification\n",
    "\n",
    "from utils.signal import StandardHeader\n",
    "\n",
    "def smooth(x: np.ndarray, window_size: int, conv_mode: str = 'same'):\n",
    "    x = np.pad(np.copy(x),(window_size,window_size),'edge')\n",
    "    window = np.hamming(window_size)/(window_size//2)\n",
    "    x = np.convolve(x, window, mode=conv_mode)\n",
    "    x = x[window_size:-window_size]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(mask_1, mask_2):\n",
    "    intersection = (mask_1 * mask_2).sum()\n",
    "    union = mask_1.sum() + mask_2.sum()\n",
    "    return 2.*intersection/(union + np.finfo('double').eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_on  = utils.load_data('/home/guille/Escritorio/Ruben/ONLYQRSFORRUBEN/onsets.csv')\n",
    "gt_off = utils.load_data('/home/guille/Escritorio/Ruben/ONLYQRSFORRUBEN/offsets.csv')\n",
    "pr_on  = utils.load_data('/home/guille/Escritorio/Ruben/ONLYQRSFORRUBEN/onsets.csv')\n",
    "pr_off = utils.load_data('/home/guille/Escritorio/Ruben/ONLYQRSFORRUBEN/offsets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jitter on on/off - differ in mean/std for delineation metrics\n",
    "for k in pr_on:\n",
    "    pr_on[k]  = np.clip(pr_on[k]+np.random.randint(0,2),  a_min=0, a_max=np.max(pr_on[k]))\n",
    "    pr_off[k] = np.clip(pr_off[k]+np.random.randint(0,2), a_min=0, a_max=np.max(pr_off[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark to delete\n",
    "deleted = {}\n",
    "\n",
    "for k in pr_on:\n",
    "    indices = np.arange(pr_on[k].size)\n",
    "    indices = np.random.permutation(indices)\n",
    "    deleted_indices = indices[:np.random.randint(0,indices.size//2)]\n",
    "    deleted[k] = np.sort(deleted_indices)\n",
    "\n",
    "# Delete from prediction (both onset and offset) -> False negatives\n",
    "for k in pr_on:\n",
    "    pr_on[k]  = np.delete(pr_on[k],  deleted[k])\n",
    "    pr_off[k] = np.delete(pr_off[k], deleted[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add random segments -> false positives\n",
    "for k in pr_on:\n",
    "    # Random number of FP\n",
    "    num_FP = np.random.randint(min([5,pr_on[k].size-2]))\n",
    "    \n",
    "    if num_FP < 1: continue\n",
    "    \n",
    "    locations = np.random.permutation(np.arange(1,pr_on[k].size-1))[:num_FP]\n",
    "    \n",
    "    for loc in locations:\n",
    "        lower_bound = pr_off[k][loc-1]\n",
    "        onset  = pr_on[k][loc]\n",
    "        offset = pr_off[k][loc]\n",
    "        upper_bound = pr_on[k][loc+1]\n",
    "        \n",
    "        try:\n",
    "            new_on  = np.random.randint(offset+10,upper_bound-10)\n",
    "            new_off = np.random.randint(new_on+10,upper_bound-10)\n",
    "\n",
    "            pr_on[k]  = np.sort(np.hstack((pr_on[k], [new_on])))\n",
    "            pr_off[k] = np.sort(np.hstack((pr_off[k],[new_off])))\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "def dice_score(input: np.ndarray, target: np.ndarray) -> float:\n",
    "    intersection = (input * target).sum()\n",
    "    union = input.sum() + target.sum()\n",
    "    return 2.*intersection/(union + np.finfo('double').eps)\n",
    "\n",
    "\n",
    "def filter_valid(onset, offset, validity_on = 0, validity_off = np.inf):\n",
    "    validity_on  = np.array( validity_on)[np.newaxis,np.newaxis]\n",
    "    validity_off = np.array(validity_off)[np.newaxis,np.newaxis]\n",
    "\n",
    "    mask_on    = (onset  >= validity_on) & (onset  <= validity_off)\n",
    "    mask_off   = (offset >= validity_on) & (offset <= validity_off)\n",
    "    mask_total = np.any(mask_on & mask_off, axis=0) # beat has to be found in every one\n",
    "\n",
    "    onset = onset[mask_total]\n",
    "    offset = offset[mask_total]\n",
    "\n",
    "    return onset, offset\n",
    "\n",
    "\n",
    "def correspondence(input_onset, input_offset, target_onset, target_offset):\n",
    "    filtA =  ( input_onset <=  target_onset[:,np.newaxis]) & ( target_onset[:,np.newaxis] <= input_offset)\n",
    "    filtB =  ( input_onset <= target_offset[:,np.newaxis]) & (target_offset[:,np.newaxis] <= input_offset)\n",
    "    filtC = ((target_onset <=   input_onset[:,np.newaxis]) & (  input_onset[:,np.newaxis] <= target_offset)).T\n",
    "    filtD = ((target_onset <=  input_offset[:,np.newaxis]) & ( input_offset[:,np.newaxis] <= target_offset)).T\n",
    "\n",
    "    filter = filtA | filtB | filtC | filtD\n",
    "\n",
    "    return filter\n",
    "\n",
    "\n",
    "def interlead_correspondence(input_onsets: List[np.ndarray], input_offsets: List[np.ndarray], \n",
    "                             target_onsets: List[np.ndarray], target_offsets: List[np.ndarray], \n",
    "                             validity_on: int, validity_off: int):\n",
    "    # ##### NOT FINISHED #####\n",
    "    # filtA =  (res_0_on <= res_1_on[:,np.newaxis]) & (res_1_on[:,np.newaxis] <= res_0_of)\n",
    "    # filtB =  (res_0_on <= res_1_of[:,np.newaxis]) & (res_1_of[:,np.newaxis] <= res_0_of)\n",
    "    # filtC = ((res_1_on <= res_0_on[:,np.newaxis]) & (res_0_on[:,np.newaxis] <= res_1_of)).T\n",
    "    # filtD = ((res_1_on <= res_0_of[:,np.newaxis]) & (res_0_of[:,np.newaxis] <= res_1_of)).T\n",
    "    # filter = filtA | filtB | filtC | filtD\n",
    "    # return filter\n",
    "    pass\n",
    "\n",
    "\n",
    "def post_processing(input_onset,input_offset,target_onset,target_offset,validity_on,validity_off):\n",
    "    input_onset,input_offset = filter_valid(input_onset,input_offset,validity_on,validity_off)\n",
    "    target_onset,target_offset = filter_valid(target_onset,target_offset,validity_on,validity_off)\n",
    "    \n",
    "    return input_onset,input_offset,target_onset,target_offset\n",
    "\n",
    "\n",
    "def compute_metrics(input_onset, input_offset, target_onset, target_offset):\n",
    "    # Init output\n",
    "    tp   = 0\n",
    "    fp   = 0\n",
    "    fn   = 0\n",
    "    dice = 0\n",
    "    onset_error  = []\n",
    "    offset_error = []\n",
    "\n",
    "    # Find correspondence between fiducials\n",
    "    filter = correspondence(input_onset, input_offset, target_onset, target_offset)\n",
    "\n",
    "    # Check correspondence of GT beats to detected beats\n",
    "    corr  = dict()\n",
    "    \n",
    "    # Account for already detected beats to calculate false positives\n",
    "    chosen = np.zeros((filter.shape[0],), dtype=bool)\n",
    "    for i,column in enumerate(filter.T):\n",
    "        corr[i] = np.where(column)[0]\n",
    "        chosen = chosen | column\n",
    "        \n",
    "    # Retrieve beats detected that do not correspond to any GT beat (potential false positives)\n",
    "    not_chosen = np.where(np.logical_not(chosen))[0]\n",
    "    \n",
    "    # Compute Dice coefficient\n",
    "    mask_input  = np.zeros((np.max(np.hstack((input_offset,target_offset)))+10,),dtype=bool)\n",
    "    mask_target = np.zeros((np.max(np.hstack((input_offset,target_offset)))+10,),dtype=bool)\n",
    "    for (onset,offset) in zip(input_onset,input_offset):\n",
    "        mask_input[onset:offset] = True\n",
    "    for (onset,offset) in zip(target_onset,target_offset):\n",
    "        mask_target[onset:offset] = True\n",
    "    dice = dice_score(mask_input, mask_target)\n",
    "\n",
    "    # Compute metrics - Fusion strategy of results of both leads, following Martinez et al.\n",
    "    for i in range(filter.shape[1]):\n",
    "        # If any GT beat has a correspondence to any segmented beat, true positive + accounts for on/offset error\n",
    "        if len(corr[i]) != 0:\n",
    "            # Mark beat as true positive\n",
    "            tp += 1\n",
    "            \n",
    "            # Compute the onset-offset errors\n",
    "            onset_error.append(int(target_onset[corr[i]]  - input_onset[i]))\n",
    "            offset_error.append(int(target_offset[corr[i]] - input_offset[i]))\n",
    "            \n",
    "        # If any GT beat has a correspondence to more than one segmented beat, \n",
    "        #     the rest of the pairs have to be false positives (Martinez et al.)\n",
    "        if len(corr[i]) > 1:\n",
    "            fp += len(corr[i]) - 1\n",
    "        \n",
    "        # If any GT beat has no correspondence to any segmented beat, false negative\n",
    "        if len(corr[i]) == 0:\n",
    "            fn += 1\n",
    "            \n",
    "    # False positives will correspond to those existing in the results that do not correspond to any beat in the GT (the not chosen)\n",
    "    fp += len(not_chosen)\n",
    "    \n",
    "    return tp,fp,fn,dice,onset_error,offset_error\n",
    "        \n",
    "\n",
    "def precision(tp: int, fp: int, fn: int) -> float:\n",
    "    return tp/(tp+fp)\n",
    "\n",
    "def recall(tp: int, fp: int, fn: int) -> float:\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def f1_score(tp: int, fp: int, fn: int) -> float:\n",
    "    return tp/(tp+(fp+fn)/2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,\n",
       " 3,\n",
       " 1,\n",
       " 0.8464163822525598,\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(pr_on[k],pr_off[k],gt_on[k],gt_off[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-to-single lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ludb\n",
    "gt = utils.load_data('/media/guille/DADES/DADES/Delineator/ludb/QRS.csv')\n",
    "pr = utils.load_data('/media/guille/DADES/DADES/Delineator/ludb/QRS.csv')\n",
    "gt_on  = {k: gt[k][0::2] for k in gt}\n",
    "gt_off = {k: gt[k][1::2] for k in gt}\n",
    "pr_on  = {k: pr[k][0::2] for k in pr}\n",
    "pr_off = {k: pr[k][1::2] for k in pr}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2388/2388 [00:00<00:00, 16083.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Jitter on on/off - differ in mean/std for delineation metrics\n",
    "for k in tqdm.tqdm(pr_on):\n",
    "    pr_on[k]  = np.clip(pr_on[k]+np.random.randint(0,2),  a_min=0, a_max=np.max(pr_on[k]))\n",
    "    pr_off[k] = np.clip(pr_off[k]+np.random.randint(0,2), a_min=0, a_max=np.max(pr_off[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2388/2388 [00:00<00:00, 80142.73it/s]\n",
      "100%|██████████| 2388/2388 [00:00<00:00, 86416.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Mark to delete\n",
    "deleted = {}\n",
    "\n",
    "for k in tqdm.tqdm(pr_on):\n",
    "    indices = np.arange(pr_on[k].size)\n",
    "    indices = np.random.permutation(indices)\n",
    "    deleted_indices = indices[:np.random.randint(0,indices.size//2)]\n",
    "    deleted[k] = np.sort(deleted_indices)\n",
    "\n",
    "# Delete from prediction (both onset and offset) -> False negatives\n",
    "for k in tqdm.tqdm(pr_on):\n",
    "    pr_on[k]  = np.delete(pr_on[k],  deleted[k])\n",
    "    pr_off[k] = np.delete(pr_off[k], deleted[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2388/2388 [00:00<00:00, 244078.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Add random segments -> false positives\n",
    "for k in tqdm.tqdm(pr_on):\n",
    "    # Random number of FP\n",
    "    num_FP = np.random.randint(min([5,pr_on[k].size-2]))\n",
    "    \n",
    "    if num_FP < 1: continue\n",
    "    \n",
    "    locations = np.random.permutation(np.arange(1,pr_on[k].size-1))[:num_FP]\n",
    "    \n",
    "    for loc in locations:\n",
    "        lower_bound = pr_off[k][loc-1]\n",
    "        onset  = pr_on[k][loc]\n",
    "        offset = pr_off[k][loc]\n",
    "        upper_bound = pr_on[k][loc+1]\n",
    "        \n",
    "        try:\n",
    "            new_on  = np.random.randint(offset+10,upper_bound-10)\n",
    "            new_off = np.random.randint(new_on+10,upper_bound-10)\n",
    "\n",
    "            pr_on[k]  = np.sort(np.hstack((pr_on[k], [new_on])))\n",
    "            pr_off[k] = np.sort(np.hstack((pr_off[k],[new_off])))\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correspondence(input_onsets_A, input_offsets_A, input_onsets_B, input_offsets_B):\n",
    "    filtA =  (input_onsets_A <=  input_onsets_B[:,None]) &  (input_onsets_B[:,None] <= input_offsets_A)\n",
    "    filtB =  (input_onsets_A <= input_offsets_B[:,None]) & (input_offsets_B[:,None] <= input_offsets_A)\n",
    "    filtC = ((input_onsets_B <=  input_onsets_A[:,None]) &  (input_onsets_A[:,None] <= input_offsets_B)).T\n",
    "    filtD = ((input_onsets_B <= input_offsets_A[:,None]) & (input_offsets_A[:,None] <= input_offsets_B)).T\n",
    "\n",
    "    filter = filtA | filtB | filtC | filtD\n",
    "\n",
    "    return filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = '49'\n",
    "key = k.split('###')[0]\n",
    "listk = [k for k in pr_on.keys() if k.startswith('{}###'.format(key))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_on[\"{}###AVL\".format(k)] = np.hstack((pr_on[\"{}###AVL\".format(k)],[2017, 2017+700]))\n",
    "pr_off[\"{}###AVL\".format(k)] = np.hstack((pr_off[\"{}###AVL\".format(k)],[2217, 2217+700]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_on[\"{}###AVL\".format(k)] = pr_on[\"{}###AVL\".format(k)][[0,2,3,4]]\n",
    "pr_off[\"{}###AVL\".format(k)] = pr_off[\"{}###AVL\".format(k)][[0,2,3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [cross_correspondence(pr_on[k1],pr_off[k1],pr_on[k2],pr_off[k2]) for k1 in listk for k2 in listk if k1 != k2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_on  = [ pr_on[k] for k in listk]\n",
    "in_off = [pr_off[k] for k in listk]\n",
    "tg_on  = [ gt_on[k] for k in listk]\n",
    "tg_off = [gt_off[k] for k in listk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters,filters_corr,chosen,corr = src.metrics.compute_multilead_metrics(in_on,in_off,tg_on,tg_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "falsepositive,filters_corr,not_chosen = src.metrics.compute_multilead_metrics(in_on,in_off,tg_on,tg_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falsepositive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([], dtype=int64),\n",
       " array([], dtype=int64),\n",
       " array([], dtype=int64),\n",
       " array([], dtype=int64),\n",
       " array([3]),\n",
       " array([], dtype=int64),\n",
       " array([], dtype=int64),\n",
       " array([], dtype=int64),\n",
       " array([], dtype=int64),\n",
       " array([], dtype=int64),\n",
       " array([], dtype=int64),\n",
       " array([], dtype=int64)]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False]\n",
      "[ True False False]\n",
      "\n",
      "[False  True False]\n",
      "[False  True False]\n",
      "\n",
      "[False False  True]\n",
      "[False False  True]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col1, col2 in zip(filters_corr[0],filters_corr[1]):\n",
    "    print(col1)\n",
    "    print(col2)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0]), 1: array([], dtype=int64), 2: array([1])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 2, 1, 0.2685624012638231, [-11, 0], [-6, 0], array([2, 3]))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.metrics.compute_metrics(in_on[4],in_off[4],tg_on[4],tg_off[4],return_not_chosen=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning3",
   "language": "python",
   "name": "deeplearning3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
