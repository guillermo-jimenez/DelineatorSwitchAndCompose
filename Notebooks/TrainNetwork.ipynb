{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import skimage\n",
    "import skimage.segmentation\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import math\n",
    "import glob\n",
    "import uuid\n",
    "import random\n",
    "import platform\n",
    "import ecgdetectors\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import wfdb\n",
    "import json\n",
    "import tqdm\n",
    "import dill\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import lognorm, norm, halfnorm\n",
    "\n",
    "import src.data\n",
    "import utils\n",
    "import utils.wavelet\n",
    "import utils.data\n",
    "import utils.data.augmentation\n",
    "import utils.visualization\n",
    "import utils.visualization.plot\n",
    "import utils.torch\n",
    "import utils.torch.nn\n",
    "import utils.torch.nn as nn\n",
    "import utils.torch.loss\n",
    "import utils.torch.train\n",
    "import utils.torch.data\n",
    "import utils.torch.preprocessing\n",
    "import utils.torch.models\n",
    "import utils.torch.models.lego\n",
    "import utils.torch.models.variational\n",
    "import utils.torch.models.classification\n",
    "\n",
    "from utils.signal import StandardHeader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load execution configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./configurations/UNet3Levels.json', 'r') as f:\n",
    "    execution = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data\n",
    "#### 1.1. Load individual segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = utils.pickleload(os.path.join('.','pickle','Psignal.pkl'))\n",
    "PQ = utils.pickleload(os.path.join('.','pickle','PQsignal.pkl'))\n",
    "QRS = utils.pickleload(os.path.join('.','pickle','QRSsignal.pkl'))\n",
    "ST = utils.pickleload(os.path.join('.','pickle','STsignal.pkl'))\n",
    "T = utils.pickleload(os.path.join('.','pickle','Tsignal.pkl'))\n",
    "TP = utils.pickleload(os.path.join('.','pickle','TPsignal.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Filter out faulty segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in (['104_II','104_III','104_AVR','104_AVF','103_III'] + \n",
    "            ['74_{}'.format(h) for h in StandardHeader] + \n",
    "            ['111_{}'.format(h) for h in StandardHeader] +\n",
    "            ['95_{}'.format(h) for h in StandardHeader] + \n",
    "            ['103_{}'.format(h) for h in StandardHeader] +\n",
    "            ['34_{}'.format(h) for h in StandardHeader]):\n",
    "    [P.pop(k) for k in list(P.keys()) if k.startswith('{}###'.format(key))]\n",
    "    [PQ.pop(k) for k in list(PQ.keys()) if k.startswith('{}###'.format(key))]\n",
    "    [QRS.pop(k) for k in list(QRS.keys()) if k.startswith('{}###'.format(key))]\n",
    "    [ST.pop(k) for k in list(ST.keys()) if k.startswith('{}###'.format(key))]\n",
    "    [T.pop(k) for k in list(T.keys()) if k.startswith('{}###'.format(key))]\n",
    "    [TP.pop(k) for k in list(TP.keys()) if k.startswith('{}###'.format(key))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in []: \n",
    "    if key in P: P.pop(key)\n",
    "for key in []: \n",
    "    if key in PQ: PQ.pop(key)\n",
    "for key in ['7_V2###0','7_V2###2','7_V2###4','95_V4###0','95_V4###1','95_V4###2']: \n",
    "    if key in QRS: QRS.pop(key)\n",
    "for key in []: \n",
    "    if key in ST: ST.pop(key)\n",
    "for key in []: \n",
    "    if key in T: T.pop(key)\n",
    "for key in (['52_III###4','34_V6###6','74_V4###0','74_V4###1','74_V4###2',\n",
    "             '74_V4###3','74_V4###4','74_V4###5','74_V4###6','74_V4###7',] + \n",
    "            ['111_V2###{}'.format(i) for i in range(7)]): \n",
    "    if key in TP: TP.pop(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Normalize amplitudes & get amplitude distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = np.max\n",
    "\n",
    "amplitudes = {k.split('###')[0]: [] for k in list(QRS)}\n",
    "for k in QRS:\n",
    "    g = k.split('###')[0]\n",
    "    segment = utils.signal.on_off_correction(QRS[k])\n",
    "    amplitudes[g].append(np.max(segment) - np.min(segment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pamplitudes = []\n",
    "for k in P:\n",
    "    segment = utils.signal.on_off_correction(P[k])/metric(amplitudes[k.split('###')[0]])\n",
    "    Pamplitudes.append(segment.max()-segment.min())\n",
    "Pamplitudes = np.array(Pamplitudes)\n",
    "PQamplitudes = []\n",
    "for k in PQ:\n",
    "    segment = utils.signal.on_off_correction(PQ[k])/metric(amplitudes[k.split('###')[0]])\n",
    "    PQamplitudes.append(segment.max()-segment.min())\n",
    "PQamplitudes = np.array(PQamplitudes)\n",
    "QRSamplitudes = []\n",
    "for k in QRS:\n",
    "    segment = utils.signal.on_off_correction(QRS[k])/metric(amplitudes[k.split('###')[0]])\n",
    "    QRSamplitudes.append(segment.max()-segment.min())\n",
    "QRSamplitudes = np.array(QRSamplitudes)\n",
    "STamplitudes = []\n",
    "for k in ST:\n",
    "    segment = utils.signal.on_off_correction(ST[k])/metric(amplitudes[k.split('###')[0]])\n",
    "    STamplitudes.append(segment.max()-segment.min())\n",
    "STamplitudes = np.array(STamplitudes)\n",
    "Tamplitudes = []\n",
    "for k in T:\n",
    "    segment = utils.signal.on_off_correction(T[k])/metric(amplitudes[k.split('###')[0]])\n",
    "    Tamplitudes.append(segment.max()-segment.min())\n",
    "Tamplitudes = np.array(Tamplitudes)\n",
    "TPamplitudes = []\n",
    "for k in TP:\n",
    "    segment = utils.signal.on_off_correction(TP[k])/metric(amplitudes[k.split('###')[0]])\n",
    "    TPamplitudes.append(segment.max()-segment.min())\n",
    "TPamplitudes = np.array(TPamplitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pamplitudes = lognorm(*lognorm.fit(Pamplitudes))\n",
    "PQamplitudes = lognorm(*lognorm.fit(PQamplitudes[PQamplitudes<0.3]))\n",
    "QRSamplitudes = lognorm(*lognorm.fit(QRSamplitudes))\n",
    "STamplitudes = lognorm(*lognorm.fit(STamplitudes))\n",
    "Tamplitudes = lognorm(*lognorm.fit(Tamplitudes[(Tamplitudes>0.05) & (Tamplitudes<0.5)]))\n",
    "TPamplitudes = lognorm(*lognorm.fit(TPamplitudes[TPamplitudes<0.4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-0e724456727d>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  PQ[k] = segment/(np.max(segment)-np.min(segment))\n",
      "<ipython-input-10-0e724456727d>:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ST[k] = segment/(np.max(segment)-np.min(segment))\n"
     ]
    }
   ],
   "source": [
    "for k in list(P.keys()):\n",
    "    segment = utils.signal.on_off_correction(P[k])\n",
    "    P[k] = segment/(np.max(segment)-np.min(segment))\n",
    "    if np.any(np.isinf(P[k])) or np.any(np.isnan(P[k])):\n",
    "        P.pop(k)\n",
    "for k in list(PQ.keys()):\n",
    "    segment = utils.signal.on_off_correction(PQ[k])\n",
    "    PQ[k] = segment/(np.max(segment)-np.min(segment))\n",
    "    if np.any(np.isinf(PQ[k])) or np.any(np.isnan(PQ[k])):\n",
    "        PQ.pop(k)\n",
    "for k in list(QRS.keys()):\n",
    "    segment = utils.signal.on_off_correction(QRS[k])\n",
    "    QRS[k] = segment/(np.max(segment)-np.min(segment))\n",
    "    if np.any(np.isinf(QRS[k])) or np.any(np.isnan(QRS[k])):\n",
    "        QRS.pop(k)\n",
    "for k in list(ST.keys()):\n",
    "    segment = utils.signal.on_off_correction(ST[k])\n",
    "    ST[k] = segment/(np.max(segment)-np.min(segment))\n",
    "    if np.any(np.isinf(ST[k])) or np.any(np.isnan(ST[k])):\n",
    "        ST.pop(k)\n",
    "for k in list(T.keys()):\n",
    "    segment = utils.signal.on_off_correction(T[k])\n",
    "    T[k] = segment/(np.max(segment)-np.min(segment))\n",
    "    if np.any(np.isinf(T[k])) or np.any(np.isnan(T[k])):\n",
    "        T.pop(k)\n",
    "for k in list(TP.keys()):\n",
    "    segment = utils.signal.on_off_correction(TP[k])\n",
    "    TP[k] = segment/(np.max(segment)-np.min(segment))\n",
    "    if np.any(np.isinf(TP[k])) or np.any(np.isnan(TP[k])):\n",
    "        TP.pop(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = {}\n",
    "for k in list(P) + list(PQ) + list(QRS) + list(ST) + list(T) + list(TP):\n",
    "    uid = k.split('###')[0].split('_')[0].split('-')[0]\n",
    "    if uid not in all_keys:\n",
    "        all_keys[uid] = [k]\n",
    "    else:\n",
    "        all_keys[uid].append(k)\n",
    "        \n",
    "# Get database and file\n",
    "filenames = []\n",
    "database = []\n",
    "for k in all_keys:\n",
    "    filenames.append(k)\n",
    "    if k.startswith('SOO'):\n",
    "        database.append(0)\n",
    "    elif k.startswith('sel'):\n",
    "        database.append(1)\n",
    "    else:\n",
    "        database.append(2)\n",
    "filenames = np.array(filenames)\n",
    "database = np.array(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(execution['seed'])\n",
    "np.random.seed(execution['seed'])\n",
    "torch.random.manual_seed(execution['seed'])\n",
    "splitter = sklearn.model_selection.StratifiedKFold(5).split(filenames,database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix_train,ix_test in splitter:\n",
    "    train_keys, test_keys = ([],[])\n",
    "    for k in np.array(filenames)[ix_train]: train_keys += all_keys[k]\n",
    "    for k in np.array(filenames)[ix_test]:  test_keys += all_keys[k]\n",
    "    \n",
    "    # Divide train/test segments\n",
    "    Ptrain = {k: P[k] for k in P if k in train_keys}\n",
    "    PQtrain = {k: PQ[k] for k in PQ if k in train_keys}\n",
    "    QRStrain = {k: QRS[k] for k in QRS if k in train_keys}\n",
    "    STtrain = {k: ST[k] for k in ST if k in train_keys}\n",
    "    Ttrain = {k: T[k] for k in T if k in train_keys}\n",
    "    TPtrain = {k: TP[k] for k in TP if k in train_keys}\n",
    "\n",
    "    Ptest = {k: P[k] for k in P if k in test_keys}\n",
    "    PQtest = {k: PQ[k] for k in PQ if k in test_keys}\n",
    "    QRStest = {k: QRS[k] for k in QRS if k in test_keys}\n",
    "    STtest = {k: ST[k] for k in ST if k in test_keys}\n",
    "    Ttest = {k: T[k] for k in T if k in test_keys}\n",
    "    TPtest = {k: TP[k] for k in TP if k in test_keys}\n",
    "    \n",
    "    dataset_train = src.data.Dataset(Ptrain, QRStrain, Ttrain, PQtrain, STtrain, TPtrain, \n",
    "                                     Pamplitudes, QRSamplitudes, Tamplitudes, PQamplitudes, \n",
    "                                     STamplitudes, TPamplitudes, 200*execution['loader']['batch_size'])\n",
    "    dataset_test = src.data.Dataset(Ptest, QRStest, Ttest, PQtest, STtest, TPtest, \n",
    "                                    Pamplitudes, QRSamplitudes, Tamplitudes, PQamplitudes, \n",
    "                                    STamplitudes, TPamplitudes, 200*execution['loader']['batch_size'])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    loader_train = torch.utils.data.DataLoader(dataset_train,**execution['loader'])\n",
    "    loader_test = torch.utils.data.DataLoader(dataset_test,**execution['loader'])\n",
    "    \n",
    "    # Define model\n",
    "    model = nn.ModelGraph(execution['model']).float()\n",
    "    \n",
    "    # \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./configurations/UNet4Levels.json', 'r') as f:\n",
    "    execution = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = nn.ModelGraph(execution['model']).float().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 507 ms, sys: 7.91 ms, total: 515 ms\n",
      "Wall time: 465 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for x,y in loader_test:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, eps: float = 1e-6, smooth: int = 1.):\n",
    "        self.eps = eps\n",
    "        self.smooth = smooth\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input: torch.tensor, target: torch.tensor) -> torch.tensor:\n",
    "        intersection = (target*input).sum()\n",
    "        union = (target+input).sum()\n",
    "        loss = (1 - ((2. * intersection + self.smooth + self.eps) / \n",
    "                        (union + self.smooth + self.eps)))\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Train) Epoch   1/500, Loss      0.562: 100%|██████████| 200/200 [01:44<00:00,  1.91it/s]\n",
      "(Valid) Epoch   1/500, Loss      0.502: 100%|██████████| 200/200 [01:33<00:00,  2.14it/s]\n",
      "/home/guille/VirtEnv/DeepLearning3/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
      "(Train) Epoch   2/500, Loss      0.513: 100%|██████████| 200/200 [01:47<00:00,  1.87it/s]\n",
      "(Valid) Epoch   2/500, Loss      0.480: 100%|██████████| 200/200 [01:31<00:00,  2.20it/s]\n",
      "(Train) Epoch   3/500, Loss      0.435: 100%|██████████| 200/200 [01:43<00:00,  1.93it/s]\n",
      "(Valid) Epoch   3/500, Loss      0.431: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n",
      "(Train) Epoch   4/500, Loss      0.262: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch   4/500, Loss      0.270: 100%|██████████| 200/200 [01:17<00:00,  2.57it/s]\n",
      "(Train) Epoch   5/500, Loss      0.317: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch   5/500, Loss      0.270: 100%|██████████| 200/200 [01:23<00:00,  2.40it/s]\n",
      "(Train) Epoch   6/500, Loss      0.243: 100%|██████████| 200/200 [01:40<00:00,  1.99it/s]\n",
      "(Valid) Epoch   6/500, Loss      0.252: 100%|██████████| 200/200 [01:23<00:00,  2.40it/s]\n",
      "(Train) Epoch   7/500, Loss      0.263: 100%|██████████| 200/200 [01:38<00:00,  2.02it/s]\n",
      "(Valid) Epoch   7/500, Loss      0.247: 100%|██████████| 200/200 [01:20<00:00,  2.47it/s]\n",
      "(Train) Epoch   8/500, Loss      0.239: 100%|██████████| 200/200 [01:37<00:00,  2.05it/s]\n",
      "(Valid) Epoch   8/500, Loss      0.250: 100%|██████████| 200/200 [01:23<00:00,  2.39it/s]\n",
      "(Train) Epoch   9/500, Loss      0.244: 100%|██████████| 200/200 [01:33<00:00,  2.14it/s]\n",
      "(Valid) Epoch   9/500, Loss      0.214: 100%|██████████| 200/200 [01:24<00:00,  2.37it/s]\n",
      "(Train) Epoch  10/500, Loss      0.221: 100%|██████████| 200/200 [01:40<00:00,  1.99it/s]\n",
      "(Valid) Epoch  10/500, Loss      0.223: 100%|██████████| 200/200 [01:20<00:00,  2.48it/s]\n",
      "(Train) Epoch  11/500, Loss      0.230: 100%|██████████| 200/200 [01:35<00:00,  2.10it/s]\n",
      "(Valid) Epoch  11/500, Loss      0.203: 100%|██████████| 200/200 [01:20<00:00,  2.48it/s]\n",
      "(Train) Epoch  12/500, Loss      0.255: 100%|██████████| 200/200 [01:34<00:00,  2.11it/s]\n",
      "(Valid) Epoch  12/500, Loss      0.218: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch  13/500, Loss      0.258: 100%|██████████| 200/200 [01:32<00:00,  2.16it/s]\n",
      "(Valid) Epoch  13/500, Loss      0.228: 100%|██████████| 200/200 [01:21<00:00,  2.46it/s]\n",
      "(Train) Epoch  14/500, Loss      0.232: 100%|██████████| 200/200 [01:35<00:00,  2.09it/s]\n",
      "(Valid) Epoch  14/500, Loss      0.219: 100%|██████████| 200/200 [01:25<00:00,  2.33it/s]\n",
      "(Train) Epoch  15/500, Loss      0.342: 100%|██████████| 200/200 [01:34<00:00,  2.12it/s]\n",
      "(Valid) Epoch  15/500, Loss      0.218: 100%|██████████| 200/200 [01:19<00:00,  2.51it/s]\n",
      "(Train) Epoch  16/500, Loss      0.221: 100%|██████████| 200/200 [01:33<00:00,  2.13it/s]\n",
      "(Valid) Epoch  16/500, Loss      0.211: 100%|██████████| 200/200 [01:19<00:00,  2.50it/s]\n",
      "(Train) Epoch  17/500, Loss      0.218: 100%|██████████| 200/200 [01:32<00:00,  2.16it/s]\n",
      "(Valid) Epoch  17/500, Loss      0.228: 100%|██████████| 200/200 [01:20<00:00,  2.49it/s]\n",
      "(Train) Epoch  18/500, Loss      0.203: 100%|██████████| 200/200 [01:33<00:00,  2.13it/s]\n",
      "(Valid) Epoch  18/500, Loss      0.235: 100%|██████████| 200/200 [01:20<00:00,  2.49it/s]\n",
      "(Train) Epoch  19/500, Loss      0.232: 100%|██████████| 200/200 [01:33<00:00,  2.14it/s]\n",
      "(Valid) Epoch  19/500, Loss      0.212: 100%|██████████| 200/200 [01:22<00:00,  2.42it/s]\n",
      "(Train) Epoch  20/500, Loss      0.224: 100%|██████████| 200/200 [01:33<00:00,  2.13it/s]\n",
      "(Valid) Epoch  20/500, Loss      0.202: 100%|██████████| 200/200 [01:22<00:00,  2.44it/s]\n",
      "(Train) Epoch  21/500, Loss      0.216: 100%|██████████| 200/200 [01:33<00:00,  2.14it/s]\n",
      "(Valid) Epoch  21/500, Loss      0.201: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "(Train) Epoch  22/500, Loss      0.231: 100%|██████████| 200/200 [01:35<00:00,  2.10it/s]\n",
      "(Valid) Epoch  22/500, Loss      0.256: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  23/500, Loss      0.208: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  23/500, Loss      0.233: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n",
      "(Train) Epoch  24/500, Loss      0.208: 100%|██████████| 200/200 [01:31<00:00,  2.20it/s]\n",
      "(Valid) Epoch  24/500, Loss      0.202: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  25/500, Loss      0.199: 100%|██████████| 200/200 [01:31<00:00,  2.20it/s]\n",
      "(Valid) Epoch  25/500, Loss      0.211: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n",
      "(Train) Epoch  26/500, Loss      0.207: 100%|██████████| 200/200 [01:30<00:00,  2.20it/s]\n",
      "(Valid) Epoch  26/500, Loss      0.190: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  27/500, Loss      0.253: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  27/500, Loss      0.208: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  28/500, Loss      0.194: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  28/500, Loss      0.217: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "(Train) Epoch  29/500, Loss      0.261: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  29/500, Loss      0.209: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  30/500, Loss      0.317: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  30/500, Loss      0.206: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  31/500, Loss      0.230: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  31/500, Loss      0.209: 100%|██████████| 200/200 [01:18<00:00,  2.53it/s]\n",
      "(Train) Epoch  32/500, Loss      0.219: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  32/500, Loss      0.201: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  33/500, Loss      0.210: 100%|██████████| 200/200 [01:32<00:00,  2.16it/s]\n",
      "(Valid) Epoch  33/500, Loss      0.204: 100%|██████████| 200/200 [01:19<00:00,  2.50it/s]\n",
      "(Train) Epoch  34/500, Loss      0.192: 100%|██████████| 200/200 [01:32<00:00,  2.16it/s]\n",
      "(Valid) Epoch  34/500, Loss      0.193: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch  35/500, Loss      0.318: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  35/500, Loss      0.206: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  36/500, Loss      0.203: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch  36/500, Loss      0.191: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "(Train) Epoch  37/500, Loss      0.202: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  37/500, Loss      0.201: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "(Train) Epoch  38/500, Loss      0.184: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  38/500, Loss      0.201: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  39/500, Loss      0.198: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  39/500, Loss      0.194: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch  40/500, Loss      0.176: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch  40/500, Loss      0.200: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch  41/500, Loss      0.186: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  41/500, Loss      0.174: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "(Train) Epoch  42/500, Loss      0.219: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  42/500, Loss      0.194: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch  43/500, Loss      0.349: 100%|██████████| 200/200 [01:31<00:00,  2.17it/s]\n",
      "(Valid) Epoch  43/500, Loss      0.198: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "(Train) Epoch  44/500, Loss      0.203: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Valid) Epoch  44/500, Loss      0.213: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch  45/500, Loss      0.181: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  45/500, Loss      0.181: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "(Train) Epoch  46/500, Loss      0.187: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  46/500, Loss      0.198: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch  47/500, Loss      0.209: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  47/500, Loss      0.206: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch  48/500, Loss      0.209: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch  48/500, Loss      0.183: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "(Train) Epoch  49/500, Loss      0.202: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  49/500, Loss      0.192: 100%|██████████| 200/200 [01:19<00:00,  2.51it/s]\n",
      "(Train) Epoch  50/500, Loss      0.169: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  50/500, Loss      0.195: 100%|██████████| 200/200 [01:18<00:00,  2.53it/s]\n",
      "(Train) Epoch  51/500, Loss      0.177: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch  51/500, Loss      0.187: 100%|██████████| 200/200 [01:19<00:00,  2.51it/s]\n",
      "(Train) Epoch  52/500, Loss      0.182: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch  52/500, Loss      0.181: 100%|██████████| 200/200 [01:20<00:00,  2.49it/s]\n",
      "(Train) Epoch  53/500, Loss      0.201: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch  53/500, Loss      0.184: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch  54/500, Loss      0.221: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  54/500, Loss      0.186: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch  55/500, Loss      0.240: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  55/500, Loss      0.213: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  56/500, Loss      0.234: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  56/500, Loss      0.187: 100%|██████████| 200/200 [01:19<00:00,  2.51it/s]\n",
      "(Train) Epoch  57/500, Loss      0.182: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  57/500, Loss      0.190: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "(Train) Epoch  58/500, Loss      0.226: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch  58/500, Loss      0.184: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  59/500, Loss      0.243: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch  59/500, Loss      0.179: 100%|██████████| 200/200 [01:19<00:00,  2.51it/s]\n",
      "(Train) Epoch  60/500, Loss      0.188: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch  60/500, Loss      0.212: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch  61/500, Loss      0.177: 100%|██████████| 200/200 [01:32<00:00,  2.16it/s]\n",
      "(Valid) Epoch  61/500, Loss      0.182: 100%|██████████| 200/200 [01:19<00:00,  2.51it/s]\n",
      "(Train) Epoch  62/500, Loss      0.226: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch  62/500, Loss      0.190: 100%|██████████| 200/200 [01:19<00:00,  2.51it/s]\n",
      "(Train) Epoch  63/500, Loss      0.226: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch  63/500, Loss      0.182: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch  64/500, Loss      0.176: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  64/500, Loss      0.190: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  65/500, Loss      0.186: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch  65/500, Loss      0.182: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "(Train) Epoch  66/500, Loss      0.278: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  66/500, Loss      0.192: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "(Train) Epoch  67/500, Loss      0.166: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch  67/500, Loss      0.183: 100%|██████████| 200/200 [01:20<00:00,  2.49it/s]\n",
      "(Train) Epoch  68/500, Loss      0.184: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch  68/500, Loss      0.173: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "(Train) Epoch  69/500, Loss      0.183: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  69/500, Loss      0.189: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "(Train) Epoch  70/500, Loss      0.184: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  70/500, Loss      0.182: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n",
      "(Train) Epoch  71/500, Loss      0.232: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  71/500, Loss      0.198: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "(Train) Epoch  72/500, Loss      0.176: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  72/500, Loss      0.196: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n",
      "(Train) Epoch  73/500, Loss      0.198: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  73/500, Loss      0.198: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch  74/500, Loss      0.187: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  74/500, Loss      0.169: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  75/500, Loss      0.171: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  75/500, Loss      0.184: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n",
      "(Train) Epoch  76/500, Loss      0.234: 100%|██████████| 200/200 [01:30<00:00,  2.20it/s]\n",
      "(Valid) Epoch  76/500, Loss      0.175: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n",
      "(Train) Epoch  77/500, Loss      0.174: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  77/500, Loss      0.180: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n",
      "(Train) Epoch  78/500, Loss      0.183: 100%|██████████| 200/200 [01:31<00:00,  2.20it/s]\n",
      "(Valid) Epoch  78/500, Loss      0.192: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n",
      "(Train) Epoch  79/500, Loss      0.178: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  79/500, Loss      0.168: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch  80/500, Loss      0.189: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  80/500, Loss      0.176: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  81/500, Loss      0.187: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  81/500, Loss      0.182: 100%|██████████| 200/200 [01:19<00:00,  2.51it/s]\n",
      "(Train) Epoch  82/500, Loss      0.354: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  82/500, Loss      0.170: 100%|██████████| 200/200 [01:18<00:00,  2.53it/s]\n",
      "(Train) Epoch  83/500, Loss      0.300: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  83/500, Loss      0.171: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n",
      "(Train) Epoch  84/500, Loss      0.241: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  84/500, Loss      0.185: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  85/500, Loss      0.211: 100%|██████████| 200/200 [01:31<00:00,  2.20it/s]\n",
      "(Valid) Epoch  85/500, Loss      0.192: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  86/500, Loss      0.252: 100%|██████████| 200/200 [01:30<00:00,  2.20it/s]\n",
      "(Valid) Epoch  86/500, Loss      0.192: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n",
      "(Train) Epoch  87/500, Loss      0.240: 100%|██████████| 200/200 [01:30<00:00,  2.20it/s]\n",
      "(Valid) Epoch  87/500, Loss      0.175: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  88/500, Loss      0.186: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch  88/500, Loss      0.191: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "(Train) Epoch  89/500, Loss      0.173: 100%|██████████| 200/200 [01:35<00:00,  2.09it/s]\n",
      "(Valid) Epoch  89/500, Loss      0.179: 100%|██████████| 200/200 [01:20<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Train) Epoch  90/500, Loss      0.321: 100%|██████████| 200/200 [01:34<00:00,  2.11it/s]\n",
      "(Valid) Epoch  90/500, Loss      0.197: 100%|██████████| 200/200 [01:23<00:00,  2.41it/s]\n",
      "(Train) Epoch  91/500, Loss      0.169: 100%|██████████| 200/200 [01:36<00:00,  2.07it/s]\n",
      "(Valid) Epoch  91/500, Loss      0.176: 100%|██████████| 200/200 [01:21<00:00,  2.46it/s]\n",
      "(Train) Epoch  92/500, Loss      0.187: 100%|██████████| 200/200 [01:36<00:00,  2.07it/s]\n",
      "(Valid) Epoch  92/500, Loss      0.163: 100%|██████████| 200/200 [01:21<00:00,  2.44it/s]\n",
      "(Train) Epoch  93/500, Loss      0.185: 100%|██████████| 200/200 [01:33<00:00,  2.14it/s]\n",
      "(Valid) Epoch  93/500, Loss      0.182: 100%|██████████| 200/200 [01:20<00:00,  2.48it/s]\n",
      "(Train) Epoch  94/500, Loss      0.187: 100%|██████████| 200/200 [01:40<00:00,  1.99it/s]\n",
      "(Valid) Epoch  94/500, Loss      0.192: 100%|██████████| 200/200 [01:20<00:00,  2.49it/s]\n",
      "(Train) Epoch  95/500, Loss      0.330: 100%|██████████| 200/200 [01:33<00:00,  2.13it/s]\n",
      "(Valid) Epoch  95/500, Loss      0.210: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch  96/500, Loss      0.171: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch  96/500, Loss      0.169: 100%|██████████| 200/200 [01:20<00:00,  2.48it/s]\n",
      "(Train) Epoch  97/500, Loss      0.178: 100%|██████████| 200/200 [01:34<00:00,  2.12it/s]\n",
      "(Valid) Epoch  97/500, Loss      0.192: 100%|██████████| 200/200 [01:21<00:00,  2.44it/s]\n",
      "(Train) Epoch  98/500, Loss      0.186: 100%|██████████| 200/200 [01:33<00:00,  2.13it/s]\n",
      "(Valid) Epoch  98/500, Loss      0.185: 100%|██████████| 200/200 [01:20<00:00,  2.48it/s]\n",
      "(Train) Epoch  99/500, Loss      0.169: 100%|██████████| 200/200 [01:33<00:00,  2.14it/s]\n",
      "(Valid) Epoch  99/500, Loss      0.186: 100%|██████████| 200/200 [01:21<00:00,  2.47it/s]\n",
      "(Train) Epoch 100/500, Loss      0.164: 100%|██████████| 200/200 [01:37<00:00,  2.06it/s]\n",
      "(Valid) Epoch 100/500, Loss      0.182: 100%|██████████| 200/200 [01:20<00:00,  2.48it/s]\n",
      "(Train) Epoch 101/500, Loss      0.206: 100%|██████████| 200/200 [01:34<00:00,  2.11it/s]\n",
      "(Valid) Epoch 101/500, Loss      0.192: 100%|██████████| 200/200 [01:20<00:00,  2.47it/s]\n",
      "(Train) Epoch 102/500, Loss      0.227: 100%|██████████| 200/200 [01:34<00:00,  2.13it/s]\n",
      "(Valid) Epoch 102/500, Loss      0.177: 100%|██████████| 200/200 [01:21<00:00,  2.47it/s]\n",
      "(Train) Epoch 103/500, Loss      0.296: 100%|██████████| 200/200 [01:33<00:00,  2.15it/s]\n",
      "(Valid) Epoch 103/500, Loss      0.206: 100%|██████████| 200/200 [01:20<00:00,  2.49it/s]\n",
      "(Train) Epoch 104/500, Loss      0.182: 100%|██████████| 200/200 [01:33<00:00,  2.14it/s]\n",
      "(Valid) Epoch 104/500, Loss      0.169: 100%|██████████| 200/200 [01:19<00:00,  2.50it/s]\n",
      "(Train) Epoch 105/500, Loss      0.341: 100%|██████████| 200/200 [01:34<00:00,  2.11it/s]\n",
      "(Valid) Epoch 105/500, Loss      0.185: 100%|██████████| 200/200 [01:20<00:00,  2.48it/s]\n",
      "(Train) Epoch 106/500, Loss      0.170: 100%|██████████| 200/200 [01:33<00:00,  2.13it/s]\n",
      "(Valid) Epoch 106/500, Loss      0.190: 100%|██████████| 200/200 [01:24<00:00,  2.36it/s]\n",
      "(Train) Epoch 107/500, Loss      0.191: 100%|██████████| 200/200 [01:37<00:00,  2.05it/s]\n",
      "(Valid) Epoch 107/500, Loss      0.179: 100%|██████████| 200/200 [01:19<00:00,  2.51it/s]\n",
      "(Train) Epoch 108/500, Loss      0.183: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch 108/500, Loss      0.192: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch 109/500, Loss      0.164: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch 109/500, Loss      0.184: 100%|██████████| 200/200 [01:19<00:00,  2.50it/s]\n",
      "(Train) Epoch 110/500, Loss      0.222: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch 110/500, Loss      0.172: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch 111/500, Loss      0.171: 100%|██████████| 200/200 [01:33<00:00,  2.13it/s]\n",
      "(Valid) Epoch 111/500, Loss      0.183: 100%|██████████| 200/200 [01:19<00:00,  2.50it/s]\n",
      "(Train) Epoch 112/500, Loss      0.213: 100%|██████████| 200/200 [01:32<00:00,  2.16it/s]\n",
      "(Valid) Epoch 112/500, Loss      0.169: 100%|██████████| 200/200 [01:19<00:00,  2.51it/s]\n",
      "(Train) Epoch 113/500, Loss      0.229: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n",
      "(Valid) Epoch 113/500, Loss      0.201: 100%|██████████| 200/200 [01:17<00:00,  2.58it/s]\n",
      "(Train) Epoch 114/500, Loss      0.169: 100%|██████████| 200/200 [01:29<00:00,  2.23it/s]\n",
      "(Valid) Epoch 114/500, Loss      0.167: 100%|██████████| 200/200 [01:17<00:00,  2.59it/s]\n",
      "(Train) Epoch 115/500, Loss      0.173: 100%|██████████| 200/200 [01:30<00:00,  2.22it/s]\n",
      "(Valid) Epoch 115/500, Loss      0.177: 100%|██████████| 200/200 [01:17<00:00,  2.58it/s]\n",
      "(Train) Epoch 116/500, Loss      0.159: 100%|██████████| 200/200 [01:30<00:00,  2.22it/s]\n",
      "(Valid) Epoch 116/500, Loss      0.175: 100%|██████████| 200/200 [01:17<00:00,  2.58it/s]\n",
      "(Train) Epoch 117/500, Loss      0.173: 100%|██████████| 200/200 [01:30<00:00,  2.22it/s]\n",
      "(Valid) Epoch 117/500, Loss      0.188: 100%|██████████| 200/200 [01:17<00:00,  2.58it/s]\n",
      "(Train) Epoch 118/500, Loss      0.340: 100%|██████████| 200/200 [01:29<00:00,  2.22it/s]\n",
      "(Valid) Epoch 118/500, Loss      0.177: 100%|██████████| 200/200 [01:17<00:00,  2.57it/s]\n",
      "(Train) Epoch 119/500, Loss      0.197: 100%|██████████| 200/200 [01:29<00:00,  2.23it/s]\n",
      "(Valid) Epoch 119/500, Loss      0.173: 100%|██████████| 200/200 [01:17<00:00,  2.58it/s]\n",
      "(Train) Epoch 120/500, Loss      0.179: 100%|██████████| 200/200 [01:30<00:00,  2.22it/s]\n",
      "(Valid) Epoch 120/500, Loss      0.167: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n",
      "(Train) Epoch 121/500, Loss      0.179: 100%|██████████| 200/200 [01:30<00:00,  2.22it/s]\n",
      "(Valid) Epoch 121/500, Loss      0.198: 100%|██████████| 200/200 [01:17<00:00,  2.58it/s]\n",
      "(Train) Epoch 122/500, Loss      0.188: 100%|██████████| 200/200 [01:29<00:00,  2.23it/s]\n",
      "(Valid) Epoch 122/500, Loss      0.186: 100%|██████████| 200/200 [01:17<00:00,  2.59it/s]\n",
      "(Train) Epoch 123/500, Loss      0.180: 100%|██████████| 200/200 [01:29<00:00,  2.23it/s]\n",
      "(Valid) Epoch 123/500, Loss      0.175: 100%|██████████| 200/200 [01:17<00:00,  2.59it/s]\n",
      "(Train) Epoch 124/500, Loss      0.158: 100%|██████████| 200/200 [01:29<00:00,  2.22it/s]\n",
      "(Valid) Epoch 124/500, Loss      0.184: 100%|██████████| 200/200 [01:17<00:00,  2.58it/s]\n",
      "(Train) Epoch 125/500, Loss      0.152: 100%|██████████| 200/200 [01:30<00:00,  2.22it/s]\n",
      "(Valid) Epoch 125/500, Loss      0.181: 100%|██████████| 200/200 [01:17<00:00,  2.57it/s]\n",
      "(Train) Epoch 126/500, Loss      0.160: 100%|██████████| 200/200 [01:29<00:00,  2.22it/s]\n",
      "(Valid) Epoch 126/500, Loss      0.189: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n",
      "(Train) Epoch 127/500, Loss      0.250: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]\n",
      "(Valid) Epoch 127/500, Loss      0.179: 100%|██████████| 200/200 [01:20<00:00,  2.48it/s]\n",
      "(Train) Epoch 128/500, Loss      0.183: 100%|██████████| 200/200 [01:33<00:00,  2.15it/s]\n",
      "(Valid) Epoch 128/500, Loss      0.156: 100%|██████████| 200/200 [01:20<00:00,  2.49it/s]\n",
      "(Train) Epoch 129/500, Loss      0.207: 100%|██████████| 200/200 [01:32<00:00,  2.15it/s]\n",
      "(Valid) Epoch 129/500, Loss      0.184: 100%|██████████| 200/200 [01:20<00:00,  2.49it/s]\n",
      "(Train) Epoch 130/500, Loss      0.222: 100%|██████████| 200/200 [01:32<00:00,  2.15it/s]\n",
      "(Valid) Epoch 130/500, Loss      0.175: 100%|██████████| 200/200 [01:21<00:00,  2.46it/s]\n",
      "(Train) Epoch 131/500, Loss      0.222: 100%|██████████| 200/200 [01:34<00:00,  2.12it/s]\n",
      "(Valid) Epoch 131/500, Loss      0.193: 100%|██████████| 200/200 [01:21<00:00,  2.45it/s]\n",
      "(Train) Epoch 132/500, Loss      0.233: 100%|██████████| 200/200 [01:34<00:00,  2.13it/s]\n",
      "(Valid) Epoch 132/500, Loss      0.156: 100%|██████████| 200/200 [01:21<00:00,  2.44it/s]\n",
      "(Train) Epoch 133/500, Loss      0.156: 100%|██████████| 200/200 [01:34<00:00,  2.11it/s]\n",
      "(Valid) Epoch 133/500, Loss      0.197: 100%|██████████| 200/200 [01:21<00:00,  2.45it/s]\n",
      "(Train) Epoch 134/500, Loss      0.174: 100%|██████████| 200/200 [01:34<00:00,  2.12it/s]\n",
      "(Valid) Epoch 134/500, Loss      0.191: 100%|██████████| 200/200 [01:21<00:00,  2.44it/s]\n",
      "(Train) Epoch 135/500, Loss      0.155: 100%|██████████| 200/200 [01:35<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Valid) Epoch 135/500, Loss      0.177: 100%|██████████| 200/200 [01:20<00:00,  2.47it/s]\n",
      "(Train) Epoch 136/500, Loss      0.175: 100%|██████████| 200/200 [01:34<00:00,  2.12it/s]\n",
      "(Valid) Epoch 136/500, Loss      0.192: 100%|██████████| 200/200 [01:21<00:00,  2.46it/s]\n",
      "(Train) Epoch 137/500, Loss      0.220: 100%|██████████| 200/200 [01:34<00:00,  2.13it/s]\n",
      "(Valid) Epoch 137/500, Loss      0.168: 100%|██████████| 200/200 [01:22<00:00,  2.44it/s]\n",
      "(Train) Epoch 138/500, Loss      0.166: 100%|██████████| 200/200 [01:36<00:00,  2.07it/s]\n",
      "(Valid) Epoch 138/500, Loss      0.171: 100%|██████████| 200/200 [01:23<00:00,  2.38it/s]\n",
      "(Train) Epoch 139/500, Loss      0.165: 100%|██████████| 200/200 [01:35<00:00,  2.10it/s]\n",
      "(Valid) Epoch 139/500, Loss      0.171: 100%|██████████| 200/200 [01:21<00:00,  2.45it/s]\n",
      "(Train) Epoch 140/500, Loss      0.169: 100%|██████████| 200/200 [01:34<00:00,  2.12it/s]\n",
      "(Valid) Epoch 140/500, Loss      0.173: 100%|██████████| 200/200 [01:21<00:00,  2.46it/s]\n",
      "(Train) Epoch 141/500, Loss      0.155: 100%|██████████| 200/200 [01:34<00:00,  2.12it/s]\n",
      "(Valid) Epoch 141/500, Loss      0.162: 100%|██████████| 200/200 [01:21<00:00,  2.46it/s]\n",
      "(Train) Epoch 142/500, Loss      0.210: 100%|██████████| 200/200 [01:35<00:00,  2.09it/s]\n",
      "(Valid) Epoch 142/500, Loss      0.189: 100%|██████████| 200/200 [01:20<00:00,  2.50it/s]\n",
      "(Train) Epoch 143/500, Loss      0.211: 100%|██████████| 200/200 [01:32<00:00,  2.16it/s]\n",
      "(Valid) Epoch 143/500, Loss      0.193: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "(Train) Epoch 144/500, Loss      0.180: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s]\n",
      "(Valid) Epoch 144/500, Loss      0.178: 100%|██████████| 200/200 [01:19<00:00,  2.50it/s]\n",
      "(Train) Epoch 145/500, Loss      0.173: 100%|██████████| 200/200 [01:35<00:00,  2.10it/s]\n",
      "(Valid) Epoch 145/500, Loss      0.190: 100%|██████████| 200/200 [01:21<00:00,  2.47it/s]\n",
      "(Train) Epoch 146/500, Loss      0.343: 100%|██████████| 200/200 [01:33<00:00,  2.13it/s]\n",
      "(Valid) Epoch 146/500, Loss      0.172: 100%|██████████| 200/200 [01:20<00:00,  2.47it/s]\n",
      "(Train) Epoch 147/500, Loss      0.162: 100%|██████████| 200/200 [01:32<00:00,  2.16it/s]\n",
      "(Valid) Epoch 147/500, Loss      0.177: 100%|██████████| 200/200 [01:20<00:00,  2.49it/s]\n",
      "(Train) Epoch 148/500, Loss      0.189: 100%|██████████| 200/200 [01:32<00:00,  2.15it/s]\n",
      "(Valid) Epoch 148/500, Loss      0.180: 100%|██████████| 200/200 [01:19<00:00,  2.51it/s]\n",
      "(Train) Epoch 149/500, Loss      0.163: 100%|██████████| 200/200 [01:34<00:00,  2.12it/s]\n",
      "(Valid) Epoch 149/500, Loss      0.173: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "(Train) Epoch 150/500, Loss      0.180: 100%|██████████| 200/200 [01:35<00:00,  2.09it/s]\n",
      "(Valid) Epoch 150/500, Loss      0.169: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "(Train) Epoch 151/500, Loss      0.225: 100%|██████████| 200/200 [01:32<00:00,  2.16it/s]\n",
      "(Valid) Epoch 151/500, Loss      0.181: 100%|██████████| 200/200 [01:20<00:00,  2.49it/s]\n",
      "(Train) Epoch 152/500, Loss      0.240: 100%|██████████| 200/200 [01:33<00:00,  2.14it/s]\n",
      "(Valid) Epoch 152/500, Loss      0.164: 100%|██████████| 200/200 [01:23<00:00,  2.39it/s]\n",
      "(Train) Epoch 153/500, Loss      0.174: 100%|██████████| 200/200 [01:34<00:00,  2.11it/s]\n",
      "(Valid) Epoch 153/500, Loss      0.162: 100%|██████████| 200/200 [01:21<00:00,  2.45it/s]\n",
      "(Train) Epoch 154/500, Loss      0.179: 100%|██████████| 200/200 [01:34<00:00,  2.12it/s]\n",
      "(Valid) Epoch 154/500, Loss      0.153: 100%|██████████| 200/200 [01:20<00:00,  2.47it/s]\n",
      "(Train) Epoch 155/500, Loss      0.161: 100%|██████████| 200/200 [01:35<00:00,  2.10it/s]\n",
      "(Valid) Epoch 155/500, Loss      0.182: 100%|██████████| 200/200 [01:22<00:00,  2.44it/s]\n",
      "(Train) Epoch 156/500, Loss      0.172: 100%|██████████| 200/200 [01:34<00:00,  2.12it/s]\n",
      "(Valid) Epoch 156/500, Loss      0.170: 100%|██████████| 200/200 [01:21<00:00,  2.45it/s]\n",
      "(Train) Epoch 157/500, Loss      0.181: 100%|██████████| 200/200 [01:33<00:00,  2.14it/s]\n",
      "(Valid) Epoch 157/500, Loss      0.160: 100%|██████████| 200/200 [01:20<00:00,  2.48it/s]\n",
      "(Train) Epoch 158/500, Loss      0.176: 100%|██████████| 200/200 [01:35<00:00,  2.09it/s]\n",
      "(Valid) Epoch 158/500, Loss      0.177: 100%|██████████| 200/200 [01:22<00:00,  2.42it/s]\n",
      "(Train) Epoch 159/500, Loss      0.246: 100%|██████████| 200/200 [01:35<00:00,  2.09it/s]\n",
      "(Valid) Epoch 159/500, Loss      0.185: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "(Train) Epoch 160/500, Loss      0.161: 100%|██████████| 200/200 [01:36<00:00,  2.08it/s]\n",
      "(Valid) Epoch 160/500, Loss      0.201: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "(Train) Epoch 161/500, Loss      0.177:   0%|          | 1/200 [00:00<01:36,  2.06it/s]"
     ]
    }
   ],
   "source": [
    "# Loss\n",
    "criterion = lambda X,y,y_pred: DiceLoss()(y_pred, y)\n",
    "metric = lambda X,y,y_pred: DiceLoss()(y_pred, y)\n",
    "\n",
    "state = {\n",
    "    'epoch'         : 0,\n",
    "    'device'        : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'optimizer'     : utils.class_selector('torch.optim',execution['optimizer']['class'])(model.parameters(), **execution['optimizer']['arguments']),\n",
    "    'root_dir'      : './'\n",
    "}\n",
    "if 'scheduler' in execution:\n",
    "    state['scheduler'] = utils.class_selector('torch.optim.lr_scheduler',execution['scheduler']['class'])(state['optimizer'], **execution['scheduler']['arguments'])\n",
    "\n",
    "state = utils.torch.train.train_model(model,state,execution,loader_train, loader_test, criterion, metric, smaller=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check stuff ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(out,) = model(x.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "w = 2\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x[i,0,:])\n",
    "plt.plot(y[i,w,:],alpha=0.5)\n",
    "plt.plot(out[i,w,:].cpu().detach().numpy(),alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT EXECUTION CONFIGURATION PARAMETERS (JSON) ###\n",
    "with open(\"./parameters.json\", 'r') as f:\n",
    "    execution = json.load(f)\n",
    "\n",
    "execution[\"root_directory\"] = input_directory\n",
    "execution[\"save_directory\"] = output_directory\n",
    "\n",
    "### SET RANDOM SEED ###\n",
    "torch.manual_seed(execution['seed'])\n",
    "random.seed(execution['seed'])\n",
    "np.random.seed(execution['seed'])\n",
    "\n",
    "### LOAD DATASET ###\n",
    "# 0) Get classes\n",
    "# print(list(glob.glob(os.path.join(input_directory,\"*.mat\"))))\n",
    "classes = get_classes(input_directory,[os.path.split(f)[-1] for f in glob.glob(os.path.join(input_directory,\"*.mat\"))])\n",
    "\n",
    "# 1) Load labels and compute detections\n",
    "print(\"########## COMPUTING DETECTIONS ##########\")\n",
    "files = []\n",
    "labels = []\n",
    "detections = []\n",
    "for f in tqdm.tqdm(glob.glob(os.path.join(input_directory,\"*.mat\"))):\n",
    "    # Load data\n",
    "    (signal,header) = wfdb.rdsamp(os.path.join(input_directory,os.path.splitext(f)[0]))\n",
    "    signal = signal.astype('float32')\n",
    "\n",
    "    # Use provided function for retrieving the true label\n",
    "    fname, label_header, label = get_true_labels(f.replace('.mat','.hea'),classes)\n",
    "\n",
    "    # Detect signal\n",
    "    detector = ecgdetectors.Detectors(header['fs'])\n",
    "    index_I = np.where(np.array(list(map(str.upper,header['sig_name']))) == 'I')[0][0]\n",
    "    qrs = detector.pan_tompkins_detector(signal[:,index_I])\n",
    "\n",
    "    # Store file name and label\n",
    "    files.append(fname)\n",
    "    detections.append(qrs)\n",
    "    labels.append(label)\n",
    "\n",
    "labels = np.array(labels)\n",
    "files = np.array(files)\n",
    "\n",
    "# 2) Train-test split\n",
    "labels_train,labels_valid,files_train,files_valid,detections_train,detections_valid = sklearn.model_selection.train_test_split(\n",
    "    labels,\n",
    "    files,\n",
    "    detections,\n",
    "    stratify=labels.argmax(-1),\n",
    "    random_state=execution['seed'],\n",
    ")\n",
    "\n",
    "# Save into folder\n",
    "src.utils.pickledump(labels_train, './training/labels_train.pkl')\n",
    "src.utils.pickledump(labels_valid, './training/labels_valid.pkl')\n",
    "src.utils.pickledump(files_train, './training/files_train.pkl')\n",
    "src.utils.pickledump(files_valid, './training/files_valid.pkl')\n",
    "src.utils.pickledump(detections_train, './training/detections_train.pkl')\n",
    "src.utils.pickledump(detections_valid, './training/detections_valid.pkl')\n",
    "\n",
    "print(\"########## GENERATING TRAIN SET ##########\")\n",
    "# Generate train/test sets\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_valid = []\n",
    "y_valid = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(files_train))):\n",
    "    # Retrieve the file information\n",
    "    (signal,_) = wfdb.rdsamp(os.path.join(execution['root_directory'],files_train[i]))\n",
    "    signal = signal.astype('float32').T\n",
    "\n",
    "    if not execution['whole_record']:\n",
    "        for j in range(1,len(detections_train[i])-1):\n",
    "            onset = detections_train[i][j-1]\n",
    "            offset = detections_train[i][j+1]\n",
    "            interp = signal[:,onset:offset]\n",
    "            interp = sp.interpolate.interp1d(np.linspace(0,1,interp.shape[1]),interp,axis=-1)(np.linspace(0,1,736)).astype('float32')\n",
    "            X_train.append(interp)\n",
    "            y_train.append(labels_train[i,:])\n",
    "    else:\n",
    "        X_train.append(signal)\n",
    "        y_train.append(labels_train[i,:])\n",
    "\n",
    "print(\"########## GENERATING TRAIN SET ##########\")\n",
    "for i in tqdm.tqdm(range(len(files_valid))):\n",
    "    # Retrieve the file information\n",
    "    (signal,_) = wfdb.rdsamp(os.path.join(execution['root_directory'],files_valid[i]))\n",
    "    signal = signal.astype('float32').T\n",
    "\n",
    "    if not execution['whole_record']:\n",
    "        for j in range(1,len(detections_valid[i])-1):\n",
    "            onset = detections_valid[i][j-1]\n",
    "            offset = detections_valid[i][j+1]\n",
    "            interp = signal[:,onset:offset]\n",
    "            interp = sp.interpolate.interp1d(np.linspace(0,1,interp.shape[1]),interp,axis=-1)(np.linspace(0,1,736)).astype('float32')\n",
    "            X_valid.append(interp)\n",
    "            y_valid.append(labels_valid[i,:])\n",
    "    else:\n",
    "        X_valid.append(signal)\n",
    "        y_valid.append(labels_valid[i,:])\n",
    "\n",
    "y_valid = np.array(y_valid, dtype='float32')\n",
    "y_train = np.array(y_train, dtype='float32')\n",
    "try:\n",
    "    X_train = np.array(X_train, dtype='float32')\n",
    "    X_valid = np.array(X_valid, dtype='float32')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "### TRAIN MODEL ###\n",
    "model = src.model.GAPModel(\n",
    "    torch.nn.Sequential(\n",
    "        src.model.CNN([12,16,16], regularization=execution['regularization_CNN']),\n",
    "        torch.nn.MaxPool1d(3),\n",
    "        src.model.CNN([16,16], regularization=execution['regularization_CNN']),\n",
    "        torch.nn.MaxPool1d(3),\n",
    "        src.model.CNN([16,32], regularization=execution['regularization_CNN']),\n",
    "        torch.nn.MaxPool1d(3),\n",
    "        src.model.CNN([32,32], regularization=execution['regularization_CNN']),\n",
    "        torch.nn.MaxPool1d(3),\n",
    "        src.model.CNN([32,64], regularization=execution['regularization_CNN']),\n",
    "        torch.nn.MaxPool1d(3),\n",
    "        src.model.CNN([64,128], regularization=execution['regularization_CNN']),\n",
    "        torch.nn.MaxPool1d(3),\n",
    "        src.model.CNN([128,256], regularization=execution['regularization_CNN'], regularize_extrema=False),\n",
    "    ),\n",
    "    src.model.DNN([256,128,64,32,9], regularization=execution['regularization_DNN'], regularize_extrema=False),\n",
    ")\n",
    "\n",
    "if execution['whole_record']:\n",
    "    dataset_train = src.data.PaddedDataset(X_train, y_train, padding_length=execution['padding_length'],swapaxes=False, mode='edge')\n",
    "    dataset_valid = src.data.PaddedDataset(X_valid, y_valid, padding_length=execution['padding_length'],swapaxes=False, mode='edge')\n",
    "else:\n",
    "    dataset_train = src.data.Dataset(X_train, y_train)\n",
    "    dataset_valid = src.data.Dataset(X_valid, y_valid)\n",
    "\n",
    "sampler_train = src.data.StratifiedSampler(y_train, *execution['sampler'])\n",
    "sampler_valid = src.data.StratifiedSampler(y_valid, *execution['sampler'])\n",
    "\n",
    "loader_train  = torch.utils.data.DataLoader(dataset_train, sampler=sampler_train, batch_size=execution['batch_size'], **execution['loader'])\n",
    "loader_valid  = torch.utils.data.DataLoader(dataset_valid, sampler=sampler_valid, batch_size=execution['batch_size'], **execution['loader'])\n",
    "\n",
    "# Loss\n",
    "criterion = lambda X,y,y_pred: torch.nn.MultiLabelSoftMarginLoss(reduction='mean')(y_pred, y.long())\n",
    "metric = lambda X,y,y_pred: src.evaluate.compute_beta_score(y.long().cpu().detach().numpy(),(torch.nn.functional.softmax(y_pred,-1) > 0.5).cpu().detach().numpy())[-1]\n",
    "\n",
    "state = {\n",
    "    'epoch'         : 0,\n",
    "    'device'        : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'optimizer'     : src.utils.class_selector('torch.optim',execution['optimizer']['name'])(model.parameters(), **execution['optimizer']['arguments']),\n",
    "    'root_dir'      : './'\n",
    "}\n",
    "if 'scheduler' in execution:\n",
    "    state['scheduler'] = src.utils.class_selector('torch.optim.lr_scheduler',execution['scheduler']['name'])(state['optimizer'], **execution['scheduler']['arguments'])\n",
    "\n",
    "print(\"########## TRAINING THE MODEL ##########\")\n",
    "state = src.train.train_model(model,state,execution,loader_train, loader_valid, criterion, metric, smaller=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boundary Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = skimage.segmentation.find_boundaries(y[:,0,None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_structure(rank, connectivity):\n",
    "    if connectivity < 1:\n",
    "        connectivity = 1\n",
    "    if rank < 1:\n",
    "        if connectivity < 1:\n",
    "            return np.array(0, dtype=bool)\n",
    "        else:\n",
    "            return np.array(1, dtype=bool)\n",
    "    output = np.fabs(np.indices([3] * rank) - 1)\n",
    "    output = np.add.reduce(output, 0)\n",
    "    \n",
    "    return np.asarray(output <= connectivity, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct = generate_binary_structure(ndim-1, 1)\n",
    "struct[0,:]=False\n",
    "struct[-1,:]=False\n",
    "struct = struct[None,None,...]\n",
    "struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selem = torch.tensor(selem).type(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "bnd = F.conv2d(label_img, selem, padding=(selem.shape[2] // 2, selem.shape[2] // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erosion1d(signal, selem):\n",
    "    inverted = torch.logical_not(signal).type(signal.dtype)\n",
    "    out = F.conv1d(inverted, selem, padding=(selem.shape[-1] // 2,)) > 0\n",
    "    return torch.logical_not(out)\n",
    "\n",
    "def dilation1d(signal, selem):\n",
    "    return F.conv1d(signal, selem, padding=(selem.shape[-1] // 2,)) > 0\n",
    "\n",
    "def erosion2d(image, selem):\n",
    "    inverted = torch.logical_not(image).type(image.dtype)\n",
    "    out = F.conv2d(inverted, selem, padding=(selem.shape[2] // 2, selem.shape[2] // 2)) > 0\n",
    "    return torch.logical_not(out)\n",
    "\n",
    "def dilation2d(image, selem):\n",
    "    return F.conv2d(image, selem, padding=(selem.shape[2] // 2, selem.shape[2] // 2)) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selem = np.zeros((3,)*(ndim-1),dtype=bool)\n",
    "selem[1,1,:] = True\n",
    "selem1 = np.zeros((3,)*(ndim-1),dtype=bool)\n",
    "selem1[0,0,:] = True\n",
    "selem1[1,1,:] = True\n",
    "selem1[2,2,:] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selem1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pbound = skimage.morphology.dilation(label_img[:,0,...].numpy(),selem.astype('bool')).squeeze()\n",
    "QRSbound = skimage.morphology.dilation(label_img[:,1,...].numpy(),selem.astype('bool')).squeeze()\n",
    "Tbound = skimage.morphology.dilation(label_img[:,2,...].numpy(),selem.astype('bool')).squeeze()\n",
    "out2 = np.concatenate((Pbound[:,None,:],QRSbound[:,None,:],Tbound[:,None,:]),axis=1)\n",
    "Per = skimage.morphology.erosion(label_img[:,0,...].numpy(),selem.astype('bool')).squeeze()\n",
    "QRSer = skimage.morphology.erosion(label_img[:,1,...].numpy(),selem.astype('bool')).squeeze()\n",
    "Ter = skimage.morphology.erosion(label_img[:,2,...].numpy(),selem.astype('bool')).squeeze()\n",
    "er2 = np.concatenate((Per[:,None,:],QRSer[:,None,:],Ter[:,None,:]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dilation1d(label_img.type(torch.float32).squeeze(),torch.tensor(selem1).type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = erosion1d(label_img.type(torch.float32).squeeze(),torch.tensor(selem1).type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "l = 1\n",
    "plt.plot(er[i,l,:])\n",
    "plt.plot(er2[i,l,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(out2.astype('bool'),out.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.ndimage.morphology.binary_dilation(label_img,selem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_img = y[:,:,None,None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnds = skimage.segmentation.find_boundaries(label_img).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.dilate(label_img.numpy(),selem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y[0,1,:])\n",
    "plt.plot(boundaries.squeeze()[0,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_img = y[:,:,None,:]\n",
    "connectivity=1\n",
    "mode='thick'\n",
    "background=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if label_img.dtype == torch.bool:\n",
    "    label_img = label_img.type(torch.uint8)\n",
    "ndim = label_img.ndim\n",
    "# selem = torch.tensor(generate_binary_structure(ndim, connectivity))\n",
    "selem = np.zeros((3,)*(ndim-1),dtype=bool)\n",
    "selem[1,1,1,:] = True\n",
    "if mode != 'subpixel':\n",
    "    boundaries = skimage.morphology.dilation(label_img, selem) != skimage.morphology.erosion(label_img, selem)\n",
    "    if mode == 'inner':\n",
    "        foreground_image = (label_img != background)\n",
    "        boundaries &= foreground_image\n",
    "    elif mode == 'outer':\n",
    "        max_label = torch.iinfo(label_img.dtype).max\n",
    "        background_image = (label_img == background)\n",
    "        selem = generate_binary_structure(ndim, ndim)\n",
    "        inverted_background = torch.tensor(label_img, copy=True)\n",
    "        inverted_background[background_image] = max_label\n",
    "        adjacent_objects = ((skimage.morphology.dilation(label_img, selem) !=\n",
    "                             skimage.morphology.erosion(inverted_background, selem)) &\n",
    "                            ~background_image)\n",
    "        boundaries &= (background_image | adjacent_objects)\n",
    "else:\n",
    "    boundaries = _find_boundaries_subpixel(label_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.morphology.dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.fabs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning3",
   "language": "python",
   "name": "deeplearning3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
