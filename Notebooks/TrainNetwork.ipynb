{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import skimage\n",
    "import skimage.segmentation\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import math\n",
    "import glob\n",
    "import uuid\n",
    "import random\n",
    "import platform\n",
    "import ecgdetectors\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import wfdb\n",
    "import json\n",
    "import tqdm\n",
    "import dill\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import src.data\n",
    "import utils\n",
    "import utils.wavelet\n",
    "import utils.data\n",
    "import utils.data.augmentation\n",
    "import utils.visualization\n",
    "import utils.visualization.plot\n",
    "import utils.torch\n",
    "import utils.torch.nn\n",
    "import utils.torch.nn as nn\n",
    "import utils.torch.loss\n",
    "import utils.torch.train\n",
    "import utils.torch.data\n",
    "import utils.torch.preprocessing\n",
    "import utils.torch.models\n",
    "import utils.torch.models.lego\n",
    "import utils.torch.models.variational\n",
    "import utils.torch.models.classification\n",
    "\n",
    "from utils.signal import StandardHeader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load execution configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./configurations/UNet4LevelsCrossEntropy.json', 'r') as f:\n",
    "#     execution = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data\n",
    "#### 1.1. Load individual segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = utils.pickleload(os.path.join('.','pickle','Psignal.pkl'))\n",
    "PQ = utils.pickleload(os.path.join('.','pickle','PQsignal.pkl'))\n",
    "QRS = utils.pickleload(os.path.join('.','pickle','QRSsignal.pkl'))\n",
    "ST = utils.pickleload(os.path.join('.','pickle','STsignal.pkl'))\n",
    "T = utils.pickleload(os.path.join('.','pickle','Tsignal.pkl'))\n",
    "TP = utils.pickleload(os.path.join('.','pickle','TPsignal.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Filter out faulty segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in (['104_II','104_III','104_AVR','104_AVF','103_III'] + \n",
    "            ['74_{}'.format(h) for h in StandardHeader] + \n",
    "            ['111_{}'.format(h) for h in StandardHeader] +\n",
    "            ['95_{}'.format(h) for h in StandardHeader] + \n",
    "            ['103_{}'.format(h) for h in StandardHeader] +\n",
    "            ['34_{}'.format(h) for h in StandardHeader]):\n",
    "    [P.pop(k) for k in list(P.keys()) if k.startswith('{}###'.format(key))]\n",
    "    [PQ.pop(k) for k in list(PQ.keys()) if k.startswith('{}###'.format(key))]\n",
    "    [QRS.pop(k) for k in list(QRS.keys()) if k.startswith('{}###'.format(key))]\n",
    "    [ST.pop(k) for k in list(ST.keys()) if k.startswith('{}###'.format(key))]\n",
    "    [T.pop(k) for k in list(T.keys()) if k.startswith('{}###'.format(key))]\n",
    "    [TP.pop(k) for k in list(TP.keys()) if k.startswith('{}###'.format(key))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in []: \n",
    "    if key in P: P.pop(key)\n",
    "for key in []: \n",
    "    if key in PQ: PQ.pop(key)\n",
    "for key in ['7_V2###0','7_V2###2','7_V2###4','95_V4###0','95_V4###1','95_V4###2']: \n",
    "    if key in QRS: QRS.pop(key)\n",
    "for key in []: \n",
    "    if key in ST: ST.pop(key)\n",
    "for key in []: \n",
    "    if key in T: T.pop(key)\n",
    "for key in (['52_III###4','34_V6###6','74_V4###0','74_V4###1','74_V4###2',\n",
    "             '74_V4###3','74_V4###4','74_V4###5','74_V4###6','74_V4###7',] + \n",
    "            ['111_V2###{}'.format(i) for i in range(7)]): \n",
    "    if key in TP: TP.pop(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Normalize amplitudes & get amplitude distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = np.max\n",
    "\n",
    "amplitudes = {k.split('###')[0]: [] for k in list(QRS)}\n",
    "for k in QRS:\n",
    "    g = k.split('###')[0]\n",
    "    segment = utils.signal.on_off_correction(QRS[k])\n",
    "    amplitudes[g].append(np.max(segment) - np.min(segment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pamplitudes = []\n",
    "for k in P:\n",
    "    segment = utils.signal.on_off_correction(P[k])/metric(amplitudes[k.split('###')[0]])\n",
    "    Pamplitudes.append(segment.max()-segment.min())\n",
    "Pamplitudes = np.array(Pamplitudes)\n",
    "PQamplitudes = []\n",
    "for k in PQ:\n",
    "    segment = utils.signal.on_off_correction(PQ[k])/metric(amplitudes[k.split('###')[0]])\n",
    "    PQamplitudes.append(segment.max()-segment.min())\n",
    "PQamplitudes = np.array(PQamplitudes)\n",
    "QRSamplitudes = []\n",
    "for k in QRS:\n",
    "    segment = utils.signal.on_off_correction(QRS[k])/metric(amplitudes[k.split('###')[0]])\n",
    "    QRSamplitudes.append(segment.max()-segment.min())\n",
    "QRSamplitudes = np.array(QRSamplitudes)\n",
    "STamplitudes = []\n",
    "for k in ST:\n",
    "    segment = utils.signal.on_off_correction(ST[k])/metric(amplitudes[k.split('###')[0]])\n",
    "    STamplitudes.append(segment.max()-segment.min())\n",
    "STamplitudes = np.array(STamplitudes)\n",
    "Tamplitudes = []\n",
    "for k in T:\n",
    "    segment = utils.signal.on_off_correction(T[k])/metric(amplitudes[k.split('###')[0]])\n",
    "    Tamplitudes.append(segment.max()-segment.min())\n",
    "Tamplitudes = np.array(Tamplitudes)\n",
    "TPamplitudes = []\n",
    "for k in TP:\n",
    "    segment = utils.signal.on_off_correction(TP[k])/metric(amplitudes[k.split('###')[0]])\n",
    "    TPamplitudes.append(segment.max()-segment.min())\n",
    "TPamplitudes = np.array(TPamplitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pamplitudes = scipy.stats.lognorm(*scipy.stats.lognorm.fit([Pamplitudes,2*Pamplitudes]))\n",
    "PQamplitudes = scipy.stats.lognorm(*scipy.stats.lognorm.fit(PQamplitudes[PQamplitudes<0.3]))\n",
    "QRSamplitudes = scipy.stats.lognorm(*scipy.stats.lognorm.fit(QRSamplitudes))\n",
    "STamplitudes = scipy.stats.lognorm(*scipy.stats.lognorm.fit(STamplitudes))\n",
    "Tamplitudes = scipy.stats.lognorm(*scipy.stats.lognorm.fit(Tamplitudes[(Tamplitudes>0.05) & (Tamplitudes<0.5)]))\n",
    "TPamplitudes = scipy.stats.lognorm(*scipy.stats.lognorm.fit(TPamplitudes[TPamplitudes<0.4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-0e724456727d>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  PQ[k] = segment/(np.max(segment)-np.min(segment))\n",
      "<ipython-input-10-0e724456727d>:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ST[k] = segment/(np.max(segment)-np.min(segment))\n"
     ]
    }
   ],
   "source": [
    "for k in list(P.keys()):\n",
    "    segment = utils.signal.on_off_correction(P[k])\n",
    "    P[k] = segment/(np.max(segment)-np.min(segment))\n",
    "    if np.any(np.isinf(P[k])) or np.any(np.isnan(P[k])):\n",
    "        P.pop(k)\n",
    "for k in list(PQ.keys()):\n",
    "    segment = utils.signal.on_off_correction(PQ[k])\n",
    "    PQ[k] = segment/(np.max(segment)-np.min(segment))\n",
    "    if np.any(np.isinf(PQ[k])) or np.any(np.isnan(PQ[k])):\n",
    "        PQ.pop(k)\n",
    "for k in list(QRS.keys()):\n",
    "    segment = utils.signal.on_off_correction(QRS[k])\n",
    "    QRS[k] = segment/(np.max(segment)-np.min(segment))\n",
    "    if np.any(np.isinf(QRS[k])) or np.any(np.isnan(QRS[k])):\n",
    "        QRS.pop(k)\n",
    "for k in list(ST.keys()):\n",
    "    segment = utils.signal.on_off_correction(ST[k])\n",
    "    ST[k] = segment/(np.max(segment)-np.min(segment))\n",
    "    if np.any(np.isinf(ST[k])) or np.any(np.isnan(ST[k])):\n",
    "        ST.pop(k)\n",
    "for k in list(T.keys()):\n",
    "    segment = utils.signal.on_off_correction(T[k])\n",
    "    T[k] = segment/(np.max(segment)-np.min(segment))\n",
    "    if np.any(np.isinf(T[k])) or np.any(np.isnan(T[k])):\n",
    "        T.pop(k)\n",
    "for k in list(TP.keys()):\n",
    "    segment = utils.signal.on_off_correction(TP[k])\n",
    "    TP[k] = segment/(np.max(segment)-np.min(segment))\n",
    "    if np.any(np.isinf(TP[k])) or np.any(np.isnan(TP[k])):\n",
    "        TP.pop(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = {}\n",
    "for k in list(P) + list(PQ) + list(QRS) + list(ST) + list(T) + list(TP):\n",
    "    uid = k.split('###')[0].split('_')[0].split('-')[0]\n",
    "    if uid not in all_keys:\n",
    "        all_keys[uid] = [k]\n",
    "    else:\n",
    "        all_keys[uid].append(k)\n",
    "        \n",
    "# Get database and file\n",
    "filenames = []\n",
    "database = []\n",
    "for k in all_keys:\n",
    "    filenames.append(k)\n",
    "    if k.startswith('SOO'):\n",
    "        database.append(0)\n",
    "    elif k.startswith('sel'):\n",
    "        database.append(1)\n",
    "    else:\n",
    "        database.append(2)\n",
    "filenames = np.array(filenames)\n",
    "database = np.array(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./configurations/UNet5Levels.json', 'r') as f:\n",
    "    execution = json.load(f)\n",
    "\n",
    "random.seed(execution['seed'])\n",
    "np.random.seed(execution['seed'])\n",
    "torch.random.manual_seed(execution['seed'])\n",
    "splitter = sklearn.model_selection.StratifiedKFold(5).split(filenames,database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix_train,ix_test in splitter:\n",
    "    train_keys, test_keys = ([],[])\n",
    "    for k in np.array(filenames)[ix_train]: train_keys += all_keys[k]\n",
    "    for k in np.array(filenames)[ix_test]:  test_keys += all_keys[k]\n",
    "    \n",
    "    # Divide train/test segments\n",
    "    Ptrain = {k: P[k] for k in P if k in train_keys}\n",
    "    PQtrain = {k: PQ[k] for k in PQ if k in train_keys}\n",
    "    QRStrain = {k: QRS[k] for k in QRS if k in train_keys}\n",
    "    STtrain = {k: ST[k] for k in ST if k in train_keys}\n",
    "    Ttrain = {k: T[k] for k in T if k in train_keys}\n",
    "    TPtrain = {k: TP[k] for k in TP if k in train_keys}\n",
    "\n",
    "    Ptest = {k: P[k] for k in P if k in test_keys}\n",
    "    PQtest = {k: PQ[k] for k in PQ if k in test_keys}\n",
    "    QRStest = {k: QRS[k] for k in QRS if k in test_keys}\n",
    "    STtest = {k: ST[k] for k in ST if k in test_keys}\n",
    "    Ttest = {k: T[k] for k in T if k in test_keys}\n",
    "    TPtest = {k: TP[k] for k in TP if k in test_keys}\n",
    "    \n",
    "    # dataset_train = src.data.Dataset(Ptrain, QRStrain, Ttrain, PQtrain, STtrain, TPtrain, \n",
    "    #                                  Pamplitudes, QRSamplitudes, Tamplitudes, PQamplitudes, \n",
    "    #                                  STamplitudes, TPamplitudes, 300*execution['loader']['batch_size'],\n",
    "    #                                  interp_std = 0.25,labels_as_masks=True)\n",
    "    # dataset_test = src.data.Dataset(Ptest, QRStest, Ttest, PQtest, STtest, TPtest, \n",
    "    #                                 Pamplitudes, QRSamplitudes, Tamplitudes, PQamplitudes, \n",
    "    #                                 STamplitudes, TPamplitudes, 100*execution['loader']['batch_size'],\n",
    "    #                                 interp_std = 0.25,labels_as_masks=True)\n",
    "\n",
    "    # # Create dataloaders\n",
    "    # loader_train = torch.utils.data.DataLoader(dataset_train,**execution['loader'])\n",
    "    # loader_test = torch.utils.data.DataLoader(dataset_test,**execution['loader'])\n",
    "\n",
    "    # # Define model\n",
    "    # model = nn.ModelGraph(execution['model']).float()\n",
    "    \n",
    "    # \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./configurations/UNet5Levels.json', 'r') as f:\n",
    "    execution = json.load(f)\n",
    "    \n",
    "execution['loader']['batch_size'] = 128\n",
    "\n",
    "# Define model\n",
    "model = nn.ModelGraph(execution['model']).float().cuda()\n",
    "\n",
    "# plt.figure(figsize=(15,15))\n",
    "# model.draw_networkx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = src.data.Dataset(Ptrain, QRStrain, Ttrain, PQtrain, STtrain, TPtrain, \n",
    "                                 Pamplitudes, QRSamplitudes, Tamplitudes, PQamplitudes, \n",
    "                                 STamplitudes, TPamplitudes, 300*128,\n",
    "                                 N = 2048, noise = 0.005, proba_no_P = 0.25,\n",
    "                                 proba_no_QRS = 0.01, proba_no_PQ = 0.15, \n",
    "                                 proba_no_ST = 0.15, proba_same_morph = 0.2,\n",
    "                                 proba_elevation = 0.2, proba_interpolation = 0.2,\n",
    "                                 proba_TV = 0.05, add_baseline_wander = True, \n",
    "                                 amplitude_std = 0.25, interp_std = 0.25,\n",
    "                                 window = 51, labels_as_masks = True)\n",
    "dataset_test = src.data.Dataset(Ptest, QRStest, Ttest, PQtest, STtest, TPtest, \n",
    "                                Pamplitudes, QRSamplitudes, Tamplitudes, PQamplitudes, \n",
    "                                STamplitudes, TPamplitudes, 100*128,\n",
    "                                N = 2048, noise = 0.005, proba_no_P = 0.25,\n",
    "                                proba_no_QRS = 0.01, proba_no_PQ = 0.15, \n",
    "                                proba_no_ST = 0.15, proba_same_morph = 0.2,\n",
    "                                proba_elevation = 0.2, proba_interpolation = 0.2,\n",
    "                                proba_TV = 0.05, add_baseline_wander = True, \n",
    "                                amplitude_std = 0.25, interp_std = 0.25,\n",
    "                                window = 51, labels_as_masks = True)\n",
    "\n",
    "# Create dataloaders\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train,**execution['loader'])\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test,**execution['loader'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.34 s, sys: 0 ns, total: 1.34 s\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for x,y in loader_test:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Train) Epoch   1/200, Loss      0.454: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch   1/200, Loss      0.464: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "/home/guille/VirtEnv/DeepLearning3/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
      "(Train) Epoch   2/200, Loss      0.246: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch   2/200, Loss      0.208: 100%|██████████| 100/100 [02:18<00:00,  1.38s/it]\n",
      "(Train) Epoch   3/200, Loss      0.209: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch   3/200, Loss      0.183: 100%|██████████| 100/100 [02:16<00:00,  1.36s/it]\n",
      "(Train) Epoch   4/200, Loss      0.208: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch   4/200, Loss      0.176: 100%|██████████| 100/100 [02:20<00:00,  1.40s/it]\n",
      "(Train) Epoch   5/200, Loss      0.226: 100%|██████████| 300/300 [08:04<00:00,  1.61s/it]\n",
      "(Valid) Epoch   5/200, Loss      0.174: 100%|██████████| 100/100 [02:18<00:00,  1.39s/it]\n",
      "(Train) Epoch   6/200, Loss      0.239: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch   6/200, Loss      0.165: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch   7/200, Loss      0.187: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch   7/200, Loss      0.156: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch   8/200, Loss      0.224: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch   8/200, Loss      0.160: 100%|██████████| 100/100 [02:16<00:00,  1.36s/it]\n",
      "(Train) Epoch   9/200, Loss      0.179: 100%|██████████| 300/300 [07:49<00:00,  1.57s/it]\n",
      "(Valid) Epoch   9/200, Loss      0.151: 100%|██████████| 100/100 [02:16<00:00,  1.36s/it]\n",
      "(Train) Epoch  10/200, Loss      0.205: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch  10/200, Loss      0.162: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  11/200, Loss      0.176: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch  11/200, Loss      0.168: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  12/200, Loss      0.273: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch  12/200, Loss      0.157: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  13/200, Loss      0.167: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  13/200, Loss      0.149: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch  14/200, Loss      0.164: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  14/200, Loss      0.145: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  15/200, Loss      0.177: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  15/200, Loss      0.148: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  16/200, Loss      0.170: 100%|██████████| 300/300 [07:45<00:00,  1.55s/it]\n",
      "(Valid) Epoch  16/200, Loss      0.154: 100%|██████████| 100/100 [02:15<00:00,  1.36s/it]\n",
      "(Train) Epoch  17/200, Loss      0.176: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch  17/200, Loss      0.143: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  18/200, Loss      0.218: 100%|██████████| 300/300 [07:46<00:00,  1.55s/it]\n",
      "(Valid) Epoch  18/200, Loss      0.148: 100%|██████████| 100/100 [02:16<00:00,  1.36s/it]\n",
      "(Train) Epoch  19/200, Loss      0.209: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch  19/200, Loss      0.146: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  20/200, Loss      0.180: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch  20/200, Loss      0.147: 100%|██████████| 100/100 [02:15<00:00,  1.36s/it]\n",
      "(Train) Epoch  21/200, Loss      0.154: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  21/200, Loss      0.136: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  22/200, Loss      0.235: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  22/200, Loss      0.172: 100%|██████████| 100/100 [02:16<00:00,  1.36s/it]\n",
      "(Train) Epoch  23/200, Loss      0.155: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch  23/200, Loss      0.140: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  24/200, Loss      0.191: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch  24/200, Loss      0.134: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  25/200, Loss      0.156: 100%|██████████| 300/300 [07:44<00:00,  1.55s/it]\n",
      "(Valid) Epoch  25/200, Loss      0.137: 100%|██████████| 100/100 [02:16<00:00,  1.36s/it]\n",
      "(Train) Epoch  26/200, Loss      0.174: 100%|██████████| 300/300 [07:45<00:00,  1.55s/it]\n",
      "(Valid) Epoch  26/200, Loss      0.143: 100%|██████████| 100/100 [02:15<00:00,  1.35s/it]\n",
      "(Train) Epoch  27/200, Loss      0.161: 100%|██████████| 300/300 [07:46<00:00,  1.56s/it]\n",
      "(Valid) Epoch  27/200, Loss      0.139: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  28/200, Loss      0.218: 100%|██████████| 300/300 [07:43<00:00,  1.54s/it]\n",
      "(Valid) Epoch  28/200, Loss      0.153: 100%|██████████| 100/100 [02:15<00:00,  1.36s/it]\n",
      "(Train) Epoch  29/200, Loss      0.162: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  29/200, Loss      0.144: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  30/200, Loss      0.157: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  30/200, Loss      0.129: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  31/200, Loss      0.188: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  31/200, Loss      0.129: 100%|██████████| 100/100 [02:16<00:00,  1.36s/it]\n",
      "(Train) Epoch  32/200, Loss      0.203: 100%|██████████| 300/300 [07:52<00:00,  1.58s/it]\n",
      "(Valid) Epoch  32/200, Loss      0.140: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch  33/200, Loss      0.165: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch  33/200, Loss      0.156: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  34/200, Loss      0.156: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch  34/200, Loss      0.136: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  35/200, Loss      0.151: 100%|██████████| 300/300 [07:49<00:00,  1.57s/it]\n",
      "(Valid) Epoch  35/200, Loss      0.147: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  36/200, Loss      0.229: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  36/200, Loss      0.138: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  37/200, Loss      0.154: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  37/200, Loss      0.126: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  38/200, Loss      0.152: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  38/200, Loss      0.139: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  39/200, Loss      0.193: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  39/200, Loss      0.134: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch  40/200, Loss      0.151: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch  40/200, Loss      0.131: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  41/200, Loss      0.207: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  41/200, Loss      0.128: 100%|██████████| 100/100 [02:18<00:00,  1.38s/it]\n",
      "(Train) Epoch  42/200, Loss      0.225: 100%|██████████| 300/300 [07:46<00:00,  1.56s/it]\n",
      "(Valid) Epoch  42/200, Loss      0.133: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  43/200, Loss      0.154: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch  43/200, Loss      0.127: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  44/200, Loss      0.184: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch  44/200, Loss      0.141: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  45/200, Loss      0.296: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  45/200, Loss      0.135: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch  46/200, Loss      0.150: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch  46/200, Loss      0.125: 100%|██████████| 100/100 [02:16<00:00,  1.36s/it]\n",
      "(Train) Epoch  47/200, Loss      0.149: 100%|██████████| 300/300 [07:49<00:00,  1.57s/it]\n",
      "(Valid) Epoch  47/200, Loss      0.127: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch  48/200, Loss      0.146: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch  48/200, Loss      0.132: 100%|██████████| 100/100 [02:16<00:00,  1.36s/it]\n",
      "(Train) Epoch  49/200, Loss      0.236: 100%|██████████| 300/300 [07:52<00:00,  1.57s/it]\n",
      "(Valid) Epoch  49/200, Loss      0.121: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  50/200, Loss      0.158: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  50/200, Loss      0.127: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  51/200, Loss      0.150: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  51/200, Loss      0.125: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch  52/200, Loss      0.152: 100%|██████████| 300/300 [07:53<00:00,  1.58s/it]\n",
      "(Valid) Epoch  52/200, Loss      0.131: 100%|██████████| 100/100 [02:18<00:00,  1.38s/it]\n",
      "(Train) Epoch  53/200, Loss      0.152: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  53/200, Loss      0.130: 100%|██████████| 100/100 [02:18<00:00,  1.39s/it]\n",
      "(Train) Epoch  54/200, Loss      0.154: 100%|██████████| 300/300 [07:49<00:00,  1.57s/it]\n",
      "(Valid) Epoch  54/200, Loss      0.116: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  55/200, Loss      0.144: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  55/200, Loss      0.119: 100%|██████████| 100/100 [02:15<00:00,  1.36s/it]\n",
      "(Train) Epoch  56/200, Loss      0.215: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch  56/200, Loss      0.133: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  57/200, Loss      0.162: 100%|██████████| 300/300 [07:49<00:00,  1.57s/it]\n",
      "(Valid) Epoch  57/200, Loss      0.130: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  58/200, Loss      0.215: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch  58/200, Loss      0.122: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  59/200, Loss      0.159: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  59/200, Loss      0.130: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  60/200, Loss      0.346: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch  60/200, Loss      0.125: 100%|██████████| 100/100 [02:16<00:00,  1.36s/it]\n",
      "(Train) Epoch  61/200, Loss      0.149: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  61/200, Loss      0.122: 100%|██████████| 100/100 [02:18<00:00,  1.38s/it]\n",
      "(Train) Epoch  62/200, Loss      0.149: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  62/200, Loss      0.127: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch  63/200, Loss      0.143: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  63/200, Loss      0.127: 100%|██████████| 100/100 [02:18<00:00,  1.39s/it]\n",
      "(Train) Epoch  64/200, Loss      0.140: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  64/200, Loss      0.126: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  65/200, Loss      0.131: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  65/200, Loss      0.123: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  66/200, Loss      0.134: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  66/200, Loss      0.121: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  67/200, Loss      0.207: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  67/200, Loss      0.125: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  68/200, Loss      0.216: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch  68/200, Loss      0.138: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  69/200, Loss      0.212: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch  69/200, Loss      0.119: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  70/200, Loss      0.171: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  70/200, Loss      0.119: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  71/200, Loss      0.132: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  71/200, Loss      0.132: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  72/200, Loss      0.137: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch  72/200, Loss      0.120: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch  73/200, Loss      0.149: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch  73/200, Loss      0.114: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  74/200, Loss      0.147: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch  74/200, Loss      0.121: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  75/200, Loss      0.136: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch  75/200, Loss      0.128: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  76/200, Loss      0.132: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  76/200, Loss      0.110: 100%|██████████| 100/100 [02:18<00:00,  1.38s/it]\n",
      "(Train) Epoch  77/200, Loss      0.213: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch  77/200, Loss      0.129: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  78/200, Loss      0.142: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch  78/200, Loss      0.122: 100%|██████████| 100/100 [02:18<00:00,  1.38s/it]\n",
      "(Train) Epoch  79/200, Loss      0.146: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch  79/200, Loss      0.110: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  80/200, Loss      0.156: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch  80/200, Loss      0.119: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch  81/200, Loss      0.149: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  81/200, Loss      0.118: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch  82/200, Loss      0.155: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch  82/200, Loss      0.124: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  83/200, Loss      0.138: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch  83/200, Loss      0.124: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  84/200, Loss      0.144: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch  84/200, Loss      0.117: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  85/200, Loss      0.151: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  85/200, Loss      0.111: 100%|██████████| 100/100 [02:16<00:00,  1.36s/it]\n",
      "(Train) Epoch  86/200, Loss      0.171: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  86/200, Loss      0.127: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  87/200, Loss      0.141: 100%|██████████| 300/300 [07:49<00:00,  1.57s/it]\n",
      "(Valid) Epoch  87/200, Loss      0.121: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  88/200, Loss      0.214: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  88/200, Loss      0.125: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  89/200, Loss      0.133: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  89/200, Loss      0.116: 100%|██████████| 100/100 [02:18<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Train) Epoch  90/200, Loss      0.131: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  90/200, Loss      0.111: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  91/200, Loss      0.309: 100%|██████████| 300/300 [07:54<00:00,  1.58s/it]\n",
      "(Valid) Epoch  91/200, Loss      0.110: 100%|██████████| 100/100 [02:19<00:00,  1.39s/it]\n",
      "(Train) Epoch  92/200, Loss      0.138: 100%|██████████| 300/300 [07:52<00:00,  1.58s/it]\n",
      "(Valid) Epoch  92/200, Loss      0.112: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  93/200, Loss      0.259: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch  93/200, Loss      0.107: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch  94/200, Loss      0.130: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  94/200, Loss      0.108: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch  95/200, Loss      0.128: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch  95/200, Loss      0.118: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  96/200, Loss      0.145: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch  96/200, Loss      0.134: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch  97/200, Loss      0.140: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch  97/200, Loss      0.135: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  98/200, Loss      0.125: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch  98/200, Loss      0.124: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch  99/200, Loss      0.214: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch  99/200, Loss      0.121: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 100/200, Loss      0.156: 100%|██████████| 300/300 [07:52<00:00,  1.58s/it]\n",
      "(Valid) Epoch 100/200, Loss      0.118: 100%|██████████| 100/100 [02:18<00:00,  1.38s/it]\n",
      "(Train) Epoch 101/200, Loss      0.136: 100%|██████████| 300/300 [07:52<00:00,  1.58s/it]\n",
      "(Valid) Epoch 101/200, Loss      0.132: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 102/200, Loss      0.130: 100%|██████████| 300/300 [07:52<00:00,  1.57s/it]\n",
      "(Valid) Epoch 102/200, Loss      0.115: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 103/200, Loss      0.136: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch 103/200, Loss      0.119: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 104/200, Loss      0.154: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 104/200, Loss      0.106: 100%|██████████| 100/100 [02:16<00:00,  1.36s/it]\n",
      "(Train) Epoch 105/200, Loss      0.132: 100%|██████████| 300/300 [07:49<00:00,  1.57s/it]\n",
      "(Valid) Epoch 105/200, Loss      0.104: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 106/200, Loss      0.360: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 106/200, Loss      0.135: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 107/200, Loss      0.143: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 107/200, Loss      0.122: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 108/200, Loss      0.133: 100%|██████████| 300/300 [07:52<00:00,  1.58s/it]\n",
      "(Valid) Epoch 108/200, Loss      0.116: 100%|██████████| 100/100 [02:18<00:00,  1.39s/it]\n",
      "(Train) Epoch 109/200, Loss      0.149: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 109/200, Loss      0.123: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 110/200, Loss      0.139: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch 110/200, Loss      0.126: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 111/200, Loss      0.134: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 111/200, Loss      0.121: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 112/200, Loss      0.214: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 112/200, Loss      0.135: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 113/200, Loss      0.215: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 113/200, Loss      0.129: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 114/200, Loss      0.134: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 114/200, Loss      0.111: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 115/200, Loss      0.130: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch 115/200, Loss      0.111: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 116/200, Loss      0.130: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch 116/200, Loss      0.118: 100%|██████████| 100/100 [02:18<00:00,  1.38s/it]\n",
      "(Train) Epoch 117/200, Loss      0.205: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 117/200, Loss      0.131: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 118/200, Loss      0.127: 100%|██████████| 300/300 [07:54<00:00,  1.58s/it]\n",
      "(Valid) Epoch 118/200, Loss      0.106: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 119/200, Loss      0.203: 100%|██████████| 300/300 [07:53<00:00,  1.58s/it]\n",
      "(Valid) Epoch 119/200, Loss      0.112: 100%|██████████| 100/100 [02:18<00:00,  1.38s/it]\n",
      "(Train) Epoch 120/200, Loss      0.140: 100%|██████████| 300/300 [07:52<00:00,  1.58s/it]\n",
      "(Valid) Epoch 120/200, Loss      0.107: 100%|██████████| 100/100 [02:18<00:00,  1.38s/it]\n",
      "(Train) Epoch 121/200, Loss      0.244: 100%|██████████| 300/300 [07:55<00:00,  1.58s/it]\n",
      "(Valid) Epoch 121/200, Loss      0.100: 100%|██████████| 100/100 [02:18<00:00,  1.39s/it]\n",
      "(Train) Epoch 122/200, Loss      0.135: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 122/200, Loss      0.115: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 123/200, Loss      0.190: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 123/200, Loss      0.103: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 124/200, Loss      0.133: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 124/200, Loss      0.116: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 125/200, Loss      0.126: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch 125/200, Loss      0.104: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 126/200, Loss      0.207: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 126/200, Loss      0.128: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 127/200, Loss      0.210: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch 127/200, Loss      0.129: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 128/200, Loss      0.313: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch 128/200, Loss      0.103: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 129/200, Loss      0.133: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch 129/200, Loss      0.106: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 130/200, Loss      0.144: 100%|██████████| 300/300 [07:52<00:00,  1.57s/it]\n",
      "(Valid) Epoch 130/200, Loss      0.116: 100%|██████████| 100/100 [02:18<00:00,  1.38s/it]\n",
      "(Train) Epoch 131/200, Loss      0.129: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch 131/200, Loss      0.118: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 132/200, Loss      0.221: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 132/200, Loss      0.119: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 133/200, Loss      0.192: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch 133/200, Loss      0.119: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 134/200, Loss      0.128: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 134/200, Loss      0.120: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 135/200, Loss      0.126: 100%|██████████| 300/300 [07:49<00:00,  1.57s/it]\n",
      "(Valid) Epoch 135/200, Loss      0.112: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 136/200, Loss      0.139: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 136/200, Loss      0.108: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 137/200, Loss      0.198: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 137/200, Loss      0.118: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 138/200, Loss      0.134: 100%|██████████| 300/300 [07:52<00:00,  1.57s/it]\n",
      "(Valid) Epoch 138/200, Loss      0.115: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 139/200, Loss      0.139: 100%|██████████| 300/300 [07:49<00:00,  1.57s/it]\n",
      "(Valid) Epoch 139/200, Loss      0.105: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 140/200, Loss      0.202: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 140/200, Loss      0.121: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 141/200, Loss      0.137: 100%|██████████| 300/300 [07:46<00:00,  1.55s/it]\n",
      "(Valid) Epoch 141/200, Loss      0.102: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 142/200, Loss      0.128: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch 142/200, Loss      0.126: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 143/200, Loss      0.142: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch 143/200, Loss      0.111: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 144/200, Loss      0.129: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 144/200, Loss      0.113: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 145/200, Loss      0.198: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 145/200, Loss      0.118: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 146/200, Loss      0.140: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch 146/200, Loss      0.114: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 147/200, Loss      0.124: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 147/200, Loss      0.118: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 148/200, Loss      0.352: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 148/200, Loss      0.118: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 149/200, Loss      0.203: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch 149/200, Loss      0.138: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 150/200, Loss      0.208: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 150/200, Loss      0.122: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 151/200, Loss      0.123: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 151/200, Loss      0.129: 100%|██████████| 100/100 [02:18<00:00,  1.38s/it]\n",
      "(Train) Epoch 152/200, Loss      0.124: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 152/200, Loss      0.118: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 153/200, Loss      0.129: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 153/200, Loss      0.114: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 154/200, Loss      0.205: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch 154/200, Loss      0.120: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 155/200, Loss      0.126: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 155/200, Loss      0.125: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 156/200, Loss      0.208: 100%|██████████| 300/300 [07:49<00:00,  1.57s/it]\n",
      "(Valid) Epoch 156/200, Loss      0.123: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 157/200, Loss      0.261: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch 157/200, Loss      0.113: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 158/200, Loss      0.154: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 158/200, Loss      0.107: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 159/200, Loss      0.130: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 159/200, Loss      0.100: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 160/200, Loss      0.142: 100%|██████████| 300/300 [07:52<00:00,  1.57s/it]\n",
      "(Valid) Epoch 160/200, Loss      0.104: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 161/200, Loss      0.187: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch 161/200, Loss      0.106: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 162/200, Loss      0.204: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 162/200, Loss      0.131: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 163/200, Loss      0.127: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch 163/200, Loss      0.120: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 164/200, Loss      0.133: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 164/200, Loss      0.114: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 165/200, Loss      0.147: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch 165/200, Loss      0.119: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 166/200, Loss      0.128: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 166/200, Loss      0.107: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 167/200, Loss      0.128: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch 167/200, Loss      0.105: 100%|██████████| 100/100 [02:18<00:00,  1.39s/it]\n",
      "(Train) Epoch 168/200, Loss      0.117: 100%|██████████| 300/300 [07:52<00:00,  1.57s/it]\n",
      "(Valid) Epoch 168/200, Loss      0.132: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 169/200, Loss      0.129: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 169/200, Loss      0.103: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 170/200, Loss      0.338: 100%|██████████| 300/300 [07:46<00:00,  1.56s/it]\n",
      "(Valid) Epoch 170/200, Loss      0.105: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 171/200, Loss      0.302: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 171/200, Loss      0.140: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 172/200, Loss      0.140: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch 172/200, Loss      0.115: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 173/200, Loss      0.203: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 173/200, Loss      0.119: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 174/200, Loss      0.126: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch 174/200, Loss      0.100: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 175/200, Loss      0.148: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch 175/200, Loss      0.111: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 176/200, Loss      0.128: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 176/200, Loss      0.114: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 177/200, Loss      0.202: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch 177/200, Loss      0.126: 100%|██████████| 100/100 [02:18<00:00,  1.39s/it]\n",
      "(Train) Epoch 178/200, Loss      0.119: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 178/200, Loss      0.124: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 179/200, Loss      0.121: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 179/200, Loss      0.116: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 180/200, Loss      0.212: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch 180/200, Loss      0.130: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Train) Epoch 181/200, Loss      0.273: 100%|██████████| 300/300 [07:49<00:00,  1.57s/it]\n",
      "(Valid) Epoch 181/200, Loss      0.106: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 182/200, Loss      0.299: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch 182/200, Loss      0.106: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 183/200, Loss      0.142: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 183/200, Loss      0.120: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 184/200, Loss      0.119: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 184/200, Loss      0.096: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 185/200, Loss      0.118: 100%|██████████| 300/300 [07:49<00:00,  1.57s/it]\n",
      "(Valid) Epoch 185/200, Loss      0.102: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 186/200, Loss      0.135: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 186/200, Loss      0.107: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 187/200, Loss      0.132: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 187/200, Loss      0.114: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 188/200, Loss      0.313: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 188/200, Loss      0.113: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 189/200, Loss      0.126: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 189/200, Loss      0.115: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 190/200, Loss      0.355: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch 190/200, Loss      0.119: 100%|██████████| 100/100 [02:15<00:00,  1.36s/it]\n",
      "(Train) Epoch 191/200, Loss      0.123: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch 191/200, Loss      0.113: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 192/200, Loss      0.132: 100%|██████████| 300/300 [07:49<00:00,  1.57s/it]\n",
      "(Valid) Epoch 192/200, Loss      0.105: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 193/200, Loss      0.127: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch 193/200, Loss      0.127: 100%|██████████| 100/100 [02:16<00:00,  1.36s/it]\n",
      "(Train) Epoch 194/200, Loss      0.270: 100%|██████████| 300/300 [07:47<00:00,  1.56s/it]\n",
      "(Valid) Epoch 194/200, Loss      0.118: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 195/200, Loss      0.122: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch 195/200, Loss      0.104: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 196/200, Loss      0.193: 100%|██████████| 300/300 [07:51<00:00,  1.57s/it]\n",
      "(Valid) Epoch 196/200, Loss      0.124: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n",
      "(Train) Epoch 197/200, Loss      0.132: 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n",
      "(Valid) Epoch 197/200, Loss      0.104: 100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
      "(Train) Epoch 198/200, Loss      0.197: 100%|██████████| 300/300 [07:49<00:00,  1.57s/it]\n",
      "(Valid) Epoch 198/200, Loss      0.144: 100%|██████████| 100/100 [02:18<00:00,  1.39s/it]\n",
      "(Train) Epoch 199/200, Loss      0.243: 100%|██████████| 300/300 [07:49<00:00,  1.56s/it]\n",
      "(Valid) Epoch 199/200, Loss      0.105: 100%|██████████| 100/100 [02:17<00:00,  1.37s/it]\n",
      "(Train) Epoch 200/200, Loss      0.307: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n",
      "(Valid) Epoch 200/200, Loss      0.108: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "### Loss\n",
    "# criterion = lambda X,y,y_pred: utils.torch.loss.CrossEntropyLoss()(y_pred, y.long())\n",
    "# metric = lambda X,y,y_pred: utils.torch.loss.CrossEntropyLoss()(y_pred, y.long())\n",
    "criterion = lambda X,y,y_pred: utils.torch.loss.DiceLoss()(y_pred, y)\n",
    "metric = lambda X,y,y_pred: utils.torch.loss.DiceLoss()(y_pred, y)\n",
    "\n",
    "state = {\n",
    "    'epoch'         : 0,\n",
    "    'device'        : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'optimizer'     : utils.class_selector('torch.optim',execution['optimizer']['class'])(model.parameters(), **execution['optimizer']['arguments']),\n",
    "    'root_dir'      : './'\n",
    "}\n",
    "if 'scheduler' in execution:\n",
    "    state['scheduler'] = utils.class_selector('torch.optim.lr_scheduler',execution['scheduler']['class'])(state['optimizer'], **execution['scheduler']['arguments'])\n",
    "\n",
    "state = utils.torch.train.train_model(model,\n",
    "                                      state,\n",
    "                                      execution,\n",
    "                                      loader_train,\n",
    "                                      loader_test,\n",
    "                                      criterion,\n",
    "                                      metric,\n",
    "                                      smaller=True)\n",
    "\n",
    "# Save model\n",
    "torch.save(model,'./modelo5nivsdiceversion5.state',pickle_module=dill)\n",
    "utils.pickledump(\"\"\"dataset_train = src.data.Dataset(Ptrain, QRStrain, Ttrain, PQtrain, STtrain, TPtrain, \n",
    "                                 Pamplitudes, QRSamplitudes, Tamplitudes, PQamplitudes, \n",
    "                                 STamplitudes, TPamplitudes, 300*128,\n",
    "                                 N = 2048, noise = 0.005, proba_no_P = 0.25,\n",
    "                                 proba_no_QRS = 0.01, proba_no_PQ = 0.15, \n",
    "                                 proba_no_ST = 0.15, proba_same_morph = 0.2,\n",
    "                                 proba_elevation = 0.2, proba_interpolation = 0.2,\n",
    "                                 proba_TV = 0.05, add_baseline_wander = True, \n",
    "                                 amplitude_std = 0.25, interp_std = 0.25,\n",
    "                                 window = 51, labels_as_masks = True)\n",
    "dataset_test = src.data.Dataset(Ptest, QRStest, Ttest, PQtest, STtest, TPtest, \n",
    "                                Pamplitudes, QRSamplitudes, Tamplitudes, PQamplitudes, \n",
    "                                STamplitudes, TPamplitudes, 100*128,\n",
    "                                N = 2048, noise = 0.005, proba_no_P = 0.25,\n",
    "                                proba_no_QRS = 0.01, proba_no_PQ = 0.15, \n",
    "                                proba_no_ST = 0.15, proba_same_morph = 0.2,\n",
    "                                proba_elevation = 0.2, proba_interpolation = 0.2,\n",
    "                                proba_TV = 0.05, add_baseline_wander = True, \n",
    "                                amplitude_std = 0.25, interp_std = 0.25,\n",
    "                                window = 51, labels_as_masks = True)\"\"\",'./modelo5nivsdiceversion5_datasets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7_V3',\n",
       " '7_V5',\n",
       " '21_V3',\n",
       " '40_I',\n",
       " '29_I',\n",
       " '19_I',\n",
       " '28_AVR',\n",
       " '4_V2',\n",
       " '26_V1',\n",
       " '30_AVF',\n",
       " '40_AVL',\n",
       " '3_V1',\n",
       " '25_I',\n",
       " 'SOO57-2-1',\n",
       " '12_V6',\n",
       " '18_V6',\n",
       " '26_AVL',\n",
       " '28_AVF',\n",
       " '4_V5',\n",
       " '14_I',\n",
       " 'SOO1-1-1',\n",
       " '19_II',\n",
       " '4_AVL',\n",
       " '33_V1',\n",
       " '31_AVL',\n",
       " '31_I',\n",
       " '13_V5',\n",
       " '42_AVR',\n",
       " '13_AVF',\n",
       " '1_V6',\n",
       " 'SOO33-1-1',\n",
       " 'sel16265_0',\n",
       " '1_AVL',\n",
       " '32_V1',\n",
       " '4_I',\n",
       " '40_AVR',\n",
       " '1_V1',\n",
       " '29_V4',\n",
       " '24_III',\n",
       " '19_AVF',\n",
       " '39_V6',\n",
       " 'SOO53-1-1',\n",
       " '25_V6',\n",
       " '42_V4',\n",
       " '5_V5',\n",
       " 'sel16483_0',\n",
       " 'sel17152_0',\n",
       " '29_V5',\n",
       " '5_AVF',\n",
       " '5_V2',\n",
       " 'sel14172_1',\n",
       " '14_V6',\n",
       " '32_AVF',\n",
       " '10_AVR',\n",
       " 'SOO21-1-1',\n",
       " '22_I',\n",
       " '11_V5',\n",
       " '12_V2',\n",
       " '10_V1',\n",
       " '3_II',\n",
       " '32_V3',\n",
       " '43_I',\n",
       " '30_III',\n",
       " '23_II',\n",
       " '15_V4',\n",
       " '19_V3',\n",
       " 'sel14172_0',\n",
       " '18_I',\n",
       " '25_V3',\n",
       " '26_V6',\n",
       " '14_III',\n",
       " '42_V2',\n",
       " '42_III',\n",
       " '27_V4',\n",
       " '26_V2',\n",
       " '39_V5',\n",
       " '40_V1',\n",
       " '27_II',\n",
       " 'SOO50-1-1',\n",
       " '17_V6',\n",
       " 'SOO7-1-1',\n",
       " 'sel16786_0',\n",
       " '14_AVF',\n",
       " '28_V2',\n",
       " 'sel16272_0',\n",
       " '11_III',\n",
       " 'sel16420_0',\n",
       " '2_V4',\n",
       " 'sel123_0',\n",
       " '15_V1',\n",
       " '12_V3',\n",
       " '24_V4',\n",
       " '26_AVR',\n",
       " '33_V2',\n",
       " '40_AVF',\n",
       " '36_AVL',\n",
       " 'SOO4-2-1',\n",
       " 'SOO16-1-1',\n",
       " 'SOO41-1-1',\n",
       " '20_AVF',\n",
       " '22_AVF',\n",
       " '17_II',\n",
       " '12_I',\n",
       " '14_V1',\n",
       " '30_V4',\n",
       " '6_V5',\n",
       " '32_II',\n",
       " 'SOO52-1-1',\n",
       " '15_V3',\n",
       " '18_V2',\n",
       " '28_V1',\n",
       " 'SOO18-1-1',\n",
       " '14_II',\n",
       " '17_III',\n",
       " 'sel17453_1',\n",
       " '1_II',\n",
       " '37_I',\n",
       " '31_AVR',\n",
       " '5_II',\n",
       " '21_AVF',\n",
       " '36_V1',\n",
       " '6_V1',\n",
       " '40_V6',\n",
       " '4_III',\n",
       " '2_V6',\n",
       " '41_V1',\n",
       " 'SOO27-1-1',\n",
       " '9_AVF',\n",
       " '42_I',\n",
       " '5_V3',\n",
       " '10_I',\n",
       " '3_V2',\n",
       " '31_V1',\n",
       " '14_V4',\n",
       " 'sel16265_1',\n",
       " '27_V6',\n",
       " '12_AVR',\n",
       " '36_III',\n",
       " '20_II',\n",
       " 'SOO51-1-1',\n",
       " '26_I',\n",
       " '25_AVF',\n",
       " '7_V4',\n",
       " 'SOO18-1-2',\n",
       " '20_V2',\n",
       " '36_V6',\n",
       " 'sel15814_0',\n",
       " 'SOO17-1-1',\n",
       " '1_III',\n",
       " '7_V6',\n",
       " '29_AVL',\n",
       " '13_V6',\n",
       " 'SOO65-1-1',\n",
       " '31_V3',\n",
       " '33_V4',\n",
       " 'sel114_1',\n",
       " 'SOO31-1-1',\n",
       " '23_I',\n",
       " '40_V4',\n",
       " '9_I',\n",
       " '16_III',\n",
       " '39_V2',\n",
       " '17_V5',\n",
       " '28_V3',\n",
       " 'sel16273_1',\n",
       " '17_AVF',\n",
       " '28_V6',\n",
       " '1_V2',\n",
       " '15_II',\n",
       " '12_V1',\n",
       " '25_V2',\n",
       " '3_AVL',\n",
       " 'SOO35-1-1',\n",
       " 'sel16795_0',\n",
       " '41_V3',\n",
       " '40_III',\n",
       " '15_V2',\n",
       " '33_AVL',\n",
       " '30_V1',\n",
       " '23_AVR',\n",
       " 'SOO23-1-1',\n",
       " 'SOO54-1-1',\n",
       " '14_AVR',\n",
       " '42_V5',\n",
       " '19_AVL',\n",
       " '21_AVR',\n",
       " '36_V4',\n",
       " '39_V1',\n",
       " '31_V6',\n",
       " '42_II',\n",
       " '13_I',\n",
       " 'SOO59-1-1',\n",
       " 'SOO55-1-1',\n",
       " '32_V5',\n",
       " 'SOO58-1-1',\n",
       " '19_V2',\n",
       " 'SOO49-1-1',\n",
       " '20_III',\n",
       " '39_III',\n",
       " '29_V1',\n",
       " '3_V4',\n",
       " '25_III',\n",
       " '3_V3',\n",
       " '41_AVR',\n",
       " '31_V4',\n",
       " '9_AVR',\n",
       " '22_V6',\n",
       " '27_AVF',\n",
       " '2_AVR',\n",
       " 'SOO60-1-1',\n",
       " 'SOO63-1-1',\n",
       " '10_V2',\n",
       " '2_III',\n",
       " '13_III',\n",
       " '27_V2',\n",
       " '29_V6',\n",
       " '19_V4',\n",
       " '5_III',\n",
       " '22_II',\n",
       " '24_AVF',\n",
       " '32_V6',\n",
       " 'sel14157_0',\n",
       " 'sel16272_1',\n",
       " '11_AVL',\n",
       " '32_AVR',\n",
       " '31_III',\n",
       " 'sel17152_1',\n",
       " '6_III',\n",
       " '2_V2',\n",
       " '5_V6',\n",
       " '2_I',\n",
       " '10_V5',\n",
       " '18_AVF',\n",
       " '39_V3',\n",
       " '12_AVF',\n",
       " 'SOO46-1-1',\n",
       " '17_V3',\n",
       " '19_V6',\n",
       " '27_V5',\n",
       " 'SOO22-1-1',\n",
       " '26_V5',\n",
       " 'SOO56-1-1',\n",
       " '30_V2',\n",
       " '7_V2',\n",
       " '7_III',\n",
       " '26_AVF',\n",
       " '30_V3',\n",
       " 'sel16786_1',\n",
       " '13_AVR',\n",
       " '4_V1',\n",
       " '6_I',\n",
       " '10_AVF',\n",
       " '36_I',\n",
       " 'sel16483_1',\n",
       " '17_V4',\n",
       " '23_III',\n",
       " '16_V6',\n",
       " '36_AVF',\n",
       " '12_V4',\n",
       " 'sel16795_1',\n",
       " 'SOO2-1-1',\n",
       " '4_V3',\n",
       " '43_V3',\n",
       " '16_I',\n",
       " '31_AVF',\n",
       " '13_V1',\n",
       " '4_V6',\n",
       " '6_V2',\n",
       " '16_II',\n",
       " '41_I',\n",
       " '43_V4',\n",
       " 'sel16539_1',\n",
       " '3_V5',\n",
       " '11_V1',\n",
       " '12_III',\n",
       " 'sel17453_0',\n",
       " '37_AVL',\n",
       " '3_I',\n",
       " '3_AVF',\n",
       " '29_II',\n",
       " '11_II',\n",
       " '9_V4',\n",
       " 'SOO29-1-1',\n",
       " '19_V1',\n",
       " '42_V1',\n",
       " '43_AVL',\n",
       " '33_V5',\n",
       " '33_V6',\n",
       " '2_AVL',\n",
       " 'sel116_1',\n",
       " 'sel14046_0',\n",
       " '16_AVF',\n",
       " 'sel117_1',\n",
       " '2_V1',\n",
       " 'SOO14-1-1',\n",
       " 'SOO3-1-1',\n",
       " '20_V4',\n",
       " 'SOO47-1-1',\n",
       " '6_V6',\n",
       " '24_I',\n",
       " 'SOO26-1-1',\n",
       " '30_AVR',\n",
       " '1_I',\n",
       " '24_V2',\n",
       " 'SOO39-1-1',\n",
       " 'SOO44-1-1',\n",
       " 'sel14046_1',\n",
       " '43_V6',\n",
       " '15_AVR',\n",
       " '28_V5',\n",
       " '32_III',\n",
       " '2_AVF',\n",
       " '4_AVR',\n",
       " 'SOO25-1-1',\n",
       " '3_AVR',\n",
       " '20_V3',\n",
       " '16_V4',\n",
       " '17_AVR',\n",
       " '25_AVR',\n",
       " '37_V1',\n",
       " '22_V5',\n",
       " '26_V4',\n",
       " '26_II',\n",
       " '36_V5',\n",
       " '37_AVR',\n",
       " 'SOO43-1-1',\n",
       " '18_V5',\n",
       " '29_V3',\n",
       " '9_V2',\n",
       " '9_III',\n",
       " '28_III',\n",
       " '32_AVL',\n",
       " '41_AVL',\n",
       " '17_V1',\n",
       " '30_V6',\n",
       " '40_V5',\n",
       " '7_II',\n",
       " '41_II',\n",
       " 'sel16273_0',\n",
       " '20_AVR',\n",
       " 'sel104_0',\n",
       " '23_V6',\n",
       " 'SOO5-1-1',\n",
       " '37_V5',\n",
       " '9_V5',\n",
       " '21_V4',\n",
       " '1_V3',\n",
       " '41_V6',\n",
       " '13_AVL',\n",
       " 'SOO24-1-1',\n",
       " '28_I',\n",
       " '42_AVL',\n",
       " '18_V1',\n",
       " '18_AVR',\n",
       " '41_III',\n",
       " '20_V6',\n",
       " '7_AVR',\n",
       " '7_AVF',\n",
       " '1_AVR',\n",
       " '32_I',\n",
       " '27_III',\n",
       " '21_III',\n",
       " '7_AVL',\n",
       " '11_V4',\n",
       " '43_II',\n",
       " '9_AVL',\n",
       " '43_AVR',\n",
       " '39_I',\n",
       " '37_AVF',\n",
       " '19_V5',\n",
       " '13_V3',\n",
       " '14_V5',\n",
       " '27_AVR',\n",
       " '22_V2',\n",
       " '43_V5',\n",
       " '32_V4',\n",
       " '3_V6',\n",
       " 'SOO19-1-1',\n",
       " '23_V3',\n",
       " '4_AVF',\n",
       " 'SOO10-1-1',\n",
       " '14_AVL',\n",
       " '43_AVF',\n",
       " '26_III',\n",
       " '22_AVR',\n",
       " '24_V1',\n",
       " '16_V3',\n",
       " '43_III',\n",
       " '2_V3',\n",
       " '39_II',\n",
       " '42_V6',\n",
       " '41_V5',\n",
       " '27_V3',\n",
       " '30_I',\n",
       " '20_V1',\n",
       " '5_AVL',\n",
       " '24_V5',\n",
       " '42_V3',\n",
       " 'SOO8-1-1',\n",
       " '37_V6',\n",
       " '15_V6',\n",
       " '31_II',\n",
       " '18_II',\n",
       " '13_II',\n",
       " '29_AVF',\n",
       " '13_V2',\n",
       " '24_V6',\n",
       " '31_V2',\n",
       " '30_II',\n",
       " '33_II',\n",
       " '6_AVF',\n",
       " '41_AVF',\n",
       " '18_V3',\n",
       " 'SOO15-1-1',\n",
       " 'SOO28-1-1',\n",
       " 'sel100_0',\n",
       " '28_AVL',\n",
       " 'sel16420_1',\n",
       " 'sel100_1',\n",
       " '39_AVF',\n",
       " '21_V5',\n",
       " '40_V2',\n",
       " '12_AVL',\n",
       " 'SOO49-1-2',\n",
       " '15_AVL',\n",
       " '9_II',\n",
       " '43_V1',\n",
       " '10_AVL',\n",
       " '17_AVL',\n",
       " '17_I',\n",
       " '37_V3',\n",
       " '4_V4',\n",
       " '17_V2',\n",
       " '4_II',\n",
       " 'SOO9-1-1',\n",
       " '1_V4',\n",
       " '21_II',\n",
       " '33_AVR',\n",
       " '39_AVL',\n",
       " '23_V4',\n",
       " '36_II',\n",
       " '36_V2',\n",
       " '18_V4',\n",
       " '37_V2',\n",
       " '11_AVF',\n",
       " '29_AVR',\n",
       " '16_AVL',\n",
       " '5_AVR',\n",
       " '21_V2',\n",
       " '23_AVF',\n",
       " '21_I',\n",
       " '15_V5',\n",
       " '37_II',\n",
       " '33_III',\n",
       " '24_V3',\n",
       " 'sel123_1',\n",
       " '30_AVL',\n",
       " 'SOO13-1-1',\n",
       " '10_V6',\n",
       " '16_V5',\n",
       " '24_AVL',\n",
       " 'SOO12-1-1',\n",
       " '16_AVR',\n",
       " 'sel104_1',\n",
       " 'SOO6-2-1',\n",
       " '39_V4',\n",
       " '12_V5',\n",
       " '2_II',\n",
       " '2_V5',\n",
       " '3_III',\n",
       " '20_I',\n",
       " 'sel14157_1',\n",
       " '6_V3',\n",
       " '25_AVL',\n",
       " 'sel16773_1',\n",
       " '6_AVR',\n",
       " '26_V3',\n",
       " 'sel16539_0',\n",
       " '5_V1',\n",
       " '41_V4',\n",
       " '27_AVL',\n",
       " '27_I',\n",
       " '16_V1',\n",
       " '22_V4',\n",
       " '11_V3',\n",
       " '19_AVR',\n",
       " '25_II',\n",
       " '9_V1',\n",
       " '6_II',\n",
       " '5_V4',\n",
       " '12_II',\n",
       " 'sel116_0',\n",
       " 'SOO20-1-1',\n",
       " '37_III',\n",
       " '37_V4',\n",
       " 'sel114_0',\n",
       " '15_III',\n",
       " '11_V6',\n",
       " '21_V1',\n",
       " '29_III',\n",
       " '22_III',\n",
       " 'sel15814_1',\n",
       " '25_V1',\n",
       " '40_II',\n",
       " '10_V3',\n",
       " '32_V2',\n",
       " '36_V3',\n",
       " '22_V1',\n",
       " '15_I',\n",
       " '19_III',\n",
       " '10_V4',\n",
       " '14_V3',\n",
       " '5_I',\n",
       " '22_V3',\n",
       " '9_V3',\n",
       " 'SOO61-1-1',\n",
       " '27_V1',\n",
       " '29_V2',\n",
       " '7_V1',\n",
       " '30_V5',\n",
       " '33_I',\n",
       " '15_AVF',\n",
       " '33_V3',\n",
       " '6_V4',\n",
       " '1_V5',\n",
       " '20_AVL',\n",
       " '21_AVL',\n",
       " 'SOO4-1-1',\n",
       " '23_AVL',\n",
       " '28_V4',\n",
       " '39_AVR',\n",
       " '42_AVF',\n",
       " '18_AVL',\n",
       " '31_V5',\n",
       " '1_AVF',\n",
       " '11_AVR',\n",
       " '23_V2',\n",
       " '14_V2',\n",
       " '36_AVR',\n",
       " '25_V4',\n",
       " 'SOO32-1-1',\n",
       " '25_V5',\n",
       " '28_II',\n",
       " '24_AVR',\n",
       " '24_II',\n",
       " '41_V2',\n",
       " '6_AVL',\n",
       " '21_V6',\n",
       " '11_I',\n",
       " '11_V2',\n",
       " '43_V2',\n",
       " '7_I',\n",
       " 'sel16773_0',\n",
       " '10_II',\n",
       " '22_AVL',\n",
       " '9_V6',\n",
       " '23_V5',\n",
       " 'SOO34-1-1',\n",
       " '40_V3',\n",
       " '20_V5',\n",
       " '33_AVF',\n",
       " '10_III',\n",
       " '13_V4',\n",
       " '16_V2',\n",
       " '23_V1',\n",
       " 'sel117_0',\n",
       " '18_III']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([k.split('###')[0] for k in test_keys]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.33 s, sys: 4 ms, total: 1.33 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for x,y in loader_test:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agkljjsgkldjs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3c7f100769dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magkljjsgkldjs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'agkljjsgkldjs' is not defined"
     ]
    }
   ],
   "source": [
    "agkljjsgkldjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(out,) = model(x.cuda())\n",
    "y2 = torch.clone(y)\n",
    "y = torch.zeros((64,3,2048),dtype=bool)\n",
    "y[:,0,:] = (y2 == 1)\n",
    "y[:,1,:] = (y2 == 2)\n",
    "y[:,2,:] = (y2 == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "w = 3\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x[i,0,:])\n",
    "plt.twinx()\n",
    "plt.plot(y[i,w-1,:],alpha=0.5)\n",
    "plt.plot(out[i,w,:].cpu().detach().numpy(),alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test against SoO db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./configurations/UNet5Levels.json', 'r') as f:\n",
    "    execution = json.load(f)\n",
    "\n",
    "# Define model\n",
    "# model = nn.ModelGraph(execution['model']).float().cuda()\n",
    "model = torch.load('./modelo5nivsdiceVTamplitude0-35interpolation200epochs.state').eval().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD DATASET ####\n",
    "basedir = '/media/guille/DADES/DADES/Delineator/'\n",
    "Files = os.listdir(os.path.join(basedir,'SoO','RETAG'))\n",
    "Files = [os.path.splitext(f)[0] for f in Files if os.path.splitext(f)[1] == '.txt']\n",
    "Segmentations = pd.read_csv(os.path.join(basedir,'SoO','SEGMENTATIONS.csv'),index_col=0,header=None).T\n",
    "Keys = Segmentations.keys().tolist()\n",
    "Keys = [k for k in Keys if '-'.join(k.split('-')[:2]) in Files]\n",
    "database = pd.read_csv(os.path.join(basedir,'SoO','DATABASE_MANUAL.csv'))\n",
    "\n",
    "# Data storage\n",
    "QRSsignalSoO = dict()\n",
    "QRSgroupSoO = dict()\n",
    "\n",
    "for k in tqdm.tqdm(Keys):\n",
    "    # Retrieve general information\n",
    "    fname = '-'.join(k.split('-')[:2]) + '.txt'\n",
    "    ID = int(k.split('-')[0])\n",
    "    \n",
    "    # Read signal and segmentation\n",
    "    Signal = pd.read_csv(os.path.join(basedir,'SoO','RETAG',fname),index_col=0).values\n",
    "    (son,soff) = Segmentations[k]\n",
    "    fs = database['Sampling_Freq'][database['ID'] == int(ID)].values[0]\n",
    "    \n",
    "    # Check correct segmentation\n",
    "    if son > soff:\n",
    "        print(\"(!!!) Check file   {:>10s} has onset ({:d}) > offset ({:d})\".format(k, son, soff))\n",
    "        continue\n",
    "\n",
    "    # Up/downsample to 1000 Hz\n",
    "    factor = int(fs/250)\n",
    "    Signal = np.round(sp.signal.decimate(Signal.T, factor)).T\n",
    "    fs = fs/factor\n",
    "\n",
    "    # Filter baseline wander and high freq. noise\n",
    "    Signal = sp.signal.filtfilt(*sp.signal.butter(4,   0.5/fs, 'high'),Signal.T).T\n",
    "    Signal = sp.signal.filtfilt(*sp.signal.butter(4, 125.0/fs,  'low'),Signal.T).T\n",
    "    \n",
    "    for i in range(len(StandardHeader)):\n",
    "        # Store data\n",
    "        QRSsignalSoO[k+'###'+str(StandardHeader[i])] = Signal[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "def moving_amplitude(x, w):\n",
    "    return [np.max(x[i:i+w])-np.min(x[i:i+w]) for i in range(0,len(x),w)]\n",
    "\n",
    "def moving_operation(x, w, operation):\n",
    "    return [operation(x[i:i+w]) for i in range(0,len(x),w)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = QRSsignalSoO['49-1-1###III']\n",
    "ampl = np.median(moving_operation(tmp,200,lambda x: np.max(x)-np.min(x)))\n",
    "tmp = scipy.interpolate.interp1d(np.linspace(0,1,tmp.size),tmp)(np.linspace(0,1,1.0*tmp.size))\n",
    "aaa = (skimage.util.view_as_windows(tmp/ampl,2048,1024)-0)[:,None,:]\n",
    "bbb = torch.zeros((aaa.shape[0],3,2048),dtype=float)\n",
    "for i in range(0,aaa.shape[0],64):\n",
    "    bbb[i:i+64] = model(torch.tensor(aaa[i:i+64]).cuda().float())[0]\n",
    "bbb = bbb.cpu().detach().numpy()\n",
    "\n",
    "i = 0\n",
    "w = 0\n",
    "f,ax = plt.subplots(nrows=3,figsize=(15,10))\n",
    "ax[0].plot(aaa[i,0,:])\n",
    "ax1 = ax[0].twinx()\n",
    "ax1.plot(bbb[i,0,:]>0.5,alpha=0.5,color='red')\n",
    "ax[1].plot(aaa[i,0,:])\n",
    "ax2 = ax[1].twinx()\n",
    "ax2.plot(bbb[i,1,:]>0.5,alpha=0.5,color='green')\n",
    "ax[2].plot(aaa[i,0,:])\n",
    "ax3 = ax[2].twinx()\n",
    "ax3.plot(bbb[i,2,:]>0.5,alpha=0.5,color='magenta')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test against LUDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LUDBsignal = {}\n",
    "\n",
    "for i in tqdm.tqdm(range(200)):\n",
    "    (signal, header) = wfdb.rdsamp(os.path.join(basedir,'ludb','{}'.format(i+1)))\n",
    "    sortOrder = np.where(np.array([x.upper() for x in header['sig_name']])[:,None] == StandardHeader)[1]\n",
    "    signal = signal[:,sortOrder]\n",
    "    signal = sp.signal.decimate(signal,header['fs']//250,axis=0)\n",
    "    \n",
    "    for j in range(len(StandardHeader)):\n",
    "        lead = StandardHeader[j]\n",
    "        name = str(i+1)+\"###\"+lead\n",
    "        LUDBsignal[name] = signal[:,j]\n",
    "\n",
    "LUDBsignal = pd.DataFrame(LUDBsignal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set([k.split('###')[0] for k in test_keys]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = LUDBsignal['39###AVL']\n",
    "ampl = np.median(moving_operation(tmp,200,lambda x: np.max(x)-np.min(x)))\n",
    "tmp = scipy.interpolate.interp1d(np.linspace(0,1,tmp.size),tmp)(np.linspace(0,1,1.0*tmp.size))\n",
    "aaa = (skimage.util.view_as_windows(tmp/ampl,2048,1024)-0)[:,None,:]\n",
    "bbb = torch.zeros((aaa.shape[0],3,2048),dtype=float)\n",
    "for i in range(0,aaa.shape[0],64):\n",
    "    bbb[i:i+64] = model(torch.tensor(aaa[i:i+64]).cuda().float())[0]\n",
    "bbb = bbb.cpu().detach().numpy()\n",
    "\n",
    "i = 0\n",
    "w = 0\n",
    "f,ax = plt.subplots(nrows=3,figsize=(15,10))\n",
    "ax[0].plot(aaa[i,0,:])\n",
    "ax1 = ax[0].twinx()\n",
    "ax1.plot(bbb[i,0,:]>0.5,alpha=0.5,color='red')\n",
    "ax[1].plot(aaa[i,0,:])\n",
    "ax2 = ax[1].twinx()\n",
    "ax2.plot(bbb[i,1,:]>0.5,alpha=0.5,color='green')\n",
    "ax[2].plot(aaa[i,0,:])\n",
    "ax3 = ax[2].twinx()\n",
    "ax3.plot(bbb[i,2,:]>0.5,alpha=0.5,color='magenta')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test against QTDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTDBsignal = pd.read_csv(os.path.join(basedir,'QTDB','Dataset.csv'), index_col=0)\n",
    "QTDBsignal = dataset.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set([k.split('###')[0] for k in test_keys]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = QTDBsignal['sel16272_0']\n",
    "ampl = np.median(moving_operation(tmp,200,lambda x: np.max(x)-np.min(x)))\n",
    "tmp = scipy.interpolate.interp1d(np.linspace(0,1,tmp.size),tmp)(np.linspace(0,1,1.0*tmp.size))\n",
    "aaa = (skimage.util.view_as_windows(tmp/ampl,2048,1024)-0)[:,None,:]\n",
    "bbb = torch.zeros((aaa.shape[0],3,2048),dtype=float)\n",
    "for i in range(0,aaa.shape[0],64):\n",
    "    bbb[i:i+64] = model(torch.tensor(aaa[i:i+64]).cuda().float())[0]\n",
    "bbb = bbb.cpu().detach().numpy()\n",
    "\n",
    "i = 74\n",
    "w = 0\n",
    "f,ax = plt.subplots(nrows=3,figsize=(15,10))\n",
    "ax[0].plot(aaa[i,0,:])\n",
    "ax1 = ax[0].twinx()\n",
    "ax1.plot(bbb[i,0,:]>0.5,alpha=0.5,color='red')\n",
    "ax[1].plot(aaa[i,0,:])\n",
    "ax2 = ax[1].twinx()\n",
    "ax2.plot(bbb[i,1,:]>0.5,alpha=0.5,color='green')\n",
    "ax[2].plot(aaa[i,0,:])\n",
    "ax3 = ax[2].twinx()\n",
    "ax3.plot(bbb[i,2,:]>0.5,alpha=0.5,color='magenta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT EXECUTION CONFIGURATION PARAMETERS (JSON) ###\n",
    "with open(\"./parameters.json\", 'r') as f:\n",
    "    execution = json.load(f)\n",
    "\n",
    "execution[\"root_directory\"] = input_directory\n",
    "execution[\"save_directory\"] = output_directory\n",
    "\n",
    "### SET RANDOM SEED ###\n",
    "torch.manual_seed(execution['seed'])\n",
    "random.seed(execution['seed'])\n",
    "np.random.seed(execution['seed'])\n",
    "\n",
    "### LOAD DATASET ###\n",
    "# 0) Get classes\n",
    "# print(list(glob.glob(os.path.join(input_directory,\"*.mat\"))))\n",
    "classes = get_classes(input_directory,[os.path.split(f)[-1] for f in glob.glob(os.path.join(input_directory,\"*.mat\"))])\n",
    "\n",
    "# 1) Load labels and compute detections\n",
    "print(\"########## COMPUTING DETECTIONS ##########\")\n",
    "files = []\n",
    "labels = []\n",
    "detections = []\n",
    "for f in tqdm.tqdm(glob.glob(os.path.join(input_directory,\"*.mat\"))):\n",
    "    # Load data\n",
    "    (signal,header) = wfdb.rdsamp(os.path.join(input_directory,os.path.splitext(f)[0]))\n",
    "    signal = signal.astype('float32')\n",
    "\n",
    "    # Use provided function for retrieving the true label\n",
    "    fname, label_header, label = get_true_labels(f.replace('.mat','.hea'),classes)\n",
    "\n",
    "    # Detect signal\n",
    "    detector = ecgdetectors.Detectors(header['fs'])\n",
    "    index_I = np.where(np.array(list(map(str.upper,header['sig_name']))) == 'I')[0][0]\n",
    "    qrs = detector.pan_tompkins_detector(signal[:,index_I])\n",
    "\n",
    "    # Store file name and label\n",
    "    files.append(fname)\n",
    "    detections.append(qrs)\n",
    "    labels.append(label)\n",
    "\n",
    "labels = np.array(labels)\n",
    "files = np.array(files)\n",
    "\n",
    "# 2) Train-test split\n",
    "labels_train,labels_valid,files_train,files_valid,detections_train,detections_valid = sklearn.model_selection.train_test_split(\n",
    "    labels,\n",
    "    files,\n",
    "    detections,\n",
    "    stratify=labels.argmax(-1),\n",
    "    random_state=execution['seed'],\n",
    ")\n",
    "\n",
    "# Save into folder\n",
    "src.utils.pickledump(labels_train, './training/labels_train.pkl')\n",
    "src.utils.pickledump(labels_valid, './training/labels_valid.pkl')\n",
    "src.utils.pickledump(files_train, './training/files_train.pkl')\n",
    "src.utils.pickledump(files_valid, './training/files_valid.pkl')\n",
    "src.utils.pickledump(detections_train, './training/detections_train.pkl')\n",
    "src.utils.pickledump(detections_valid, './training/detections_valid.pkl')\n",
    "\n",
    "print(\"########## GENERATING TRAIN SET ##########\")\n",
    "# Generate train/test sets\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_valid = []\n",
    "y_valid = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(files_train))):\n",
    "    # Retrieve the file information\n",
    "    (signal,_) = wfdb.rdsamp(os.path.join(execution['root_directory'],files_train[i]))\n",
    "    signal = signal.astype('float32').T\n",
    "\n",
    "    if not execution['whole_record']:\n",
    "        for j in range(1,len(detections_train[i])-1):\n",
    "            onset = detections_train[i][j-1]\n",
    "            offset = detections_train[i][j+1]\n",
    "            interp = signal[:,onset:offset]\n",
    "            interp = sp.interpolate.interp1d(np.linspace(0,1,interp.shape[1]),interp,axis=-1)(np.linspace(0,1,736)).astype('float32')\n",
    "            X_train.append(interp)\n",
    "            y_train.append(labels_train[i,:])\n",
    "    else:\n",
    "        X_train.append(signal)\n",
    "        y_train.append(labels_train[i,:])\n",
    "\n",
    "print(\"########## GENERATING TRAIN SET ##########\")\n",
    "for i in tqdm.tqdm(range(len(files_valid))):\n",
    "    # Retrieve the file information\n",
    "    (signal,_) = wfdb.rdsamp(os.path.join(execution['root_directory'],files_valid[i]))\n",
    "    signal = signal.astype('float32').T\n",
    "\n",
    "    if not execution['whole_record']:\n",
    "        for j in range(1,len(detections_valid[i])-1):\n",
    "            onset = detections_valid[i][j-1]\n",
    "            offset = detections_valid[i][j+1]\n",
    "            interp = signal[:,onset:offset]\n",
    "            interp = sp.interpolate.interp1d(np.linspace(0,1,interp.shape[1]),interp,axis=-1)(np.linspace(0,1,736)).astype('float32')\n",
    "            X_valid.append(interp)\n",
    "            y_valid.append(labels_valid[i,:])\n",
    "    else:\n",
    "        X_valid.append(signal)\n",
    "        y_valid.append(labels_valid[i,:])\n",
    "\n",
    "y_valid = np.array(y_valid, dtype='float32')\n",
    "y_train = np.array(y_train, dtype='float32')\n",
    "try:\n",
    "    X_train = np.array(X_train, dtype='float32')\n",
    "    X_valid = np.array(X_valid, dtype='float32')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "### TRAIN MODEL ###\n",
    "model = src.model.GAPModel(\n",
    "    torch.nn.Sequential(\n",
    "        src.model.CNN([12,16,16], regularization=execution['regularization_CNN']),\n",
    "        torch.nn.MaxPool1d(3),\n",
    "        src.model.CNN([16,16], regularization=execution['regularization_CNN']),\n",
    "        torch.nn.MaxPool1d(3),\n",
    "        src.model.CNN([16,32], regularization=execution['regularization_CNN']),\n",
    "        torch.nn.MaxPool1d(3),\n",
    "        src.model.CNN([32,32], regularization=execution['regularization_CNN']),\n",
    "        torch.nn.MaxPool1d(3),\n",
    "        src.model.CNN([32,64], regularization=execution['regularization_CNN']),\n",
    "        torch.nn.MaxPool1d(3),\n",
    "        src.model.CNN([64,128], regularization=execution['regularization_CNN']),\n",
    "        torch.nn.MaxPool1d(3),\n",
    "        src.model.CNN([128,256], regularization=execution['regularization_CNN'], regularize_extrema=False),\n",
    "    ),\n",
    "    src.model.DNN([256,128,64,32,9], regularization=execution['regularization_DNN'], regularize_extrema=False),\n",
    ")\n",
    "\n",
    "if execution['whole_record']:\n",
    "    dataset_train = src.data.PaddedDataset(X_train, y_train, padding_length=execution['padding_length'],swapaxes=False, mode='edge')\n",
    "    dataset_valid = src.data.PaddedDataset(X_valid, y_valid, padding_length=execution['padding_length'],swapaxes=False, mode='edge')\n",
    "else:\n",
    "    dataset_train = src.data.Dataset(X_train, y_train)\n",
    "    dataset_valid = src.data.Dataset(X_valid, y_valid)\n",
    "\n",
    "sampler_train = src.data.StratifiedSampler(y_train, *execution['sampler'])\n",
    "sampler_valid = src.data.StratifiedSampler(y_valid, *execution['sampler'])\n",
    "\n",
    "loader_train  = torch.utils.data.DataLoader(dataset_train, sampler=sampler_train, batch_size=execution['batch_size'], **execution['loader'])\n",
    "loader_valid  = torch.utils.data.DataLoader(dataset_valid, sampler=sampler_valid, batch_size=execution['batch_size'], **execution['loader'])\n",
    "\n",
    "# Loss\n",
    "criterion = lambda X,y,y_pred: torch.nn.MultiLabelSoftMarginLoss(reduction='mean')(y_pred, y.long())\n",
    "metric = lambda X,y,y_pred: src.evaluate.compute_beta_score(y.long().cpu().detach().numpy(),(torch.nn.functional.softmax(y_pred,-1) > 0.5).cpu().detach().numpy())[-1]\n",
    "\n",
    "state = {\n",
    "    'epoch'         : 0,\n",
    "    'device'        : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'optimizer'     : src.utils.class_selector('torch.optim',execution['optimizer']['name'])(model.parameters(), **execution['optimizer']['arguments']),\n",
    "    'root_dir'      : './'\n",
    "}\n",
    "if 'scheduler' in execution:\n",
    "    state['scheduler'] = src.utils.class_selector('torch.optim.lr_scheduler',execution['scheduler']['name'])(state['optimizer'], **execution['scheduler']['arguments'])\n",
    "\n",
    "print(\"########## TRAINING THE MODEL ##########\")\n",
    "state = src.train.train_model(model,state,execution,loader_train, loader_valid, criterion, metric, smaller=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boundary Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class SurfaceLoss():\n",
    "    \"\"\"Adapted from https://github.com/LIVIAETS/surface-loss/blob/master/losses.py#L74\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # Self.idc is used to filter out some classes of the target mask. Use fancy indexing\n",
    "        self.idc: List[int] = kwargs[\"idc\"]\n",
    "\n",
    "    def __call__(self, probs: torch.Tensor, dist_maps: Tensor) -> torch.Tensor:\n",
    "        assert simplex(probs)\n",
    "        assert not one_hot(dist_maps)\n",
    "\n",
    "        pc = probs[:, self.idc, ...].type(torch.float32)\n",
    "        dc = dist_maps[:, self.idc, ...].type(torch.float32)\n",
    "\n",
    "        multipled = torch.einsum(\"bcs,bcs->bcs\", pc, dc)\n",
    "\n",
    "        loss = multipled.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = skimage.segmentation.find_boundaries(y[:,0,None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_structure(rank, connectivity):\n",
    "    if connectivity < 1:\n",
    "        connectivity = 1\n",
    "    if rank < 1:\n",
    "        if connectivity < 1:\n",
    "            return np.array(0, dtype=bool)\n",
    "        else:\n",
    "            return np.array(1, dtype=bool)\n",
    "    output = np.fabs(np.indices([3] * rank) - 1)\n",
    "    output = np.add.reduce(output, 0)\n",
    "    \n",
    "    return np.asarray(output <= connectivity, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct = generate_binary_structure(ndim-1, 1)\n",
    "struct[0,:]=False\n",
    "struct[-1,:]=False\n",
    "struct = struct[None,None,...]\n",
    "struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selem = torch.tensor(selem).type(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "bnd = F.conv2d(label_img, selem, padding=(selem.shape[2] // 2, selem.shape[2] // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erosion1d(signal, selem):\n",
    "    inverted = torch.logical_not(signal).type(signal.dtype)\n",
    "    out = F.conv1d(inverted, selem, padding=(selem.shape[-1] // 2,)) > 0\n",
    "    return torch.logical_not(out)\n",
    "\n",
    "def dilation1d(signal, selem):\n",
    "    return F.conv1d(signal, selem, padding=(selem.shape[-1] // 2,)) > 0\n",
    "\n",
    "def erosion2d(image, selem):\n",
    "    inverted = torch.logical_not(image).type(image.dtype)\n",
    "    out = F.conv2d(inverted, selem, padding=(selem.shape[2] // 2, selem.shape[2] // 2)) > 0\n",
    "    return torch.logical_not(out)\n",
    "\n",
    "def dilation2d(image, selem):\n",
    "    return F.conv2d(image, selem, padding=(selem.shape[2] // 2, selem.shape[2] // 2)) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selem = np.zeros((3,)*(ndim-1),dtype=bool)\n",
    "selem[1,1,:] = True\n",
    "selem1 = np.zeros((3,)*(ndim-1),dtype=bool)\n",
    "selem1[0,0,:] = True\n",
    "selem1[1,1,:] = True\n",
    "selem1[2,2,:] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selem1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pbound = skimage.morphology.dilation(label_img[:,0,...].numpy(),selem.astype('bool')).squeeze()\n",
    "QRSbound = skimage.morphology.dilation(label_img[:,1,...].numpy(),selem.astype('bool')).squeeze()\n",
    "Tbound = skimage.morphology.dilation(label_img[:,2,...].numpy(),selem.astype('bool')).squeeze()\n",
    "out2 = np.concatenate((Pbound[:,None,:],QRSbound[:,None,:],Tbound[:,None,:]),axis=1)\n",
    "Per = skimage.morphology.erosion(label_img[:,0,...].numpy(),selem.astype('bool')).squeeze()\n",
    "QRSer = skimage.morphology.erosion(label_img[:,1,...].numpy(),selem.astype('bool')).squeeze()\n",
    "Ter = skimage.morphology.erosion(label_img[:,2,...].numpy(),selem.astype('bool')).squeeze()\n",
    "er2 = np.concatenate((Per[:,None,:],QRSer[:,None,:],Ter[:,None,:]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dilation1d(label_img.type(torch.float32).squeeze(),torch.tensor(selem1).type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = erosion1d(label_img.type(torch.float32).squeeze(),torch.tensor(selem1).type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "l = 1\n",
    "plt.plot(er[i,l,:])\n",
    "plt.plot(er2[i,l,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(out2.astype('bool'),out.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.ndimage.morphology.binary_dilation(label_img,selem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_img = y[:,:,None,None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnds = skimage.segmentation.find_boundaries(label_img).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.dilate(label_img.numpy(),selem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y[0,1,:])\n",
    "plt.plot(boundaries.squeeze()[0,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_img = y[:,:,None,:]\n",
    "connectivity=1\n",
    "mode='thick'\n",
    "background=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if label_img.dtype == torch.bool:\n",
    "    label_img = label_img.type(torch.uint8)\n",
    "ndim = label_img.ndim\n",
    "# selem = torch.tensor(generate_binary_structure(ndim, connectivity))\n",
    "selem = np.zeros((3,)*(ndim-1),dtype=bool)\n",
    "selem[1,1,1,:] = True\n",
    "if mode != 'subpixel':\n",
    "    boundaries = skimage.morphology.dilation(label_img, selem) != skimage.morphology.erosion(label_img, selem)\n",
    "    if mode == 'inner':\n",
    "        foreground_image = (label_img != background)\n",
    "        boundaries &= foreground_image\n",
    "    elif mode == 'outer':\n",
    "        max_label = torch.iinfo(label_img.dtype).max\n",
    "        background_image = (label_img == background)\n",
    "        selem = generate_binary_structure(ndim, ndim)\n",
    "        inverted_background = torch.tensor(label_img, copy=True)\n",
    "        inverted_background[background_image] = max_label\n",
    "        adjacent_objects = ((skimage.morphology.dilation(label_img, selem) !=\n",
    "                             skimage.morphology.erosion(inverted_background, selem)) &\n",
    "                            ~background_image)\n",
    "        boundaries &= (background_image | adjacent_objects)\n",
    "else:\n",
    "    boundaries = _find_boundaries_subpixel(label_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.morphology.dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.fabs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning3",
   "language": "python",
   "name": "deeplearning3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
