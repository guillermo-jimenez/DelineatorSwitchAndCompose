{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import random\n",
    "import uuid\n",
    "import os\n",
    "import os.path\n",
    "import skimage\n",
    "import utils\n",
    "import utils.wavelet\n",
    "import utils.data\n",
    "import utils.data.augmentation\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import networkx.algorithms.approximation\n",
    "import wfdb\n",
    "import json\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from utils.signal import StandardHeader\n",
    "\n",
    "# Data loader to un-clutter code    \n",
    "def load_data(filepath):\n",
    "    dic = dict()\n",
    "    with open(filepath) as f:\n",
    "        text = list(f)\n",
    "    for line in text:\n",
    "        line = line.replace(' ','').replace('\\n','').replace(',,','')\n",
    "        if line[-1] == ',': line = line[:-1]\n",
    "        head = line.split(',')[0]\n",
    "        tail = line.split(',')[1:]\n",
    "        if tail == ['']:\n",
    "            tail = np.asarray([])\n",
    "        else:\n",
    "            tail = np.asarray(tail).astype(int)\n",
    "\n",
    "        dic[head] = tail\n",
    "    return dic\n",
    "\n",
    "\n",
    "def trailonset(sig,on):\n",
    "    on = on-sig[0]\n",
    "    off = on-sig[0]+sig[-1]\n",
    "    sig = sig+np.linspace(on,off,sig.size)\n",
    "    \n",
    "    return sig\n",
    "\n",
    "def getcorr(segments):\n",
    "    if len(segments) > 0:\n",
    "        length = 2*max([segments[i][2].size for i in range(len(segments))])\n",
    "    else:\n",
    "        return np.zeros((0,0))\n",
    "\n",
    "    corr = np.zeros((len(segments),len(segments)))\n",
    "\n",
    "    for i in range(len(segments)):\n",
    "        for j in range(len(segments)):\n",
    "            if i != j:\n",
    "                if segments[i][2].size != segments[j][2].size:\n",
    "                    if segments[i][2].size != 1:\n",
    "                        x1 = sp.interpolate.interp1d(np.linspace(0,1,len(segments[i][2])),segments[i][2])(np.linspace(0,1,length))\n",
    "                    else:\n",
    "                        x1 = np.full((length,),segments[i][2][0])\n",
    "                    if segments[j][2].size != 1:\n",
    "                        x2 = sp.interpolate.interp1d(np.linspace(0,1,len(segments[j][2])),segments[j][2])(np.linspace(0,1,length))\n",
    "                    else:\n",
    "                        x2 = np.full((length,),segments[j][2][0])\n",
    "                else:\n",
    "                    x1 = segments[i][2]\n",
    "                    x2 = segments[j][2]\n",
    "                if (x1.size == 1) and (x2.size == 1):\n",
    "                    corr[i,j] = 1\n",
    "                else:\n",
    "                    c,_ = utils.signal.xcorr(x1,x2)\n",
    "                    corr[i,j] = np.max(np.abs(c))\n",
    "            else:\n",
    "                corr[i,j] = 1\n",
    "                \n",
    "    return corr\n",
    "\n",
    "def getdelete(segments, threshold):\n",
    "    corr = getcorr(segments)\n",
    "    \n",
    "    index_delete = []\n",
    "    \n",
    "    for i in range(corr.shape[0]):\n",
    "        if i in index_delete:\n",
    "            continue\n",
    "        for j in range(corr.shape[1]):\n",
    "            if j == i:\n",
    "                continue\n",
    "            if corr[i,j] > threshold:\n",
    "                if j not in index_delete:\n",
    "                    index_delete.append(j)\n",
    "                \n",
    "    return index_delete\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() in ['Linux', 'Linux2']:\n",
    "    basedir = '/media/guille/DADES/DADES/Delineator'\n",
    "else:\n",
    "    basedir = r'C:\\Users\\Emilio\\Documents\\DADES\\DADES\\Delineator'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LUDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:17<00:00, 11.22it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "Pon = {}\n",
    "Ppeak = {}\n",
    "Poff = {}\n",
    "QRSon = {}\n",
    "QRSpeak = {}\n",
    "QRSoff = {}\n",
    "Ton = {}\n",
    "Tpeak = {}\n",
    "Toff = {}\n",
    "group = {}\n",
    "\n",
    "for i in tqdm.tqdm(range(200)):\n",
    "    (signal, header) = wfdb.rdsamp(os.path.join(basedir,'ludb','{}'.format(i+1)))\n",
    "    sortOrder = np.where(np.array([x.upper() for x in header['sig_name']])[:,None] == StandardHeader)[1]\n",
    "    signal = signal[:,sortOrder]\n",
    "    if header['fs'] != 500:\n",
    "        print(header['fs'])\n",
    "    signal = sp.signal.decimate(signal,2,axis=0)\n",
    "    \n",
    "    # 1st step: reduce noise\n",
    "    signal = sp.signal.filtfilt(*sp.signal.butter(4,   0.5/250., 'high'),signal.T).T\n",
    "    signal = sp.signal.filtfilt(*sp.signal.butter(4, 125.0/250.,  'low'),signal.T).T\n",
    "\n",
    "    # 2nd step: retrieve onsets and offsets\n",
    "    for j in range(len(StandardHeader)):\n",
    "        lead = StandardHeader[j]\n",
    "        name = str(i+1)+\"_\"+lead\n",
    "        ann = wfdb.rdann(os.path.join(basedir,'ludb','{}'.format(i+1)),'atr_{}'.format(lead.lower()))\n",
    "        dataset[name] = signal[:,j]\n",
    "        \n",
    "        locP = np.where(np.array(ann.symbol) == 'p')[0]\n",
    "        if len(locP) != 0:\n",
    "            if locP[0]-1 < 0:\n",
    "                locP = locP[1:]\n",
    "            if locP[-1]+1 == len(ann.sample):\n",
    "                locP = locP[:-1]\n",
    "        Pon[name] = ann.sample[locP-1]//2\n",
    "        Ppeak[name] = ann.sample[locP]//2\n",
    "        Poff[name] = ann.sample[locP+1]//2\n",
    "\n",
    "        locQRS = np.where(np.array(ann.symbol) == 'N')[0]\n",
    "        if len(locQRS) != 0:\n",
    "            if locQRS[0]-1 < 0:\n",
    "                locQRS = locQRS[1:]\n",
    "            if locQRS[-1]+1 == len(ann.sample):\n",
    "                locQRS = locQRS[:-1]\n",
    "        QRSon[name] = ann.sample[locQRS-1]//2\n",
    "        QRSpeak[name] = ann.sample[locQRS]//2\n",
    "        QRSoff[name] = ann.sample[locQRS+1]//2\n",
    "\n",
    "        locT = np.where(np.array(ann.symbol) == 't')[0]\n",
    "        if len(locT) != 0:\n",
    "            if locT[0]-1 < 0:\n",
    "                locT = locT[1:]\n",
    "            if locT[-1]+1 == len(ann.sample):\n",
    "                locT = locT[:-1]\n",
    "        Ton[name] = ann.sample[locT-1]//2\n",
    "        Tpeak[name] = ann.sample[locT]//2\n",
    "        Toff[name] = ann.sample[locT+1]//2\n",
    "        \n",
    "        # Store group\n",
    "        group[name] = str(i+1)\n",
    "\n",
    "dataset = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 414/2400 [00:36<09:12,  3.60it/s]"
     ]
    }
   ],
   "source": [
    "threshold = 0.99\n",
    "\n",
    "PsignalLUDB = {}\n",
    "PQsignalLUDB = {}\n",
    "QRSsignalLUDB = {}\n",
    "STsignalLUDB = {}\n",
    "TsignalLUDB = {}\n",
    "TPsignalLUDB = {}\n",
    "\n",
    "PgroupLUDB = {}\n",
    "PQgroupLUDB = {}\n",
    "QRSgroupLUDB = {}\n",
    "STgroupLUDB = {}\n",
    "TgroupLUDB = {}\n",
    "TPgroupLUDB = {}\n",
    "\n",
    "for k in tqdm.tqdm(dataset.keys()):\n",
    "    # Buggy files\n",
    "    if k in (['116_{}'.format(h) for h in StandardHeader] + \n",
    "             ['104_{}'.format(h) for h in StandardHeader] + \n",
    "             ['103_III',]):\n",
    "        continue\n",
    "    pon = Pon.get(k,np.array([]))\n",
    "    pof = Poff.get(k,np.array([]))\n",
    "    qon = QRSon.get(k,np.array([]))\n",
    "    qof = QRSoff.get(k,np.array([]))\n",
    "    ton = Ton.get(k,np.array([]))\n",
    "    tof = Toff.get(k,np.array([]))\n",
    "    \n",
    "    unordered_samples = np.concatenate([pon,pof,qon,qof,ton,tof,]).astype(float)\n",
    "    unordered_symbols = np.concatenate([['Pon']*pon.size,['Poff']*pof.size,\n",
    "                                        ['QRSon']*qon.size,['QRSoff']*qof.size,\n",
    "                                        ['Ton']*ton.size,['Toff']*tof.size,])\n",
    "    # Sort fiducials taking logical orders if same sample of occurrence\n",
    "    # There is (I'm definitely sure) a better way to do it\n",
    "    samples = []\n",
    "    symbols = []\n",
    "    for i in range(unordered_samples.size):\n",
    "        minimum = np.where(unordered_samples == min(unordered_samples))[0]\n",
    "        if minimum.size == 1:\n",
    "            minimum = minimum[0]\n",
    "            samples.append(int(unordered_samples[minimum]))\n",
    "            symbols.append(unordered_symbols[minimum])\n",
    "            unordered_samples[minimum] = np.inf\n",
    "        elif minimum.size == 2:\n",
    "            if symbols[-1] == 'Pon':\n",
    "                if unordered_symbols[minimum[0]] == 'Poff':\n",
    "                    samples.append(int(unordered_samples[minimum[0]]))\n",
    "                    symbols.append(unordered_symbols[minimum[0]])\n",
    "                    unordered_samples[minimum[0]] = np.inf\n",
    "                elif unordered_symbols[minimum[1]] == 'Poff':\n",
    "                    samples.append(int(unordered_samples[minimum[1]]))\n",
    "                    symbols.append(unordered_symbols[minimum[1]])\n",
    "                    unordered_samples[minimum[1]] = np.inf\n",
    "            elif symbols[-1] == 'QRSon':\n",
    "                if unordered_symbols[minimum[0]] == 'QRSoff':\n",
    "                    samples.append(int(unordered_samples[minimum[0]]))\n",
    "                    symbols.append(unordered_symbols[minimum[0]])\n",
    "                    unordered_samples[minimum[0]] = np.inf\n",
    "                elif unordered_symbols[minimum[1]] == 'QRSoff':\n",
    "                    samples.append(int(unordered_samples[minimum[1]]))\n",
    "                    symbols.append(unordered_symbols[minimum[1]])\n",
    "                    unordered_samples[minimum[1]] = np.inf\n",
    "            elif symbols[-1] == 'Ton':\n",
    "                if unordered_symbols[minimum[0]] == 'Toff':\n",
    "                    samples.append(int(unordered_samples[minimum[0]]))\n",
    "                    symbols.append(unordered_symbols[minimum[0]])\n",
    "                    unordered_samples[minimum[0]] = np.inf\n",
    "                elif unordered_symbols[minimum[1]] == 'Toff':\n",
    "                    samples.append(int(unordered_samples[minimum[1]]))\n",
    "                    symbols.append(unordered_symbols[minimum[1]])\n",
    "                    unordered_samples[minimum[1]] = np.inf\n",
    "            else:\n",
    "                raise ValueError(\"Should not happen at all\")\n",
    "        else:\n",
    "            raise ValueError(\"Definitely should not happen. Check file {}\".format(k))\n",
    "    samples = np.array(samples)\n",
    "    symbols = np.array(symbols)\n",
    "    \n",
    "    # Extract segments\n",
    "    P = []\n",
    "    QRS = []\n",
    "    T = []\n",
    "    TP = []\n",
    "    PQ = []\n",
    "    ST = []\n",
    "\n",
    "    # Extract segments\n",
    "    for i in range(samples.size-1):\n",
    "        if samples[i] == samples[i+1]:\n",
    "            continue\n",
    "        if symbols[i] == 'Pon':\n",
    "            if symbols[i+1] == 'Poff':\n",
    "                P.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            else:\n",
    "                print(\"Check file {}. P onset not followed by offset\".format(k))\n",
    "        elif symbols[i] == 'QRSon':\n",
    "            if symbols[i+1] == 'QRSoff':\n",
    "                QRS.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            else:\n",
    "                print(\"Check file {}. QRS onset not followed by offset\".format(k))\n",
    "        elif symbols[i] == 'Ton':\n",
    "            if symbols[i+1] == 'Toff':\n",
    "                T.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            else:\n",
    "                print(\"Check file {}. T onset not followed by offset\".format(k))\n",
    "        elif symbols[i] == 'Poff':\n",
    "            if symbols[i+1] == 'Pon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'QRSon':\n",
    "                PQ.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'Ton':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] in ['Poff','QRSoff','Toff']:\n",
    "                print(\"Check file {}. P offset not followed by onset\".format(k))\n",
    "        elif symbols[i] == 'QRSoff':\n",
    "            if symbols[i+1] == 'Pon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'QRSon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'Ton':\n",
    "                ST.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] in ['Poff','QRSoff','Toff']:\n",
    "                print(\"Check file {}. P offset not followed by onset\".format(k))\n",
    "        elif symbols[i] == 'Toff':\n",
    "            if symbols[i+1] == 'Pon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'QRSon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'Ton':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] in ['Poff','QRSoff','Toff']:\n",
    "                print(\"Check file {}. P offset not followed by onset\".format(k))\n",
    "        else:\n",
    "            raise ValueError(\"This should definitely not happen\")\n",
    "\n",
    "    # Filter out too similar segments\n",
    "    corr = getcorr(P)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    P = [P[i] for i in range(len(P)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(QRS)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    QRS = [QRS[i] for i in range(len(QRS)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(T)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    T = [T[i] for i in range(len(T)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(TP)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    TP = [TP[i] for i in range(len(TP)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(PQ)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    PQ = [PQ[i] for i in range(len(PQ)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(ST)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    ST = [ST[i] for i in range(len(ST)) if i in nodesclique]\n",
    "    \n",
    "    # Store segments\n",
    "    for i in range(len(P)):\n",
    "        PsignalLUDB[k + '_' + str(i)] = P[i][2]\n",
    "        PgroupLUDB[k + '_' + str(i)] = (P[i][0],P[i][1])\n",
    "    for i in range(len(QRS)):\n",
    "        QRSsignalLUDB[k + '_' + str(i)] = QRS[i][2]\n",
    "        QRSgroupLUDB[k + '_' + str(i)] = (QRS[i][0],QRS[i][1])\n",
    "    for i in range(len(T)):\n",
    "        TsignalLUDB[k + '_' + str(i)] = T[i][2]\n",
    "        TgroupLUDB[k + '_' + str(i)] = (T[i][0],T[i][1])\n",
    "    for i in range(len(TP)):\n",
    "        TPsignalLUDB[k + '_' + str(i)] = TP[i][2]\n",
    "        TPgroupLUDB[k + '_' + str(i)] = (TP[i][0],TP[i][1])\n",
    "    for i in range(len(PQ)):\n",
    "        PQsignalLUDB[k + '_' + str(i)] = PQ[i][2]\n",
    "        PQgroupLUDB[k + '_' + str(i)] = (PQ[i][0],PQ[i][1])\n",
    "    for i in range(len(ST)):\n",
    "        STsignalLUDB[k + '_' + str(i)] = ST[i][2]\n",
    "        STgroupLUDB[k + '_' + str(i)] = (ST[i][0],ST[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(PgroupLUDB))\n",
    "print(len(PQgroupLUDB))\n",
    "print(len(QRSgroupLUDB))\n",
    "print(len(STgroupLUDB))\n",
    "print(len(TgroupLUDB))\n",
    "print(len(TPgroupLUDB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load QT db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD DATASETS ####\n",
    "dataset             = pd.read_csv(os.path.join(basedir,'QTDB','Dataset.csv'), index_col=0)\n",
    "dataset             = dataset.sort_index(axis=1)\n",
    "labels              = np.asarray(list(dataset)) # In case no data augmentation is applied\n",
    "description         = dataset.describe()\n",
    "group               = {k: '_'.join(k.split('_')[:-1]) for k in dataset}\n",
    "\n",
    "# Zero-center data\n",
    "for key in description:\n",
    "    dataset[key]    = (dataset[key] - description[key]['mean'])/description[key]['std']\n",
    "    \n",
    "# Filter the data\n",
    "for col in dataset:\n",
    "    dataset[col] = sp.signal.filtfilt(*sp.signal.butter(4,   0.5/250., 'high'),dataset[col].T).T\n",
    "    dataset[col] = sp.signal.filtfilt(*sp.signal.butter(4, 125.0/250.,  'low'),dataset[col].T).T\n",
    "    \n",
    "# Load fiducials\n",
    "Pon = load_data(os.path.join(basedir,'QTDB','PonNew.csv'))\n",
    "Poff = load_data(os.path.join(basedir,'QTDB','PoffNew.csv'))\n",
    "QRSon = load_data(os.path.join(basedir,'QTDB','QRSonNew.csv'))\n",
    "QRSoff = load_data(os.path.join(basedir,'QTDB','QRSoffNew.csv'))\n",
    "Ton = load_data(os.path.join(basedir,'QTDB','TonNew.csv'))\n",
    "Toff = load_data(os.path.join(basedir,'QTDB','ToffNew.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.99\n",
    "\n",
    "PsignalQTDB = {}\n",
    "PQsignalQTDB = {}\n",
    "QRSsignalQTDB = {}\n",
    "STsignalQTDB = {}\n",
    "TsignalQTDB = {}\n",
    "TPsignalQTDB = {}\n",
    "\n",
    "PgroupQTDB = {}\n",
    "PQgroupQTDB = {}\n",
    "QRSgroupQTDB = {}\n",
    "STgroupQTDB = {}\n",
    "TgroupQTDB = {}\n",
    "TPgroupQTDB = {}\n",
    "\n",
    "for k in tqdm.tqdm(dataset.keys()):\n",
    "    # Buggy files\n",
    "    if k in ['sel232_0', 'sel232_1']:\n",
    "        continue\n",
    "    pon = Pon.get(k,np.array([]))\n",
    "    pof = Poff.get(k,np.array([]))\n",
    "    qon = QRSon.get(k,np.array([]))\n",
    "    qof = QRSoff.get(k,np.array([]))\n",
    "    ton = Ton.get(k,np.array([]))\n",
    "    tof = Toff.get(k,np.array([]))\n",
    "    \n",
    "    unordered_samples = np.concatenate([pon,pof,qon,qof,ton,tof,]).astype(float)\n",
    "    unordered_symbols = np.concatenate([['Pon']*pon.size,['Poff']*pof.size,\n",
    "                                        ['QRSon']*qon.size,['QRSoff']*qof.size,\n",
    "                                        ['Ton']*ton.size,['Toff']*tof.size,])\n",
    "    # Sort fiducials taking logical orders if same sample of occurrence\n",
    "    # There is (I'm definitely sure) a better way to do it\n",
    "    samples = []\n",
    "    symbols = []\n",
    "    for i in range(unordered_samples.size):\n",
    "        minimum = np.where(unordered_samples == min(unordered_samples))[0]\n",
    "        if minimum.size == 1:\n",
    "            minimum = minimum[0]\n",
    "            samples.append(int(unordered_samples[minimum]))\n",
    "            symbols.append(unordered_symbols[minimum])\n",
    "            unordered_samples[minimum] = np.inf\n",
    "        elif minimum.size == 2:\n",
    "            if symbols[-1] == 'Pon':\n",
    "                if unordered_symbols[minimum[0]] == 'Poff':\n",
    "                    samples.append(int(unordered_samples[minimum[0]]))\n",
    "                    symbols.append(unordered_symbols[minimum[0]])\n",
    "                    unordered_samples[minimum[0]] = np.inf\n",
    "                elif unordered_symbols[minimum[1]] == 'Poff':\n",
    "                    samples.append(int(unordered_samples[minimum[1]]))\n",
    "                    symbols.append(unordered_symbols[minimum[1]])\n",
    "                    unordered_samples[minimum[1]] = np.inf\n",
    "            elif symbols[-1] == 'QRSon':\n",
    "                if unordered_symbols[minimum[0]] == 'QRSoff':\n",
    "                    samples.append(int(unordered_samples[minimum[0]]))\n",
    "                    symbols.append(unordered_symbols[minimum[0]])\n",
    "                    unordered_samples[minimum[0]] = np.inf\n",
    "                elif unordered_symbols[minimum[1]] == 'QRSoff':\n",
    "                    samples.append(int(unordered_samples[minimum[1]]))\n",
    "                    symbols.append(unordered_symbols[minimum[1]])\n",
    "                    unordered_samples[minimum[1]] = np.inf\n",
    "            elif symbols[-1] == 'Ton':\n",
    "                if unordered_symbols[minimum[0]] == 'Toff':\n",
    "                    samples.append(int(unordered_samples[minimum[0]]))\n",
    "                    symbols.append(unordered_symbols[minimum[0]])\n",
    "                    unordered_samples[minimum[0]] = np.inf\n",
    "                elif unordered_symbols[minimum[1]] == 'Toff':\n",
    "                    samples.append(int(unordered_samples[minimum[1]]))\n",
    "                    symbols.append(unordered_symbols[minimum[1]])\n",
    "                    unordered_samples[minimum[1]] = np.inf\n",
    "            else:\n",
    "                raise ValueError(\"Should not happen at all\")\n",
    "        else:\n",
    "            raise ValueError(\"Definitely should not happen. Check file {}\".format(k))\n",
    "    samples = np.array(samples)\n",
    "    symbols = np.array(symbols)\n",
    "    \n",
    "    # Extract segments\n",
    "    P = []\n",
    "    QRS = []\n",
    "    T = []\n",
    "    TP = []\n",
    "    PQ = []\n",
    "    ST = []\n",
    "\n",
    "    for i in range(samples.size-1):\n",
    "        if samples[i] == samples[i+1]:\n",
    "            continue\n",
    "        if symbols[i] == 'Pon':\n",
    "            if symbols[i+1] == 'Poff':\n",
    "                P.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            else:\n",
    "                print(\"Check file {}. P onset not followed by offset\".format(k))\n",
    "        elif symbols[i] == 'QRSon':\n",
    "            if symbols[i+1] == 'QRSoff':\n",
    "                QRS.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            else:\n",
    "                print(\"Check file {}. QRS onset not followed by offset\".format(k))\n",
    "        elif symbols[i] == 'Ton':\n",
    "            if symbols[i+1] == 'Toff':\n",
    "                T.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            else:\n",
    "                print(\"Check file {}. T onset not followed by offset\".format(k))\n",
    "        elif symbols[i] == 'Poff':\n",
    "            if symbols[i+1] == 'Pon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'QRSon':\n",
    "                PQ.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'Ton':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] in ['Poff','QRSoff','Toff']:\n",
    "                print(\"Check file {}. P offset not followed by onset\".format(k))\n",
    "        elif symbols[i] == 'QRSoff':\n",
    "            if symbols[i+1] == 'Pon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'QRSon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'Ton':\n",
    "                ST.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] in ['Poff','QRSoff','Toff']:\n",
    "                print(\"Check file {}. P offset not followed by onset\".format(k))\n",
    "        elif symbols[i] == 'Toff':\n",
    "            if symbols[i+1] == 'Pon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'QRSon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'Ton':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] in ['Poff','QRSoff','Toff']:\n",
    "                print(\"Check file {}. P offset not followed by onset\".format(k))\n",
    "        else:\n",
    "            raise ValueError(\"This should definitely not happen\")\n",
    "            \n",
    "    # Filter out too long TP segments (causing this to break)\n",
    "    TP = [TP[i] for i in range(len(TP)) if TP[i][2].size < 250]\n",
    "\n",
    "    # Filter out too similar segments\n",
    "    corr = getcorr(P)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    P = [P[i] for i in range(len(P)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(QRS)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    QRS = [QRS[i] for i in range(len(QRS)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(T)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    T = [T[i] for i in range(len(T)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(TP)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    TP = [TP[i] for i in range(len(TP)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(PQ)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    PQ = [PQ[i] for i in range(len(PQ)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(ST)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    ST = [ST[i] for i in range(len(ST)) if i in nodesclique]\n",
    "    \n",
    "    # Store segments\n",
    "    for i in range(len(P)):\n",
    "        PsignalQTDB[k + '_' + str(i)] = P[i][2]\n",
    "        PgroupQTDB[k + '_' + str(i)] = (P[i][0],P[i][1])\n",
    "    for i in range(len(QRS)):\n",
    "        QRSsignalQTDB[k + '_' + str(i)] = QRS[i][2]\n",
    "        QRSgroupQTDB[k + '_' + str(i)] = (QRS[i][0],QRS[i][1])\n",
    "    for i in range(len(T)):\n",
    "        TsignalQTDB[k + '_' + str(i)] = T[i][2]\n",
    "        TgroupQTDB[k + '_' + str(i)] = (T[i][0],T[i][1])\n",
    "    for i in range(len(TP)):\n",
    "        TPsignalQTDB[k + '_' + str(i)] = TP[i][2]\n",
    "        TPgroupQTDB[k + '_' + str(i)] = (TP[i][0],TP[i][1])\n",
    "    for i in range(len(PQ)):\n",
    "        PQsignalQTDB[k + '_' + str(i)] = PQ[i][2]\n",
    "        PQgroupQTDB[k + '_' + str(i)] = (PQ[i][0],PQ[i][1])\n",
    "    for i in range(len(ST)):\n",
    "        STsignalQTDB[k + '_' + str(i)] = ST[i][2]\n",
    "        STgroupQTDB[k + '_' + str(i)] = (ST[i][0],ST[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(PgroupQTDB))\n",
    "print(len(PQgroupQTDB))\n",
    "print(len(QRSgroupQTDB))\n",
    "print(len(STgroupQTDB))\n",
    "print(len(TgroupQTDB))\n",
    "print(len(TPgroupQTDB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Psignal = {}\n",
    "Pgroup = {}\n",
    "PQsignal = {}\n",
    "PQgroup = {}\n",
    "QRSsignal = {}\n",
    "QRSgroup = {}\n",
    "STsignal = {}\n",
    "STgroup = {}\n",
    "Tsignal = {}\n",
    "Tgroup = {}\n",
    "TPsignal = {}\n",
    "TPgroup = {}\n",
    "\n",
    "Psignal.update(PsignalQTDB)\n",
    "Pgroup.update(PgroupQTDB)\n",
    "PQsignal.update(PQsignalQTDB)\n",
    "PQgroup.update(PQgroupQTDB)\n",
    "QRSsignal.update(QRSsignalQTDB)\n",
    "QRSgroup.update(QRSgroupQTDB)\n",
    "STsignal.update(STsignalQTDB)\n",
    "STgroup.update(STgroupQTDB)\n",
    "Tsignal.update(TsignalQTDB)\n",
    "Tgroup.update(TgroupQTDB)\n",
    "TPsignal.update(TPsignalQTDB)\n",
    "TPgroup.update(TPgroupQTDB)\n",
    "\n",
    "Psignal.update(PsignalLUDB)\n",
    "Pgroup.update(PgroupLUDB)\n",
    "PQsignal.update(PQsignalLUDB)\n",
    "PQgroup.update(PQgroupLUDB)\n",
    "QRSsignal.update(QRSsignalLUDB)\n",
    "QRSgroup.update(QRSgroupLUDB)\n",
    "STsignal.update(STsignalLUDB)\n",
    "STgroup.update(STgroupLUDB)\n",
    "Tsignal.update(TsignalLUDB)\n",
    "Tgroup.update(TgroupLUDB)\n",
    "TPsignal.update(TPsignalLUDB)\n",
    "TPgroup.update(TPgroupLUDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Pgroup))\n",
    "print(len(PQgroup))\n",
    "print(len(QRSgroup))\n",
    "print(len(STgroup))\n",
    "print(len(Tgroup))\n",
    "print(len(TPgroup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete too short or too long signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal lengths\n",
    "Plength = {k: len(Psignal[k]) for k in Psignal.keys() if not isinstance(Psignal[k],float)}\n",
    "PQlength = {k: len(PQsignal[k]) for k in PQsignal.keys() if not isinstance(PQsignal[k],float)}\n",
    "QRSlength = {k: len(QRSsignal[k]) for k in QRSsignal.keys() if not isinstance(QRSsignal[k],float)}\n",
    "STlength = {k: len(STsignal[k]) for k in STsignal.keys() if not isinstance(STsignal[k],float)}\n",
    "Tlength = {k: len(Tsignal[k]) for k in Tsignal.keys() if not isinstance(Tsignal[k],float)}\n",
    "TPlength = {k: len(TPsignal[k]) for k in TPsignal.keys() if not isinstance(TPsignal[k],float)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter signals by length\n",
    "for k in list(Psignal.keys()):\n",
    "    if isinstance(Psignal[k],float):\n",
    "        Psignal.pop(k)\n",
    "        Pgroup.pop(k)\n",
    "    elif not ((len(Psignal[k]) > 1) and (len(Psignal[k]) < 45)):\n",
    "        Psignal.pop(k)\n",
    "        Pgroup.pop(k)\n",
    "for k in list(PQsignal.keys()):\n",
    "    if isinstance(PQsignal[k],float):\n",
    "        PQsignal.pop(k)\n",
    "        PQgroup.pop(k)\n",
    "    elif not ((len(PQsignal[k]) > 1) and (len(PQsignal[k]) < 35)):\n",
    "        PQsignal.pop(k)\n",
    "        PQgroup.pop(k)\n",
    "for k in list(QRSsignal.keys()):\n",
    "    if isinstance(QRSsignal[k],float):\n",
    "        QRSsignal.pop(k)\n",
    "        QRSgroup.pop(k)\n",
    "    elif not ((len(QRSsignal[k]) > 1) and (len(QRSsignal[k]) < 50)):\n",
    "        QRSsignal.pop(k)\n",
    "        QRSgroup.pop(k)\n",
    "for k in list(STsignal.keys()):\n",
    "    if isinstance(STsignal[k],float):\n",
    "        STsignal.pop(k)\n",
    "        STgroup.pop(k)\n",
    "    elif not ((len(STsignal[k]) > 1) and (len(STsignal[k]) < 65)):\n",
    "        STsignal.pop(k)\n",
    "        STgroup.pop(k)\n",
    "for k in list(Tsignal.keys()):\n",
    "    if isinstance(Tsignal[k],float):\n",
    "        Tsignal.pop(k)\n",
    "        Tgroup.pop(k)\n",
    "    elif not ((len(Tsignal[k]) > 1) and (len(Tsignal[k]) < 100)):\n",
    "        Tsignal.pop(k)\n",
    "        Tgroup.pop(k)\n",
    "for k in list(TPsignal.keys()):\n",
    "    if isinstance(TPsignal[k],float):\n",
    "        TPsignal.pop(k)\n",
    "        TPgroup.pop(k)\n",
    "    elif not ((len(TPsignal[k]) > 1) and (len(TPsignal[k]) < 250)):\n",
    "        TPsignal.pop(k)\n",
    "        TPgroup.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Pgroup))\n",
    "print(len(PQgroup))\n",
    "print(len(QRSgroup))\n",
    "print(len(STgroup))\n",
    "print(len(Tgroup))\n",
    "print(len(TPgroup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.pickledump(Psignal,os.path.join('.','pickle','Psignal.pkl'))\n",
    "utils.pickledump(Pgroup,os.path.join('.','pickle','Pgroup.pkl'))\n",
    "utils.pickledump(PQsignal,os.path.join('.','pickle','PQsignal.pkl'))\n",
    "utils.pickledump(PQgroup,os.path.join('.','pickle','PQgroup.pkl'))\n",
    "utils.pickledump(QRSsignal,os.path.join('.','pickle','QRSsignal.pkl'))\n",
    "utils.pickledump(QRSgroup,os.path.join('.','pickle','QRSgroup.pkl'))\n",
    "utils.pickledump(STsignal,os.path.join('.','pickle','STsignal.pkl'))\n",
    "utils.pickledump(STgroup,os.path.join('.','pickle','STgroup.pkl'))\n",
    "utils.pickledump(Tsignal,os.path.join('.','pickle','Tsignal.pkl'))\n",
    "utils.pickledump(Tgroup,os.path.join('.','pickle','Tgroup.pkl'))\n",
    "utils.pickledump(TPsignal,os.path.join('.','pickle','TPsignal.pkl'))\n",
    "utils.pickledump(TPgroup,os.path.join('.','pickle','TPgroup.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_strategy = 0\n",
    "\n",
    "amplitudes = {stratification[normalization_strategy]: [] for stratification in list(set(QRSgroup.values()))}\n",
    "for k in QRSsignal:\n",
    "    stratification = QRSgroup[k]\n",
    "    g = stratification[normalization_strategy]\n",
    "    amplitudes[g].append(np.max(np.abs(utils.signal.on_off_correction(QRSsignal[k]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = np.median\n",
    "\n",
    "for k in Psignal:\n",
    "    stratification = Pgroup[k]\n",
    "    g = stratification[normalization_strategy]\n",
    "    Psignal[k] = utils.signal.on_off_correction(Psignal[k])/metric(amplitudes[g])\n",
    "\n",
    "for k in PQsignal:\n",
    "    stratification = PQgroup[k]\n",
    "    g = stratification[normalization_strategy]\n",
    "    PQsignal[k] = utils.signal.on_off_correction(PQsignal[k])/metric(amplitudes[g])\n",
    "\n",
    "for k in QRSsignal:\n",
    "    stratification = QRSgroup[k]\n",
    "    g = stratification[normalization_strategy]\n",
    "    QRSsignal[k] = utils.signal.on_off_correction(QRSsignal[k])/metric(amplitudes[g])\n",
    "\n",
    "for k in STsignal:\n",
    "    stratification = STgroup[k]\n",
    "    g = stratification[normalization_strategy]\n",
    "    STsignal[k] = utils.signal.on_off_correction(STsignal[k])/metric(amplitudes[g])\n",
    "\n",
    "for k in Tsignal:\n",
    "    stratification = Tgroup[k]\n",
    "    g = stratification[normalization_strategy]\n",
    "    Tsignal[k] = utils.signal.on_off_correction(Tsignal[k])/metric(amplitudes[g])\n",
    "\n",
    "for k in TPsignal:\n",
    "    stratification = TPgroup[k]\n",
    "    g = stratification[normalization_strategy]\n",
    "    TPsignal[k] = utils.signal.on_off_correction(TPsignal[k])/metric(amplitudes[g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding inverses - Data augmentation\n",
    "\n",
    "NOPE! At runtime. Otherwise, memory issues that fuck my mixup up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Psignal.update({'-'+k: -Psignal[k] for k in Psignal})\n",
    "# Pgroup.update({'-'+k: Pgroup[k] for k in Pgroup})\n",
    "# PQsignal.update({'-'+k: -PQsignal[k] for k in PQsignal})\n",
    "# PQgroup.update({'-'+k: PQgroup[k] for k in PQgroup})\n",
    "# QRSsignal.update({'-'+k: -QRSsignal[k] for k in QRSsignal})\n",
    "# QRSgroup.update({'-'+k: QRSgroup[k] for k in QRSgroup})\n",
    "# STsignal.update({'-'+k: -STsignal[k] for k in STsignal})\n",
    "# STgroup.update({'-'+k: STgroup[k] for k in STgroup})\n",
    "# Tsignal.update({'-'+k: -Tsignal[k] for k in Tsignal})\n",
    "# Tgroup.update({'-'+k: Tgroup[k] for k in Tgroup})\n",
    "# TPsignal.update({'-'+k: -TPsignal[k] for k in TPsignal})\n",
    "# TPgroup.update({'-'+k: TPgroup[k] for k in TPgroup})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(Pgroup))\n",
    "# print(len(PQgroup))\n",
    "# print(len(QRSgroup))\n",
    "# print(len(STgroup))\n",
    "# print(len(Tgroup))\n",
    "# print(len(TPgroup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixup - Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number = 1\n",
    "\n",
    "# too_much_it_squares_amount_of_data\n",
    "\n",
    "# permuted = np.random.permutation(list(Psignal))\n",
    "# for k1 in tqdm.tqdm(list(Psignal.keys())):\n",
    "#     visited = {}\n",
    "#     (k_all_1,g_1) = Pgroup[k1]\n",
    "#     counter = 0\n",
    "#     for k2 in permuted:\n",
    "#         (k_all_2,g_2) = Pgroup[k2]\n",
    "#         if (k1 != k2) & (g_1 != g_2):\n",
    "#             visited[g_2] = visited.get(g_2,0)+1\n",
    "#             if visited[g_2] > number:\n",
    "#                 continue\n",
    "#             if Psignal[k1].size != Psignal[k2].size:\n",
    "#                 intlen = np.random.randint(min([Psignal[k1].size,Psignal[k2].size]),max([Psignal[k1].size,Psignal[k2].size]))\n",
    "#                 x1 = sp.interpolate.interp1d(np.linspace(0,1,Psignal[k1].size),Psignal[k1])(np.linspace(0,1,intlen))\n",
    "#                 x2 = sp.interpolate.interp1d(np.linspace(0,1,Psignal[k2].size),Psignal[k2])(np.linspace(0,1,intlen))\n",
    "#             else:\n",
    "#                 x1 = Psignal[k1]\n",
    "#                 x2 = Psignal[k2]\n",
    "#             (xhat,lmbda) = utils.data.augmentation.mixup(x1,x2,5.,1.5)\n",
    "#             Psignal[k1+'m'+str(counter)] = xhat.squeeze()\n",
    "#             Pgroup[k1+'m'+str(counter)] = Pgroup[k1]\n",
    "#             counter += 1\n",
    "\n",
    "# permuted = np.random.permutation(list(QRSsignal))\n",
    "# for k1 in tqdm.tqdm(list(QRSsignal.keys())):\n",
    "#     visited = {}\n",
    "#     (k_all_1,g_1) = QRSgroup[k1]\n",
    "#     counter = 0\n",
    "#     for k2 in permuted:\n",
    "#         (k_all_2,g_2) = QRSgroup[k2]\n",
    "#         if (k1 != k2) & (g_1 != g_2):\n",
    "#             visited[g_2] = visited.get(g_2,0)+1\n",
    "#             if visited[g_2] > number:\n",
    "#                 continue\n",
    "#             if QRSsignal[k1].size != QRSsignal[k2].size:\n",
    "#                 intlen = np.random.randint(min([QRSsignal[k1].size,QRSsignal[k2].size]),max([QRSsignal[k1].size,QRSsignal[k2].size]))\n",
    "#                 x1 = sp.interpolate.interp1d(np.linspace(0,1,QRSsignal[k1].size),QRSsignal[k1])(np.linspace(0,1,intlen))\n",
    "#                 x2 = sp.interpolate.interp1d(np.linspace(0,1,QRSsignal[k2].size),QRSsignal[k2])(np.linspace(0,1,intlen))\n",
    "#             else:\n",
    "#                 x1 = QRSsignal[k1]\n",
    "#                 x2 = QRSsignal[k2]\n",
    "#             (xhat,lmbda) = utils.data.augmentation.mixup(x1,x2,5.,1.5)\n",
    "#             QRSsignal[k1+'m'+str(counter)] = xhat.squeeze()\n",
    "#             QRSgroup[k1+'m'+str(counter)] = QRSgroup[k1]\n",
    "#             counter += 1\n",
    "\n",
    "# permuted = np.random.permutation(list(Tsignal))\n",
    "# for k1 in tqdm.tqdm(list(Tsignal.keys())):\n",
    "#     visited = {}\n",
    "#     (k_all_1,g_1) = Tgroup[k1]\n",
    "#     counter = 0\n",
    "#     for k2 in permuted:\n",
    "#         (k_all_2,g_2) = Tgroup[k2]\n",
    "#         if (k1 != k2) & (g_1 != g_2):\n",
    "#             visited[g_2] = visited.get(g_2,0)+1\n",
    "#             if visited[g_2] > number:\n",
    "#                 continue\n",
    "#             if Tsignal[k1].size != Tsignal[k2].size:\n",
    "#                 intlen = np.random.randint(min([Tsignal[k1].size,Tsignal[k2].size]),max([Tsignal[k1].size,Tsignal[k2].size]))\n",
    "#                 x1 = sp.interpolate.interp1d(np.linspace(0,1,Tsignal[k1].size),Tsignal[k1])(np.linspace(0,1,intlen))\n",
    "#                 x2 = sp.interpolate.interp1d(np.linspace(0,1,Tsignal[k2].size),Tsignal[k2])(np.linspace(0,1,intlen))\n",
    "#             else:\n",
    "#                 x1 = Tsignal[k1]\n",
    "#                 x2 = Tsignal[k2]\n",
    "#             (xhat,lmbda) = utils.data.augmentation.mixup(x1,x2,5.,1.5)\n",
    "#             Tsignal[k1+'m'+str(counter)] = xhat.squeeze()\n",
    "#             Tgroup[k1+'m'+str(counter)] = Tgroup[k1]\n",
    "#             counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(Pgroup))\n",
    "# print(len(PQgroup))\n",
    "# print(len(QRSgroup))\n",
    "# print(len(STgroup))\n",
    "# print(len(Tgroup))\n",
    "# print(len(TPgroup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate wavelets\n",
    "Pwavelet = {k: utils.wavelet.transform(Psignal[k],250.).squeeze() for k in tqdm.tqdm(Psignal.keys())}\n",
    "PQwavelet = {k: utils.wavelet.transform(PQsignal[k],250.).squeeze() for k in tqdm.tqdm(PQsignal.keys())}\n",
    "QRSwavelet = {k: utils.wavelet.transform(QRSsignal[k],250.).squeeze() for k in tqdm.tqdm(QRSsignal.keys())}\n",
    "STwavelet = {k: utils.wavelet.transform(STsignal[k],250.).squeeze() for k in tqdm.tqdm(STsignal.keys())}\n",
    "Twavelet = {k: utils.wavelet.transform(Tsignal[k],250.).squeeze() for k in tqdm.tqdm(Tsignal.keys())}\n",
    "TPwavelet = {k: utils.wavelet.transform(TPsignal[k],250.).squeeze() for k in tqdm.tqdm(TPsignal.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate criteria\n",
    "s = 3 # wavelet scale\n",
    "eps = np.finfo('float').eps\n",
    "Pcriteria = {k: ((np.sign(Pwavelet[k][0,s]),(Pwavelet[k][0,s]-Pwavelet[k][1,s]+eps)/(np.max(Pwavelet[k][:,s])-np.min(Pwavelet[k][:,s]+eps))),\n",
    "                 (np.sign(Pwavelet[k][-1,s]),(Pwavelet[k][-1,s]-Pwavelet[k][-2,s]+eps)/(np.max(Pwavelet[k][:,s])-np.min(Pwavelet[k][:,s]+eps)))) for k in Psignal.keys()}\n",
    "PQcriteria = {k: ((np.sign(PQwavelet[k][0,s]),(PQwavelet[k][0,s]-PQwavelet[k][1,s]+eps)/(np.max(PQwavelet[k][:,s])-np.min(PQwavelet[k][:,s]+eps))),\n",
    "                 (np.sign(PQwavelet[k][-1,s]),(PQwavelet[k][-1,s]-PQwavelet[k][-2,s]+eps)/(np.max(PQwavelet[k][:,s])-np.min(PQwavelet[k][:,s]+eps)))) for k in PQsignal.keys()}\n",
    "QRScriteria = {k: ((np.sign(QRSwavelet[k][0,s]),(QRSwavelet[k][0,s]-QRSwavelet[k][1,s]+eps)/(np.max(QRSwavelet[k][:,s])-np.min(QRSwavelet[k][:,s]+eps))),\n",
    "                 (np.sign(QRSwavelet[k][-1,s]),(QRSwavelet[k][-1,s]-QRSwavelet[k][-2,s]+eps)/(np.max(QRSwavelet[k][:,s])-np.min(QRSwavelet[k][:,s]+eps)))) for k in QRSsignal.keys()}\n",
    "STcriteria = {k: ((np.sign(STwavelet[k][0,s]),(STwavelet[k][0,s]-STwavelet[k][1,s]+eps)/(np.max(STwavelet[k][:,s])-np.min(STwavelet[k][:,s]+eps))),\n",
    "                 (np.sign(STwavelet[k][-1,s]),(STwavelet[k][-1,s]-STwavelet[k][-2,s]+eps)/(np.max(STwavelet[k][:,s])-np.min(STwavelet[k][:,s]+eps)))) for k in STsignal.keys()}\n",
    "Tcriteria = {k: ((np.sign(Twavelet[k][0,s]),(Twavelet[k][0,s]-Twavelet[k][1,s]+eps)/(np.max(Twavelet[k][:,s])-np.min(Twavelet[k][:,s]+eps))),\n",
    "                 (np.sign(Twavelet[k][-1,s]),(Twavelet[k][-1,s]-Twavelet[k][-2,s]+eps)/(np.max(Twavelet[k][:,s])-np.min(Twavelet[k][:,s]+eps)))) for k in Tsignal.keys()}\n",
    "TPcriteria = {k: ((np.sign(TPwavelet[k][0,s]),(TPwavelet[k][0,s]-TPwavelet[k][1,s]+eps)/(np.max(TPwavelet[k][:,s])-np.min(TPwavelet[k][:,s]+eps))),\n",
    "                 (np.sign(TPwavelet[k][-1,s]),(TPwavelet[k][-1,s]-TPwavelet[k][-2,s]+eps)/(np.max(TPwavelet[k][:,s])-np.min(TPwavelet[k][:,s]+eps)))) for k in TPsignal.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sample record with QT and LUDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkeys = list(Psignal.keys())\n",
    "PQkeys = list(PQsignal.keys())\n",
    "QRSkeys = list(QRSsignal.keys())\n",
    "STkeys = list(STsignal.keys())\n",
    "Tkeys = list(Tsignal.keys())\n",
    "TPkeys = list(TPsignal.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "N = 2048\n",
    "\n",
    "# Hyperparams\n",
    "size = 20\n",
    "onset = np.random.randint(0,50)\n",
    "begining_wave = np.random.randint(0,6)\n",
    "has_P = np.random.rand(1) > 0.1\n",
    "has_PQ = np.random.rand(1) > 0.2\n",
    "has_ST = np.random.rand(1) > 0.2\n",
    "has_BBB = np.random.rand(1) > 0.9\n",
    "proba_P = 0.15\n",
    "proba_PQ = 0.15\n",
    "proba_QRS = 0.01\n",
    "proba_ST = 0.15\n",
    "\n",
    "##### Data structure\n",
    "ids = []\n",
    "\n",
    "##### Identifiers\n",
    "id_P = np.random.randint(0,len(Psignal),size=size)\n",
    "id_PQ = np.random.randint(0,len(PQsignal),size=size)\n",
    "id_QRS = np.random.randint(0,len(QRSsignal),size=size)\n",
    "id_ST = np.random.randint(0,len(STsignal),size=size)\n",
    "id_T = np.random.randint(0,len(Tsignal),size=size)\n",
    "id_TP = np.random.randint(0,len(TPsignal),size=size)\n",
    "\n",
    "# In case QRS is not expressed\n",
    "filt_QRS = np.random.rand(size) < proba_QRS\n",
    "\n",
    "# P wave\n",
    "id_P[(np.random.rand(size) < proba_P) | np.logical_not(has_P)] = -1\n",
    "id_PQ[filt_QRS | (np.random.rand(size) < proba_PQ) | np.logical_not(has_PQ)] = -1\n",
    "id_QRS[filt_QRS] = -1\n",
    "id_ST[filt_QRS | (np.random.rand(size) < proba_ST) | np.logical_not(has_ST)] = -1\n",
    "id_T[filt_QRS] = -1\n",
    "\n",
    "beats = []\n",
    "masks = []\n",
    "offset = 0\n",
    "record_size = 0\n",
    "mark_break = False\n",
    "for i in range(size):\n",
    "    for j in range(6):\n",
    "        if (i == 0) and (j < begining_wave): \n",
    "            continue\n",
    "        if (j == 0) and (id_P[i] != -1):\n",
    "            beats.append(trailonset(Psignal[Pkeys[id_P[i]]],offset))\n",
    "            masks.append(np.full((beats[-1].size,),1,dtype='int8'))\n",
    "            offset = beats[-1][-1]\n",
    "            record_size += beats[-1].size\n",
    "        if (j == 1) and (id_PQ[i] != -1):\n",
    "            beats.append(trailonset(PQsignal[PQkeys[id_PQ[i]]],offset))\n",
    "            masks.append(np.zeros((beats[-1].size,),dtype='int8'))\n",
    "            offset = beats[-1][-1]\n",
    "            record_size += beats[-1].size\n",
    "        if (j == 2) and (id_QRS[i] != -1):\n",
    "            beats.append(trailonset(QRSsignal[QRSkeys[id_QRS[i]]],offset))\n",
    "            masks.append(np.full((beats[-1].size,),2,dtype='int8'))\n",
    "            offset = beats[-1][-1]\n",
    "            record_size += beats[-1].size\n",
    "        if (j == 3) and (id_ST[i] != -1):\n",
    "            beats.append(trailonset(STsignal[STkeys[id_ST[i]]],offset))\n",
    "            masks.append(np.zeros((beats[-1].size,),dtype='int8'))\n",
    "            offset = beats[-1][-1]\n",
    "            record_size += beats[-1].size\n",
    "        if (j == 4) and (id_T[i] != -1):\n",
    "            beats.append(trailonset(Tsignal[Tkeys[id_T[i]]],offset))\n",
    "            masks.append(np.full((beats[-1].size,),3,dtype='int8'))\n",
    "            offset = beats[-1][-1]\n",
    "            record_size += beats[-1].size\n",
    "        if (j == 5) and (id_TP[i] != -1):\n",
    "            beats.append(trailonset(TPsignal[TPkeys[id_TP[i]]],offset))\n",
    "            masks.append(np.zeros((beats[-1].size,),dtype='int8'))\n",
    "            offset = beats[-1][-1]\n",
    "            record_size += beats[-1].size\n",
    "        if (record_size-onset) >= N:\n",
    "            mark_break = True\n",
    "            break\n",
    "    if mark_break:\n",
    "        break\n",
    "        \n",
    "# Obtain final stuff\n",
    "signal = np.concatenate(beats)\n",
    "masks = np.concatenate(masks)\n",
    "masks_all = np.zeros((record_size,3),dtype=bool)\n",
    "masks_all[:,0] = (masks == 1)\n",
    "masks_all[:,1] = (masks == 2)\n",
    "masks_all[:,2] = (masks == 3)\n",
    "\n",
    "# Move onset\n",
    "signal = signal[onset:onset+N]\n",
    "masks_all = masks_all[onset:onset+N,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mskplt = ((np.max(signal)-np.min(signal))*masks_all)+np.min(signal)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(signal)\n",
    "plt.gca().fill_between(np.arange(N), mskplt[:,0], mskplt[:,0].min(), linewidth=0, alpha=0.15, color='red')\n",
    "plt.gca().fill_between(np.arange(N), mskplt[:,1], mskplt[:,1].min(), linewidth=0, alpha=0.15, color='green')\n",
    "plt.gca().fill_between(np.arange(N), mskplt[:,2], mskplt[:,2].min(), linewidth=0, alpha=0.15, color='magenta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 2048\n",
    "# s = 3\n",
    "\n",
    "# has_P = np.random.rand(1) > 0.1\n",
    "# has_PQ = np.random.rand(1) > 0.2\n",
    "# has_ST = np.random.rand(1) > 0.2\n",
    "# has_BBB = np.random.rand(1) > 0.9\n",
    "# counter_BBB = 0\n",
    "# repetitions_BBB = np.random.randint(2,4)\n",
    "\n",
    "# beats = []\n",
    "# ids = []\n",
    "\n",
    "# # Include first beat\n",
    "# ids.append(('TPsignal',np.random.randint(0,len(TPsignal))))\n",
    "# beats.append(utils.signal.on_off_correction(TPsignal[list(TPsignal)[ids[-1][1]]]))\n",
    "# size = beats[0].size\n",
    "# masks = np.zeros((size,),dtype='int8')\n",
    "# onset = np.random.randint(0,size)\n",
    "# while size-onset < N:\n",
    "#     # P wave (sometimes)\n",
    "#     if has_BBB:\n",
    "#         if counter_BBB == 0:\n",
    "#             id_BBB_P = ('Psignal',np.random.randint(0,len(Psignal)))\n",
    "#         ids.append(id_BBB_P)\n",
    "#         p = trailonset(Psignal[list(Psignal)[ids[-1][1]]],beats[-1][-1])[1:]\n",
    "#         beats.append(p)\n",
    "#         masks = np.concatenate((masks,1*np.ones((p.size,),dtype='int8')))\n",
    "\n",
    "#         if has_PQ:\n",
    "#             # PQ segment\n",
    "#             ids.append(('PQsignal',np.random.randint(0,len(PQsignal))))\n",
    "#             pq = trailonset(PQsignal[list(PQsignal)[ids[-1][1]]],beats[-1][-1])[1:]\n",
    "#             beats.append(pq)\n",
    "#             masks = np.concatenate((masks,np.zeros((pq.size,),dtype='int8')))\n",
    "#     elif (np.random.rand(1) < 0.75) and (has_P):\n",
    "#         ids.append(('Psignal',np.random.randint(0,len(Psignal))))\n",
    "#         p = trailonset(Psignal[list(Psignal)[ids[-1][1]]],beats[-1][-1])[1:]\n",
    "#         beats.append(p)\n",
    "#         masks = np.concatenate((masks,1*np.ones((p.size,),dtype='int8')))\n",
    "\n",
    "#         if has_PQ:\n",
    "#             # PQ segment\n",
    "#             ids.append(('PQsignal',np.random.randint(0,len(PQsignal))))\n",
    "#             pq = trailonset(PQsignal[list(PQsignal)[ids[-1][1]]],beats[-1][-1])[1:]\n",
    "#             beats.append(pq)\n",
    "#             masks = np.concatenate((masks,np.zeros((pq.size,),dtype='int8')))\n",
    "\n",
    "#     # QRS wave\n",
    "#     has_QRS = np.random.rand(1)\n",
    "#     if has_BBB:\n",
    "#         if counter_BBB%repetitions_BBB == 0:\n",
    "#             id_BBB_QRS = ('QRSsignal',np.random.randint(0,len(QRSsignal)))\n",
    "#             ids.append(id_BBB_QRS)\n",
    "#             qrs = trailonset(QRSsignal[list(QRSsignal)[ids[-1][1]]],beats[-1][-1])[1:]\n",
    "#             beats.append(qrs)\n",
    "#             masks = np.concatenate((masks,2*np.ones((qrs.size,),dtype='int8')))\n",
    "#         else:\n",
    "#             pass\n",
    "#     elif (has_QRS < 0.99):\n",
    "#         ids.append(('QRSsignal',np.random.randint(0,len(QRSsignal))))\n",
    "#         qrs = trailonset(QRSsignal[list(QRSsignal)[ids[-1][1]]],beats[-1][-1])[1:]\n",
    "#         beats.append(qrs)\n",
    "#         masks = np.concatenate((masks,2*np.ones((qrs.size,),dtype='int8')))\n",
    "    \n",
    "#     # ST segment\n",
    "#     if has_BBB and (counter_BBB%repetitions_BBB != 0):\n",
    "#         pass\n",
    "#     elif (np.random.rand(1) < 0.75) and (has_ST):\n",
    "#         ids.append(('STsignal',np.random.randint(0,len(STsignal))))\n",
    "#         st = trailonset(STsignal[list(STsignal)[ids[-1][1]]],beats[-1][-1])[1:]\n",
    "#         beats.append(st)\n",
    "#         masks = np.concatenate((masks,np.zeros((st.size,),dtype='int8')))\n",
    "\n",
    "#     # T wave\n",
    "#     if has_BBB:\n",
    "#         if counter_BBB%repetitions_BBB == 0:\n",
    "#             ids.append(('Tsignal',np.random.randint(0,len(Tsignal))))\n",
    "#             t = trailonset(Tsignal[list(Tsignal)[ids[-1][1]]],beats[-1][-1])[1:]\n",
    "#             beats.append(t)\n",
    "#             masks = np.concatenate((masks,3*np.ones((t.size,),dtype='int8')))\n",
    "#         else:\n",
    "#             pass\n",
    "#     elif (has_QRS < 0.99):\n",
    "#         ids.append(('Tsignal',np.random.randint(0,len(Tsignal))))\n",
    "#         t = trailonset(Tsignal[list(Tsignal)[ids[-1][1]]],beats[-1][-1])[1:]\n",
    "#         beats.append(t)\n",
    "#         masks = np.concatenate((masks,3*np.ones((t.size,),dtype='int8')))\n",
    "\n",
    "#     # TP segment\n",
    "#     if has_BBB:\n",
    "#         if counter_BBB == 0:\n",
    "#             id_BBB_TP = ('TPsignal',np.random.randint(0,len(TPsignal)))\n",
    "#         ids.append(id_BBB_TP)\n",
    "#         tp = trailonset(TPsignal[list(TPsignal)[ids[-1][1]]],beats[-1][-1])[1:]\n",
    "#         beats.append(tp)\n",
    "#         masks = np.concatenate((masks,np.zeros((tp.size,),dtype='int8')))\n",
    "#     else:\n",
    "#         ids.append(('TPsignal',np.random.randint(0,len(TPsignal))))\n",
    "#         tp = trailonset(TPsignal[list(TPsignal)[ids[-1][1]]],beats[-1][-1])[1:]\n",
    "#         beats.append(tp)\n",
    "#         masks = np.concatenate((masks,np.zeros((tp.size,),dtype='int8')))\n",
    "    \n",
    "#     # Account for total signal size\n",
    "#     size = sum([beats[i].size for i in range(len(beats))])\n",
    "        \n",
    "#     # Update BBB counter\n",
    "#     if has_BBB:\n",
    "#         counter_BBB += 1\n",
    "\n",
    "# sig = np.concatenate(beats)[onset:onset+2048]\n",
    "# # sig = sp.signal.filtfilt(*sp.signal.butter(4,   0.5/250, 'high'),sig)\n",
    "# signal = sig# + np.convolve(np.cumsum(norm.rvs(scale=0.15**(2*0.5),size=N)),np.hamming(w)/(w/2),mode='same')\n",
    "\n",
    "# masks = masks[onset:onset+2048]\n",
    "# masks_all = np.zeros((N,3),dtype=bool)\n",
    "# masks_all[:,0] = masks == 1\n",
    "# masks_all[:,1] = masks == 2\n",
    "# masks_all[:,2] = masks == 3\n",
    "# mskplt = ((np.max(signal)-np.min(signal))*masks_all)+np.min(signal)\n",
    "\n",
    "# # f,ax = plt.subplots(nrows=1,figsize=(20,4))\n",
    "# # ax = np.array(ax)\n",
    "# # if len(ax.shape) == 0: ax = ax[None]\n",
    "# # [ax[i].set_xlim([0,N]) for i in range(ax.size)]\n",
    "# # [ax[i].fill_between(np.arange(N), mskplt[:,0], mskplt[:,0].min(), linewidth=0, alpha=0.15, color='red') for i in range(ax.size)]\n",
    "# # [ax[i].fill_between(np.arange(N), mskplt[:,1], mskplt[:,1].min(), linewidth=0, alpha=0.15, color='green') for i in range(ax.size)]\n",
    "# # [ax[i].fill_between(np.arange(N), mskplt[:,2], mskplt[:,2].min(), linewidth=0, alpha=0.15, color='magenta') for i in range(ax.size)]\n",
    "# # ax[0].plot(signal)\n",
    "# # # ax[1].plot(wvlts,color='orange')\n",
    "# # # ax[2].plot(wvlts_signal,color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(P)\n",
    "plt.plot(QRS)\n",
    "plt.plot(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.concatenate((P,QRS,T)))\n",
    "plt.plot(np.concatenate((Pw,QRSw,Tw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2048\n",
    "\n",
    "beats = []\n",
    "ids = []\n",
    "\n",
    "# Include first beat\n",
    "ids.append(('TPsignal',np.random.randint(0,len(TPsignal))))\n",
    "beats.append(utils.signal.on_off_correction(TPsignal[list(TPsignal)[ids[-1][1]]]))\n",
    "# beats.append(TPsignal[list(TPsignal)[np.random.randint(0,len(TPsignal))]])\n",
    "size = beats[0].size\n",
    "masks = np.zeros((size,))\n",
    "onset = np.random.randint(0,size)\n",
    "while size-onset < N:\n",
    "    # P wave (sometimes)\n",
    "    if np.random.rand(1) < 0.75:\n",
    "        # p = utils.signal.on_off_correction(Psignal[list(Psignal)[np.random.randint(0,len(Psignal))]])\n",
    "        ids.append(('Psignal',np.random.randint(0,len(Psignal))))\n",
    "        p = trailonset(Psignal[list(Psignal)[ids[-1][1]]],beats[-1][-1])\n",
    "        # p = Psignal[list(Psignal)[np.random.randint(0,len(Psignal))]]\n",
    "        beats.append(p)\n",
    "        masks = np.concatenate((masks,1*np.ones((p.size,))))\n",
    "\n",
    "    # PQ segment\n",
    "    # pq = utils.signal.on_off_correction(PQsignal[list(PQsignal)[np.random.randint(0,len(PQsignal))]])\n",
    "    ids.append(('PQsignal',np.random.randint(0,len(PQsignal))))\n",
    "    pq = trailonset(PQsignal[list(PQsignal)[ids[-1][1]]],beats[-1][-1])\n",
    "    # pq = PQsignal[list(PQsignal)[np.random.randint(0,len(PQsignal))]]\n",
    "    beats.append(pq)\n",
    "    masks = np.concatenate((masks,np.zeros((pq.size,))))\n",
    "\n",
    "    # QRS wave\n",
    "    # qrs = utils.signal.on_off_correction(QRSsignal[list(QRSsignal)[np.random.randint(0,len(QRSsignal))]])\n",
    "    ids.append(('QRSsignal',np.random.randint(0,len(QRSsignal))))\n",
    "    qrs = trailonset(QRSsignal[list(QRSsignal)[ids[-1][1]]],beats[-1][-1])\n",
    "    # qrs = QRSsignal[list(QRSsignal)[np.random.randint(0,len(QRSsignal))]]\n",
    "    beats.append(qrs)\n",
    "    masks = np.concatenate((masks,2*np.ones((qrs.size,))))\n",
    "\n",
    "    # ST segment\n",
    "    # st = utils.signal.on_off_correction(STsignal[list(STsignal)[np.random.randint(0,len(STsignal))]])\n",
    "    ids.append(('STsignal',np.random.randint(0,len(STsignal))))\n",
    "    st = trailonset(STsignal[list(STsignal)[ids[-1][1]]],beats[-1][-1])\n",
    "    # st = STsignal[list(STsignal)[np.random.randint(0,len(STsignal))]]\n",
    "    beats.append(st)\n",
    "    masks = np.concatenate((masks,np.zeros((st.size,))))\n",
    "\n",
    "    # T wave\n",
    "    # t = utils.signal.on_off_correction(Tsignal[list(Tsignal)[np.random.randint(0,len(Tsignal))]])\n",
    "    ids.append(('Tsignal',np.random.randint(0,len(Tsignal))))\n",
    "    t = trailonset(Tsignal[list(Tsignal)[ids[-1][1]]],beats[-1][-1])\n",
    "    # t = Tsignal[list(Tsignal)[np.random.randint(0,len(Tsignal))]]\n",
    "    beats.append(t)\n",
    "    masks = np.concatenate((masks,3*np.ones((t.size,))))\n",
    "\n",
    "    # TP segment\n",
    "    # tp = utils.signal.on_off_correction(TPsignal[list(TPsignal)[np.random.randint(0,len(TPsignal))]])\n",
    "    ids.append(('TPsignal',np.random.randint(0,len(TPsignal))))\n",
    "    tp = trailonset(TPsignal[list(TPsignal)[ids[-1][1]]],beats[-1][-1])\n",
    "    # tp = TPsignal[list(TPsignal)[np.random.randint(0,len(TPsignal))]]\n",
    "    beats.append(tp)\n",
    "    masks = np.concatenate((masks,np.zeros((tp.size,))))\n",
    "\n",
    "    size = sum([beats[i].size for i in range(len(beats))])\n",
    "\n",
    "w = 51\n",
    "sig = np.concatenate(beats)[onset:onset+2048]\n",
    "sig = sp.signal.filtfilt(*sp.signal.butter(4,   0.5/250, 'high'),sig)\n",
    "signal = sig# + np.convolve(np.cumsum(norm.rvs(scale=0.15**(2*0.5),size=N)),np.hamming(w)/(w/2),mode='same')\n",
    "\n",
    "masks = masks[onset:onset+2048]\n",
    "masks_all = np.zeros((N,3),dtype=bool)\n",
    "masks_all[:,0] = masks == 1\n",
    "masks_all[:,1] = masks == 2\n",
    "masks_all[:,2] = masks == 3\n",
    "mskplt = ((np.max(signal)-np.min(signal))*masks_all)+np.min(signal)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(signal)\n",
    "plt.gca().fill_between(np.arange(N), mskplt[:,0], mskplt[:,0].min(), linewidth=0, alpha=0.15, color='red')\n",
    "plt.gca().fill_between(np.arange(N), mskplt[:,1], mskplt[:,1].min(), linewidth=0, alpha=0.15, color='green')\n",
    "plt.gca().fill_between(np.arange(N), mskplt[:,2], mskplt[:,2].min(), linewidth=0, alpha=0.15, color='magenta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 46000\n",
    "f = TPsignal\n",
    "print(list(f)[i])\n",
    "plt.plot(f[list(f)[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fiducials to delete\n",
    "\n",
    "* QRS - 111_AVF_24\n",
    "* QRS - sel820_1_156\n",
    "* QRS - 95_AVR_16\n",
    "* ~TP - sel306_1_211~\n",
    "* ~TP - sel114_0_179~\n",
    "* PT - sel803_0_109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning3",
   "language": "python",
   "name": "deeplearning3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
