{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import random\n",
    "import uuid\n",
    "import os\n",
    "import os.path\n",
    "import skimage\n",
    "import utils\n",
    "import utils.wavelet\n",
    "import utils.data\n",
    "import utils.data.augmentation\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import networkx.algorithms.approximation\n",
    "import wfdb\n",
    "import json\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from utils.signal import StandardHeader\n",
    "\n",
    "# Data loader to un-clutter code    \n",
    "def load_data(filepath):\n",
    "    dic = dict()\n",
    "    with open(filepath) as f:\n",
    "        text = list(f)\n",
    "    for line in text:\n",
    "        line = line.replace(' ','').replace('\\n','').replace(',,','')\n",
    "        if line[-1] == ',': line = line[:-1]\n",
    "        head = line.split(',')[0]\n",
    "        tail = line.split(',')[1:]\n",
    "        if tail == ['']:\n",
    "            tail = np.asarray([])\n",
    "        else:\n",
    "            tail = np.asarray(tail).astype(int)\n",
    "\n",
    "        dic[head] = tail\n",
    "    return dic\n",
    "\n",
    "\n",
    "def trailonset(sig,on):\n",
    "    on = on-sig[0]\n",
    "    off = on-sig[0]+sig[-1]\n",
    "    sig = sig+np.linspace(on,off,sig.size)\n",
    "    \n",
    "    return sig\n",
    "\n",
    "def getcorr(segments):\n",
    "    if len(segments) > 0:\n",
    "        length = 2*max([segments[i][2].size for i in range(len(segments))])\n",
    "    else:\n",
    "        return np.zeros((0,0))\n",
    "\n",
    "    corr = np.zeros((len(segments),len(segments)))\n",
    "\n",
    "    for i in range(len(segments)):\n",
    "        for j in range(len(segments)):\n",
    "            if i != j:\n",
    "                if segments[i][2].size != segments[j][2].size:\n",
    "                    if segments[i][2].size != 1:\n",
    "                        x1 = sp.interpolate.interp1d(np.linspace(0,1,len(segments[i][2])),segments[i][2])(np.linspace(0,1,length))\n",
    "                    else:\n",
    "                        x1 = np.full((length,),segments[i][2][0])\n",
    "                    if segments[j][2].size != 1:\n",
    "                        x2 = sp.interpolate.interp1d(np.linspace(0,1,len(segments[j][2])),segments[j][2])(np.linspace(0,1,length))\n",
    "                    else:\n",
    "                        x2 = np.full((length,),segments[j][2][0])\n",
    "                else:\n",
    "                    x1 = segments[i][2]\n",
    "                    x2 = segments[j][2]\n",
    "                if (x1.size == 1) and (x2.size == 1):\n",
    "                    corr[i,j] = 1\n",
    "                else:\n",
    "                    c,_ = utils.signal.xcorr(x1,x2)\n",
    "                    corr[i,j] = np.max(np.abs(c))\n",
    "            else:\n",
    "                corr[i,j] = 1\n",
    "                \n",
    "    return corr\n",
    "\n",
    "def getdelete(segments, threshold):\n",
    "    corr = getcorr(segments)\n",
    "    \n",
    "    index_delete = []\n",
    "    \n",
    "    for i in range(corr.shape[0]):\n",
    "        if i in index_delete:\n",
    "            continue\n",
    "        for j in range(corr.shape[1]):\n",
    "            if j == i:\n",
    "                continue\n",
    "            if corr[i,j] > threshold:\n",
    "                if j not in index_delete:\n",
    "                    index_delete.append(j)\n",
    "                \n",
    "    return index_delete\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() in ['Linux', 'Linux2']:\n",
    "    basedir = '/media/guille/DADES/DADES/Delineator'\n",
    "else:\n",
    "    basedir = r'C:\\Users\\Emilio\\Documents\\DADES\\DADES\\Delineator'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LUDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:17<00:00, 11.22it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "Pon = {}\n",
    "Ppeak = {}\n",
    "Poff = {}\n",
    "QRSon = {}\n",
    "QRSpeak = {}\n",
    "QRSoff = {}\n",
    "Ton = {}\n",
    "Tpeak = {}\n",
    "Toff = {}\n",
    "group = {}\n",
    "\n",
    "for i in tqdm.tqdm(range(200)):\n",
    "    (signal, header) = wfdb.rdsamp(os.path.join(basedir,'ludb','{}'.format(i+1)))\n",
    "    sortOrder = np.where(np.array([x.upper() for x in header['sig_name']])[:,None] == StandardHeader)[1]\n",
    "    signal = signal[:,sortOrder]\n",
    "    if header['fs'] != 500:\n",
    "        print(header['fs'])\n",
    "    signal = sp.signal.decimate(signal,2,axis=0)\n",
    "    \n",
    "    # 1st step: reduce noise\n",
    "    signal = sp.signal.filtfilt(*sp.signal.butter(4,   0.5/250., 'high'),signal.T).T\n",
    "    signal = sp.signal.filtfilt(*sp.signal.butter(4, 125.0/250.,  'low'),signal.T).T\n",
    "\n",
    "    # 2nd step: retrieve onsets and offsets\n",
    "    for j in range(len(StandardHeader)):\n",
    "        lead = StandardHeader[j]\n",
    "        name = str(i+1)+\"_\"+lead\n",
    "        ann = wfdb.rdann(os.path.join(basedir,'ludb','{}'.format(i+1)),'atr_{}'.format(lead.lower()))\n",
    "        dataset[name] = signal[:,j]\n",
    "        \n",
    "        locP = np.where(np.array(ann.symbol) == 'p')[0]\n",
    "        if len(locP) != 0:\n",
    "            if locP[0]-1 < 0:\n",
    "                locP = locP[1:]\n",
    "            if locP[-1]+1 == len(ann.sample):\n",
    "                locP = locP[:-1]\n",
    "        Pon[name] = ann.sample[locP-1]//2\n",
    "        Ppeak[name] = ann.sample[locP]//2\n",
    "        Poff[name] = ann.sample[locP+1]//2\n",
    "\n",
    "        locQRS = np.where(np.array(ann.symbol) == 'N')[0]\n",
    "        if len(locQRS) != 0:\n",
    "            if locQRS[0]-1 < 0:\n",
    "                locQRS = locQRS[1:]\n",
    "            if locQRS[-1]+1 == len(ann.sample):\n",
    "                locQRS = locQRS[:-1]\n",
    "        QRSon[name] = ann.sample[locQRS-1]//2\n",
    "        QRSpeak[name] = ann.sample[locQRS]//2\n",
    "        QRSoff[name] = ann.sample[locQRS+1]//2\n",
    "\n",
    "        locT = np.where(np.array(ann.symbol) == 't')[0]\n",
    "        if len(locT) != 0:\n",
    "            if locT[0]-1 < 0:\n",
    "                locT = locT[1:]\n",
    "            if locT[-1]+1 == len(ann.sample):\n",
    "                locT = locT[:-1]\n",
    "        Ton[name] = ann.sample[locT-1]//2\n",
    "        Tpeak[name] = ann.sample[locT]//2\n",
    "        Toff[name] = ann.sample[locT+1]//2\n",
    "        \n",
    "        # Store group\n",
    "        group[name] = str(i+1)\n",
    "\n",
    "dataset = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2400/2400 [04:01<00:00,  9.93it/s]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.99\n",
    "\n",
    "PsignalLUDB = {}\n",
    "PQsignalLUDB = {}\n",
    "QRSsignalLUDB = {}\n",
    "STsignalLUDB = {}\n",
    "TsignalLUDB = {}\n",
    "TPsignalLUDB = {}\n",
    "\n",
    "PgroupLUDB = {}\n",
    "PQgroupLUDB = {}\n",
    "QRSgroupLUDB = {}\n",
    "STgroupLUDB = {}\n",
    "TgroupLUDB = {}\n",
    "TPgroupLUDB = {}\n",
    "\n",
    "for k in tqdm.tqdm(dataset.keys()):\n",
    "    # Buggy files\n",
    "    if k in (['116_{}'.format(h) for h in StandardHeader] + \n",
    "             ['104_{}'.format(h) for h in StandardHeader] + \n",
    "             ['103_III',]):\n",
    "        continue\n",
    "    pon = Pon.get(k,np.array([]))\n",
    "    pof = Poff.get(k,np.array([]))\n",
    "    qon = QRSon.get(k,np.array([]))\n",
    "    qof = QRSoff.get(k,np.array([]))\n",
    "    ton = Ton.get(k,np.array([]))\n",
    "    tof = Toff.get(k,np.array([]))\n",
    "    \n",
    "    unordered_samples = np.concatenate([pon,pof,qon,qof,ton,tof,]).astype(float)\n",
    "    unordered_symbols = np.concatenate([['Pon']*pon.size,['Poff']*pof.size,\n",
    "                                        ['QRSon']*qon.size,['QRSoff']*qof.size,\n",
    "                                        ['Ton']*ton.size,['Toff']*tof.size,])\n",
    "    # Sort fiducials taking logical orders if same sample of occurrence\n",
    "    # There is (I'm definitely sure) a better way to do it\n",
    "    samples = []\n",
    "    symbols = []\n",
    "    for i in range(unordered_samples.size):\n",
    "        minimum = np.where(unordered_samples == min(unordered_samples))[0]\n",
    "        if minimum.size == 1:\n",
    "            minimum = minimum[0]\n",
    "            samples.append(int(unordered_samples[minimum]))\n",
    "            symbols.append(unordered_symbols[minimum])\n",
    "            unordered_samples[minimum] = np.inf\n",
    "        elif minimum.size == 2:\n",
    "            if symbols[-1] == 'Pon':\n",
    "                if unordered_symbols[minimum[0]] == 'Poff':\n",
    "                    samples.append(int(unordered_samples[minimum[0]]))\n",
    "                    symbols.append(unordered_symbols[minimum[0]])\n",
    "                    unordered_samples[minimum[0]] = np.inf\n",
    "                elif unordered_symbols[minimum[1]] == 'Poff':\n",
    "                    samples.append(int(unordered_samples[minimum[1]]))\n",
    "                    symbols.append(unordered_symbols[minimum[1]])\n",
    "                    unordered_samples[minimum[1]] = np.inf\n",
    "            elif symbols[-1] == 'QRSon':\n",
    "                if unordered_symbols[minimum[0]] == 'QRSoff':\n",
    "                    samples.append(int(unordered_samples[minimum[0]]))\n",
    "                    symbols.append(unordered_symbols[minimum[0]])\n",
    "                    unordered_samples[minimum[0]] = np.inf\n",
    "                elif unordered_symbols[minimum[1]] == 'QRSoff':\n",
    "                    samples.append(int(unordered_samples[minimum[1]]))\n",
    "                    symbols.append(unordered_symbols[minimum[1]])\n",
    "                    unordered_samples[minimum[1]] = np.inf\n",
    "            elif symbols[-1] == 'Ton':\n",
    "                if unordered_symbols[minimum[0]] == 'Toff':\n",
    "                    samples.append(int(unordered_samples[minimum[0]]))\n",
    "                    symbols.append(unordered_symbols[minimum[0]])\n",
    "                    unordered_samples[minimum[0]] = np.inf\n",
    "                elif unordered_symbols[minimum[1]] == 'Toff':\n",
    "                    samples.append(int(unordered_samples[minimum[1]]))\n",
    "                    symbols.append(unordered_symbols[minimum[1]])\n",
    "                    unordered_samples[minimum[1]] = np.inf\n",
    "            else:\n",
    "                raise ValueError(\"Should not happen at all\")\n",
    "        else:\n",
    "            raise ValueError(\"Definitely should not happen. Check file {}\".format(k))\n",
    "    samples = np.array(samples)\n",
    "    symbols = np.array(symbols)\n",
    "    \n",
    "    # Extract segments\n",
    "    P = []\n",
    "    QRS = []\n",
    "    T = []\n",
    "    TP = []\n",
    "    PQ = []\n",
    "    ST = []\n",
    "\n",
    "    # Extract segments\n",
    "    for i in range(samples.size-1):\n",
    "        if samples[i] == samples[i+1]:\n",
    "            continue\n",
    "        if symbols[i] == 'Pon':\n",
    "            if symbols[i+1] == 'Poff':\n",
    "                P.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            else:\n",
    "                print(\"Check file {}. P onset not followed by offset\".format(k))\n",
    "        elif symbols[i] == 'QRSon':\n",
    "            if symbols[i+1] == 'QRSoff':\n",
    "                QRS.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            else:\n",
    "                print(\"Check file {}. QRS onset not followed by offset\".format(k))\n",
    "        elif symbols[i] == 'Ton':\n",
    "            if symbols[i+1] == 'Toff':\n",
    "                T.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            else:\n",
    "                print(\"Check file {}. T onset not followed by offset\".format(k))\n",
    "        elif symbols[i] == 'Poff':\n",
    "            if symbols[i+1] == 'Pon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'QRSon':\n",
    "                PQ.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'Ton':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] in ['Poff','QRSoff','Toff']:\n",
    "                print(\"Check file {}. P offset not followed by onset\".format(k))\n",
    "        elif symbols[i] == 'QRSoff':\n",
    "            if symbols[i+1] == 'Pon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'QRSon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'Ton':\n",
    "                ST.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] in ['Poff','QRSoff','Toff']:\n",
    "                print(\"Check file {}. P offset not followed by onset\".format(k))\n",
    "        elif symbols[i] == 'Toff':\n",
    "            if symbols[i+1] == 'Pon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'QRSon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'Ton':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] in ['Poff','QRSoff','Toff']:\n",
    "                print(\"Check file {}. P offset not followed by onset\".format(k))\n",
    "        else:\n",
    "            raise ValueError(\"This should definitely not happen\")\n",
    "\n",
    "    # Filter out too similar segments\n",
    "    corr = getcorr(P)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    P = [P[i] for i in range(len(P)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(QRS)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    QRS = [QRS[i] for i in range(len(QRS)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(T)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    T = [T[i] for i in range(len(T)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(TP)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    TP = [TP[i] for i in range(len(TP)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(PQ)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    PQ = [PQ[i] for i in range(len(PQ)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(ST)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    ST = [ST[i] for i in range(len(ST)) if i in nodesclique]\n",
    "    \n",
    "    # Store segments\n",
    "    for i in range(len(P)):\n",
    "        PsignalLUDB[k + '_' + str(i)] = P[i][2]\n",
    "        PgroupLUDB[k + '_' + str(i)] = (P[i][0],P[i][1])\n",
    "    for i in range(len(QRS)):\n",
    "        QRSsignalLUDB[k + '_' + str(i)] = QRS[i][2]\n",
    "        QRSgroupLUDB[k + '_' + str(i)] = (QRS[i][0],QRS[i][1])\n",
    "    for i in range(len(T)):\n",
    "        TsignalLUDB[k + '_' + str(i)] = T[i][2]\n",
    "        TgroupLUDB[k + '_' + str(i)] = (T[i][0],T[i][1])\n",
    "    for i in range(len(TP)):\n",
    "        TPsignalLUDB[k + '_' + str(i)] = TP[i][2]\n",
    "        TPgroupLUDB[k + '_' + str(i)] = (TP[i][0],TP[i][1])\n",
    "    for i in range(len(PQ)):\n",
    "        PQsignalLUDB[k + '_' + str(i)] = PQ[i][2]\n",
    "        PQgroupLUDB[k + '_' + str(i)] = (PQ[i][0],PQ[i][1])\n",
    "    for i in range(len(ST)):\n",
    "        STsignalLUDB[k + '_' + str(i)] = ST[i][2]\n",
    "        STgroupLUDB[k + '_' + str(i)] = (ST[i][0],ST[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13822\n",
      "9193\n",
      "11216\n",
      "13567\n",
      "12656\n",
      "16565\n"
     ]
    }
   ],
   "source": [
    "print(len(PgroupLUDB))\n",
    "print(len(PQgroupLUDB))\n",
    "print(len(QRSgroupLUDB))\n",
    "print(len(STgroupLUDB))\n",
    "print(len(TgroupLUDB))\n",
    "print(len(TPgroupLUDB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load QT db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD DATASETS ####\n",
    "dataset             = pd.read_csv(os.path.join(basedir,'QTDB','Dataset.csv'), index_col=0)\n",
    "dataset             = dataset.sort_index(axis=1)\n",
    "labels              = np.asarray(list(dataset)) # In case no data augmentation is applied\n",
    "description         = dataset.describe()\n",
    "group               = {k: '_'.join(k.split('_')[:-1]) for k in dataset}\n",
    "\n",
    "# Zero-center data\n",
    "for key in description:\n",
    "    dataset[key]    = (dataset[key] - description[key]['mean'])/description[key]['std']\n",
    "    \n",
    "# Filter the data\n",
    "for col in dataset:\n",
    "    dataset[col] = sp.signal.filtfilt(*sp.signal.butter(4,   0.5/250., 'high'),dataset[col].T).T\n",
    "    dataset[col] = sp.signal.filtfilt(*sp.signal.butter(4, 125.0/250.,  'low'),dataset[col].T).T\n",
    "    \n",
    "# Load fiducials\n",
    "Pon = load_data(os.path.join(basedir,'QTDB','PonNew.csv'))\n",
    "Poff = load_data(os.path.join(basedir,'QTDB','PoffNew.csv'))\n",
    "QRSon = load_data(os.path.join(basedir,'QTDB','QRSonNew.csv'))\n",
    "QRSoff = load_data(os.path.join(basedir,'QTDB','QRSoffNew.csv'))\n",
    "Ton = load_data(os.path.join(basedir,'QTDB','TonNew.csv'))\n",
    "Toff = load_data(os.path.join(basedir,'QTDB','ToffNew.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [06:18<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.99\n",
    "\n",
    "PsignalQTDB = {}\n",
    "PQsignalQTDB = {}\n",
    "QRSsignalQTDB = {}\n",
    "STsignalQTDB = {}\n",
    "TsignalQTDB = {}\n",
    "TPsignalQTDB = {}\n",
    "\n",
    "PgroupQTDB = {}\n",
    "PQgroupQTDB = {}\n",
    "QRSgroupQTDB = {}\n",
    "STgroupQTDB = {}\n",
    "TgroupQTDB = {}\n",
    "TPgroupQTDB = {}\n",
    "\n",
    "for k in tqdm.tqdm(dataset.keys()):\n",
    "    # Buggy files\n",
    "    if k in ['sel232_0', 'sel232_1']:\n",
    "        continue\n",
    "    pon = Pon.get(k,np.array([]))\n",
    "    pof = Poff.get(k,np.array([]))\n",
    "    qon = QRSon.get(k,np.array([]))\n",
    "    qof = QRSoff.get(k,np.array([]))\n",
    "    ton = Ton.get(k,np.array([]))\n",
    "    tof = Toff.get(k,np.array([]))\n",
    "    \n",
    "    unordered_samples = np.concatenate([pon,pof,qon,qof,ton,tof,]).astype(float)\n",
    "    unordered_symbols = np.concatenate([['Pon']*pon.size,['Poff']*pof.size,\n",
    "                                        ['QRSon']*qon.size,['QRSoff']*qof.size,\n",
    "                                        ['Ton']*ton.size,['Toff']*tof.size,])\n",
    "    # Sort fiducials taking logical orders if same sample of occurrence\n",
    "    # There is (I'm definitely sure) a better way to do it\n",
    "    samples = []\n",
    "    symbols = []\n",
    "    for i in range(unordered_samples.size):\n",
    "        minimum = np.where(unordered_samples == min(unordered_samples))[0]\n",
    "        if minimum.size == 1:\n",
    "            minimum = minimum[0]\n",
    "            samples.append(int(unordered_samples[minimum]))\n",
    "            symbols.append(unordered_symbols[minimum])\n",
    "            unordered_samples[minimum] = np.inf\n",
    "        elif minimum.size == 2:\n",
    "            if symbols[-1] == 'Pon':\n",
    "                if unordered_symbols[minimum[0]] == 'Poff':\n",
    "                    samples.append(int(unordered_samples[minimum[0]]))\n",
    "                    symbols.append(unordered_symbols[minimum[0]])\n",
    "                    unordered_samples[minimum[0]] = np.inf\n",
    "                elif unordered_symbols[minimum[1]] == 'Poff':\n",
    "                    samples.append(int(unordered_samples[minimum[1]]))\n",
    "                    symbols.append(unordered_symbols[minimum[1]])\n",
    "                    unordered_samples[minimum[1]] = np.inf\n",
    "            elif symbols[-1] == 'QRSon':\n",
    "                if unordered_symbols[minimum[0]] == 'QRSoff':\n",
    "                    samples.append(int(unordered_samples[minimum[0]]))\n",
    "                    symbols.append(unordered_symbols[minimum[0]])\n",
    "                    unordered_samples[minimum[0]] = np.inf\n",
    "                elif unordered_symbols[minimum[1]] == 'QRSoff':\n",
    "                    samples.append(int(unordered_samples[minimum[1]]))\n",
    "                    symbols.append(unordered_symbols[minimum[1]])\n",
    "                    unordered_samples[minimum[1]] = np.inf\n",
    "            elif symbols[-1] == 'Ton':\n",
    "                if unordered_symbols[minimum[0]] == 'Toff':\n",
    "                    samples.append(int(unordered_samples[minimum[0]]))\n",
    "                    symbols.append(unordered_symbols[minimum[0]])\n",
    "                    unordered_samples[minimum[0]] = np.inf\n",
    "                elif unordered_symbols[minimum[1]] == 'Toff':\n",
    "                    samples.append(int(unordered_samples[minimum[1]]))\n",
    "                    symbols.append(unordered_symbols[minimum[1]])\n",
    "                    unordered_samples[minimum[1]] = np.inf\n",
    "            else:\n",
    "                raise ValueError(\"Should not happen at all\")\n",
    "        else:\n",
    "            raise ValueError(\"Definitely should not happen. Check file {}\".format(k))\n",
    "    samples = np.array(samples)\n",
    "    symbols = np.array(symbols)\n",
    "    \n",
    "    # Extract segments\n",
    "    P = []\n",
    "    QRS = []\n",
    "    T = []\n",
    "    TP = []\n",
    "    PQ = []\n",
    "    ST = []\n",
    "\n",
    "    for i in range(samples.size-1):\n",
    "        if samples[i] == samples[i+1]:\n",
    "            continue\n",
    "        if symbols[i] == 'Pon':\n",
    "            if symbols[i+1] == 'Poff':\n",
    "                P.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            else:\n",
    "                print(\"Check file {}. P onset not followed by offset\".format(k))\n",
    "        elif symbols[i] == 'QRSon':\n",
    "            if symbols[i+1] == 'QRSoff':\n",
    "                QRS.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            else:\n",
    "                print(\"Check file {}. QRS onset not followed by offset\".format(k))\n",
    "        elif symbols[i] == 'Ton':\n",
    "            if symbols[i+1] == 'Toff':\n",
    "                T.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            else:\n",
    "                print(\"Check file {}. T onset not followed by offset\".format(k))\n",
    "        elif symbols[i] == 'Poff':\n",
    "            if symbols[i+1] == 'Pon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'QRSon':\n",
    "                PQ.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'Ton':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] in ['Poff','QRSoff','Toff']:\n",
    "                print(\"Check file {}. P offset not followed by onset\".format(k))\n",
    "        elif symbols[i] == 'QRSoff':\n",
    "            if symbols[i+1] == 'Pon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'QRSon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'Ton':\n",
    "                ST.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] in ['Poff','QRSoff','Toff']:\n",
    "                print(\"Check file {}. P offset not followed by onset\".format(k))\n",
    "        elif symbols[i] == 'Toff':\n",
    "            if symbols[i+1] == 'Pon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'QRSon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'Ton':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] in ['Poff','QRSoff','Toff']:\n",
    "                print(\"Check file {}. P offset not followed by onset\".format(k))\n",
    "        else:\n",
    "            raise ValueError(\"This should definitely not happen\")\n",
    "            \n",
    "    # Filter out too long TP segments (causing this to break)\n",
    "    TP = [TP[i] for i in range(len(TP)) if TP[i][2].size < 250]\n",
    "\n",
    "    # Filter out too similar segments\n",
    "    corr = getcorr(P)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    P = [P[i] for i in range(len(P)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(QRS)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    QRS = [QRS[i] for i in range(len(QRS)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(T)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    T = [T[i] for i in range(len(T)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(TP)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    TP = [TP[i] for i in range(len(TP)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(PQ)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    PQ = [PQ[i] for i in range(len(PQ)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(ST)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    ST = [ST[i] for i in range(len(ST)) if i in nodesclique]\n",
    "    \n",
    "    # Store segments\n",
    "    for i in range(len(P)):\n",
    "        PsignalQTDB[k + '_' + str(i)] = P[i][2]\n",
    "        PgroupQTDB[k + '_' + str(i)] = (P[i][0],P[i][1])\n",
    "    for i in range(len(QRS)):\n",
    "        QRSsignalQTDB[k + '_' + str(i)] = QRS[i][2]\n",
    "        QRSgroupQTDB[k + '_' + str(i)] = (QRS[i][0],QRS[i][1])\n",
    "    for i in range(len(T)):\n",
    "        TsignalQTDB[k + '_' + str(i)] = T[i][2]\n",
    "        TgroupQTDB[k + '_' + str(i)] = (T[i][0],T[i][1])\n",
    "    for i in range(len(TP)):\n",
    "        TPsignalQTDB[k + '_' + str(i)] = TP[i][2]\n",
    "        TPgroupQTDB[k + '_' + str(i)] = (TP[i][0],TP[i][1])\n",
    "    for i in range(len(PQ)):\n",
    "        PQsignalQTDB[k + '_' + str(i)] = PQ[i][2]\n",
    "        PQgroupQTDB[k + '_' + str(i)] = (PQ[i][0],PQ[i][1])\n",
    "    for i in range(len(ST)):\n",
    "        STsignalQTDB[k + '_' + str(i)] = ST[i][2]\n",
    "        STgroupQTDB[k + '_' + str(i)] = (ST[i][0],ST[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4717\n",
      "2672\n",
      "1912\n",
      "2732\n",
      "2756\n",
      "4710\n"
     ]
    }
   ],
   "source": [
    "print(len(PgroupQTDB))\n",
    "print(len(PQgroupQTDB))\n",
    "print(len(QRSgroupQTDB))\n",
    "print(len(STgroupQTDB))\n",
    "print(len(TgroupQTDB))\n",
    "print(len(TPgroupQTDB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD DATASETS ####\n",
    "dataset             = pd.read_csv(os.path.join(basedir,'QTDB','Dataset.csv'), index_col=0)\n",
    "dataset             = dataset.sort_index(axis=1)\n",
    "labels              = np.asarray(list(dataset)) # In case no data augmentation is applied\n",
    "description         = dataset.describe()\n",
    "group               = {k: '_'.join(k.split('_')[:-1]) for k in dataset}\n",
    "\n",
    "# Zero-center data\n",
    "for key in description:\n",
    "    dataset[key]    = (dataset[key] - description[key]['mean'])/description[key]['std']\n",
    "    \n",
    "# Filter the data\n",
    "for col in dataset:\n",
    "    dataset[col] = sp.signal.filtfilt(*sp.signal.butter(4,   0.5/250., 'high'),dataset[col].T).T\n",
    "    dataset[col] = sp.signal.filtfilt(*sp.signal.butter(4, 125.0/250.,  'low'),dataset[col].T).T\n",
    "    \n",
    "# Load fiducials\n",
    "Pon = load_data(os.path.join(basedir,'QTDB','PonNew.csv'))\n",
    "Poff = load_data(os.path.join(basedir,'QTDB','PoffNew.csv'))\n",
    "QRSon = load_data(os.path.join(basedir,'QTDB','QRSonNew.csv'))\n",
    "QRSoff = load_data(os.path.join(basedir,'QTDB','QRSoffNew.csv'))\n",
    "Ton = load_data(os.path.join(basedir,'QTDB','TonNew.csv'))\n",
    "Toff = load_data(os.path.join(basedir,'QTDB','ToffNew.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.99\n",
    "\n",
    "PsignalQTDB = {}\n",
    "PQsignalQTDB = {}\n",
    "QRSsignalQTDB = {}\n",
    "STsignalQTDB = {}\n",
    "TsignalQTDB = {}\n",
    "TPsignalQTDB = {}\n",
    "\n",
    "PgroupQTDB = {}\n",
    "PQgroupQTDB = {}\n",
    "QRSgroupQTDB = {}\n",
    "STgroupQTDB = {}\n",
    "TgroupQTDB = {}\n",
    "TPgroupQTDB = {}\n",
    "\n",
    "for k in tqdm.tqdm(dataset.keys()):\n",
    "    # Buggy files\n",
    "    if k in ['sel232_0', 'sel232_1']:\n",
    "        continue\n",
    "    pon = Pon.get(k,np.array([]))\n",
    "    pof = Poff.get(k,np.array([]))\n",
    "    qon = QRSon.get(k,np.array([]))\n",
    "    qof = QRSoff.get(k,np.array([]))\n",
    "    ton = Ton.get(k,np.array([]))\n",
    "    tof = Toff.get(k,np.array([]))\n",
    "    \n",
    "    unordered_samples = np.concatenate([pon,pof,qon,qof,ton,tof,]).astype(float)\n",
    "    unordered_symbols = np.concatenate([['Pon']*pon.size,['Poff']*pof.size,\n",
    "                                        ['QRSon']*qon.size,['QRSoff']*qof.size,\n",
    "                                        ['Ton']*ton.size,['Toff']*tof.size,])\n",
    "    # Sort fiducials taking logical orders if same sample of occurrence\n",
    "    # There is (I'm definitely sure) a better way to do it\n",
    "    samples = []\n",
    "    symbols = []\n",
    "    for i in range(unordered_samples.size):\n",
    "        minimum = np.where(unordered_samples == min(unordered_samples))[0]\n",
    "        if minimum.size == 1:\n",
    "            minimum = minimum[0]\n",
    "            samples.append(int(unordered_samples[minimum]))\n",
    "            symbols.append(unordered_symbols[minimum])\n",
    "            unordered_samples[minimum] = np.inf\n",
    "        elif minimum.size == 2:\n",
    "            if symbols[-1] == 'Pon':\n",
    "                if unordered_symbols[minimum[0]] == 'Poff':\n",
    "                    samples.append(int(unordered_samples[minimum[0]]))\n",
    "                    symbols.append(unordered_symbols[minimum[0]])\n",
    "                    unordered_samples[minimum[0]] = np.inf\n",
    "                elif unordered_symbols[minimum[1]] == 'Poff':\n",
    "                    samples.append(int(unordered_samples[minimum[1]]))\n",
    "                    symbols.append(unordered_symbols[minimum[1]])\n",
    "                    unordered_samples[minimum[1]] = np.inf\n",
    "            elif symbols[-1] == 'QRSon':\n",
    "                if unordered_symbols[minimum[0]] == 'QRSoff':\n",
    "                    samples.append(int(unordered_samples[minimum[0]]))\n",
    "                    symbols.append(unordered_symbols[minimum[0]])\n",
    "                    unordered_samples[minimum[0]] = np.inf\n",
    "                elif unordered_symbols[minimum[1]] == 'QRSoff':\n",
    "                    samples.append(int(unordered_samples[minimum[1]]))\n",
    "                    symbols.append(unordered_symbols[minimum[1]])\n",
    "                    unordered_samples[minimum[1]] = np.inf\n",
    "            elif symbols[-1] == 'Ton':\n",
    "                if unordered_symbols[minimum[0]] == 'Toff':\n",
    "                    samples.append(int(unordered_samples[minimum[0]]))\n",
    "                    symbols.append(unordered_symbols[minimum[0]])\n",
    "                    unordered_samples[minimum[0]] = np.inf\n",
    "                elif unordered_symbols[minimum[1]] == 'Toff':\n",
    "                    samples.append(int(unordered_samples[minimum[1]]))\n",
    "                    symbols.append(unordered_symbols[minimum[1]])\n",
    "                    unordered_samples[minimum[1]] = np.inf\n",
    "            else:\n",
    "                raise ValueError(\"Should not happen at all\")\n",
    "        else:\n",
    "            raise ValueError(\"Definitely should not happen. Check file {}\".format(k))\n",
    "    samples = np.array(samples)\n",
    "    symbols = np.array(symbols)\n",
    "    \n",
    "    # Extract segments\n",
    "    P = []\n",
    "    QRS = []\n",
    "    T = []\n",
    "    TP = []\n",
    "    PQ = []\n",
    "    ST = []\n",
    "\n",
    "    for i in range(samples.size-1):\n",
    "        if samples[i] == samples[i+1]:\n",
    "            continue\n",
    "        if symbols[i] == 'Pon':\n",
    "            if symbols[i+1] == 'Poff':\n",
    "                P.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            else:\n",
    "                print(\"Check file {}. P onset not followed by offset\".format(k))\n",
    "        elif symbols[i] == 'QRSon':\n",
    "            if symbols[i+1] == 'QRSoff':\n",
    "                QRS.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            else:\n",
    "                print(\"Check file {}. QRS onset not followed by offset\".format(k))\n",
    "        elif symbols[i] == 'Ton':\n",
    "            if symbols[i+1] == 'Toff':\n",
    "                T.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            else:\n",
    "                print(\"Check file {}. T onset not followed by offset\".format(k))\n",
    "        elif symbols[i] == 'Poff':\n",
    "            if symbols[i+1] == 'Pon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'QRSon':\n",
    "                PQ.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'Ton':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] in ['Poff','QRSoff','Toff']:\n",
    "                print(\"Check file {}. P offset not followed by onset\".format(k))\n",
    "        elif symbols[i] == 'QRSoff':\n",
    "            if symbols[i+1] == 'Pon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'QRSon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'Ton':\n",
    "                ST.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] in ['Poff','QRSoff','Toff']:\n",
    "                print(\"Check file {}. P offset not followed by onset\".format(k))\n",
    "        elif symbols[i] == 'Toff':\n",
    "            if symbols[i+1] == 'Pon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'QRSon':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] == 'Ton':\n",
    "                TP.append((k,group[k],dataset[k][samples[i]:samples[i+1]].values))\n",
    "            elif symbols[i+1] in ['Poff','QRSoff','Toff']:\n",
    "                print(\"Check file {}. P offset not followed by onset\".format(k))\n",
    "        else:\n",
    "            raise ValueError(\"This should definitely not happen\")\n",
    "            \n",
    "    # Filter out too long TP segments (causing this to break)\n",
    "    TP = [TP[i] for i in range(len(TP)) if TP[i][2].size < 250]\n",
    "\n",
    "    # Filter out too similar segments\n",
    "    corr = getcorr(P)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    P = [P[i] for i in range(len(P)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(QRS)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    QRS = [QRS[i] for i in range(len(QRS)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(T)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    T = [T[i] for i in range(len(T)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(TP)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    TP = [TP[i] for i in range(len(TP)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(PQ)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    PQ = [PQ[i] for i in range(len(PQ)) if i in nodesclique]\n",
    "\n",
    "    corr = getcorr(ST)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    ST = [ST[i] for i in range(len(ST)) if i in nodesclique]\n",
    "    \n",
    "    # Store segments\n",
    "    for i in range(len(P)):\n",
    "        PsignalQTDB[k + '_' + str(i)] = P[i][2]\n",
    "        PgroupQTDB[k + '_' + str(i)] = (P[i][0],P[i][1])\n",
    "    for i in range(len(QRS)):\n",
    "        QRSsignalQTDB[k + '_' + str(i)] = QRS[i][2]\n",
    "        QRSgroupQTDB[k + '_' + str(i)] = (QRS[i][0],QRS[i][1])\n",
    "    for i in range(len(T)):\n",
    "        TsignalQTDB[k + '_' + str(i)] = T[i][2]\n",
    "        TgroupQTDB[k + '_' + str(i)] = (T[i][0],T[i][1])\n",
    "    for i in range(len(TP)):\n",
    "        TPsignalQTDB[k + '_' + str(i)] = TP[i][2]\n",
    "        TPgroupQTDB[k + '_' + str(i)] = (TP[i][0],TP[i][1])\n",
    "    for i in range(len(PQ)):\n",
    "        PQsignalQTDB[k + '_' + str(i)] = PQ[i][2]\n",
    "        PQgroupQTDB[k + '_' + str(i)] = (PQ[i][0],PQ[i][1])\n",
    "    for i in range(len(ST)):\n",
    "        STsignalQTDB[k + '_' + str(i)] = ST[i][2]\n",
    "        STgroupQTDB[k + '_' + str(i)] = (ST[i][0],ST[i][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Psignal = {}\n",
    "Pgroup = {}\n",
    "PQsignal = {}\n",
    "PQgroup = {}\n",
    "QRSsignal = {}\n",
    "QRSgroup = {}\n",
    "STsignal = {}\n",
    "STgroup = {}\n",
    "Tsignal = {}\n",
    "Tgroup = {}\n",
    "TPsignal = {}\n",
    "TPgroup = {}\n",
    "\n",
    "Psignal.update(PsignalQTDB)\n",
    "Pgroup.update(PgroupQTDB)\n",
    "PQsignal.update(PQsignalQTDB)\n",
    "PQgroup.update(PQgroupQTDB)\n",
    "QRSsignal.update(QRSsignalQTDB)\n",
    "QRSgroup.update(QRSgroupQTDB)\n",
    "STsignal.update(STsignalQTDB)\n",
    "STgroup.update(STgroupQTDB)\n",
    "Tsignal.update(TsignalQTDB)\n",
    "Tgroup.update(TgroupQTDB)\n",
    "TPsignal.update(TPsignalQTDB)\n",
    "TPgroup.update(TPgroupQTDB)\n",
    "\n",
    "Psignal.update(PsignalLUDB)\n",
    "Pgroup.update(PgroupLUDB)\n",
    "PQsignal.update(PQsignalLUDB)\n",
    "PQgroup.update(PQgroupLUDB)\n",
    "QRSsignal.update(QRSsignalLUDB)\n",
    "QRSgroup.update(QRSgroupLUDB)\n",
    "STsignal.update(STsignalLUDB)\n",
    "STgroup.update(STgroupLUDB)\n",
    "Tsignal.update(TsignalLUDB)\n",
    "Tgroup.update(TgroupLUDB)\n",
    "TPsignal.update(TPsignalLUDB)\n",
    "TPgroup.update(TPgroupLUDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18539\n",
      "11865\n",
      "13128\n",
      "16299\n",
      "15412\n",
      "21275\n"
     ]
    }
   ],
   "source": [
    "print(len(Pgroup))\n",
    "print(len(PQgroup))\n",
    "print(len(QRSgroup))\n",
    "print(len(STgroup))\n",
    "print(len(Tgroup))\n",
    "print(len(TPgroup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete too short or too long signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal lengths\n",
    "Plength = {k: len(Psignal[k]) for k in Psignal.keys() if not isinstance(Psignal[k],float)}\n",
    "PQlength = {k: len(PQsignal[k]) for k in PQsignal.keys() if not isinstance(PQsignal[k],float)}\n",
    "QRSlength = {k: len(QRSsignal[k]) for k in QRSsignal.keys() if not isinstance(QRSsignal[k],float)}\n",
    "STlength = {k: len(STsignal[k]) for k in STsignal.keys() if not isinstance(STsignal[k],float)}\n",
    "Tlength = {k: len(Tsignal[k]) for k in Tsignal.keys() if not isinstance(Tsignal[k],float)}\n",
    "TPlength = {k: len(TPsignal[k]) for k in TPsignal.keys() if not isinstance(TPsignal[k],float)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter signals by length\n",
    "for k in list(Psignal.keys()):\n",
    "    if isinstance(Psignal[k],float):\n",
    "        Psignal.pop(k)\n",
    "        Pgroup.pop(k)\n",
    "    elif not ((len(Psignal[k]) > 1) and (len(Psignal[k]) < 45)):\n",
    "        Psignal.pop(k)\n",
    "        Pgroup.pop(k)\n",
    "for k in list(PQsignal.keys()):\n",
    "    if isinstance(PQsignal[k],float):\n",
    "        PQsignal.pop(k)\n",
    "        PQgroup.pop(k)\n",
    "    elif not ((len(PQsignal[k]) > 1) and (len(PQsignal[k]) < 35)):\n",
    "        PQsignal.pop(k)\n",
    "        PQgroup.pop(k)\n",
    "for k in list(QRSsignal.keys()):\n",
    "    if isinstance(QRSsignal[k],float):\n",
    "        QRSsignal.pop(k)\n",
    "        QRSgroup.pop(k)\n",
    "    elif not ((len(QRSsignal[k]) > 1) and (len(QRSsignal[k]) < 50)):\n",
    "        QRSsignal.pop(k)\n",
    "        QRSgroup.pop(k)\n",
    "for k in list(STsignal.keys()):\n",
    "    if isinstance(STsignal[k],float):\n",
    "        STsignal.pop(k)\n",
    "        STgroup.pop(k)\n",
    "    elif not ((len(STsignal[k]) > 1) and (len(STsignal[k]) < 65)):\n",
    "        STsignal.pop(k)\n",
    "        STgroup.pop(k)\n",
    "for k in list(Tsignal.keys()):\n",
    "    if isinstance(Tsignal[k],float):\n",
    "        Tsignal.pop(k)\n",
    "        Tgroup.pop(k)\n",
    "    elif not ((len(Tsignal[k]) > 1) and (len(Tsignal[k]) < 100)):\n",
    "        Tsignal.pop(k)\n",
    "        Tgroup.pop(k)\n",
    "for k in list(TPsignal.keys()):\n",
    "    if isinstance(TPsignal[k],float):\n",
    "        TPsignal.pop(k)\n",
    "        TPgroup.pop(k)\n",
    "    elif not ((len(TPsignal[k]) > 1) and (len(TPsignal[k]) < 250)):\n",
    "        TPsignal.pop(k)\n",
    "        TPgroup.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18429\n",
      "11628\n",
      "12971\n",
      "16200\n",
      "15303\n",
      "21236\n"
     ]
    }
   ],
   "source": [
    "print(len(Pgroup))\n",
    "print(len(PQgroup))\n",
    "print(len(QRSgroup))\n",
    "print(len(STgroup))\n",
    "print(len(Tgroup))\n",
    "print(len(TPgroup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.pickledump(Psignal,os.path.join('.','pickle','Psignal.pkl'))\n",
    "utils.pickledump(Pgroup,os.path.join('.','pickle','Pgroup.pkl'))\n",
    "utils.pickledump(PQsignal,os.path.join('.','pickle','PQsignal.pkl'))\n",
    "utils.pickledump(PQgroup,os.path.join('.','pickle','PQgroup.pkl'))\n",
    "utils.pickledump(QRSsignal,os.path.join('.','pickle','QRSsignal.pkl'))\n",
    "utils.pickledump(QRSgroup,os.path.join('.','pickle','QRSgroup.pkl'))\n",
    "utils.pickledump(STsignal,os.path.join('.','pickle','STsignal.pkl'))\n",
    "utils.pickledump(STgroup,os.path.join('.','pickle','STgroup.pkl'))\n",
    "utils.pickledump(Tsignal,os.path.join('.','pickle','Tsignal.pkl'))\n",
    "utils.pickledump(Tgroup,os.path.join('.','pickle','Tgroup.pkl'))\n",
    "utils.pickledump(TPsignal,os.path.join('.','pickle','TPsignal.pkl'))\n",
    "utils.pickledump(TPgroup,os.path.join('.','pickle','TPgroup.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning3",
   "language": "python",
   "name": "deeplearning3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
