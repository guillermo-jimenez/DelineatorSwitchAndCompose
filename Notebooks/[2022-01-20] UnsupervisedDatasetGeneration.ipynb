{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import skimage\n",
    "import skimage.segmentation\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import math\n",
    "import shutil\n",
    "import pathlib\n",
    "import glob\n",
    "import shutil\n",
    "import uuid\n",
    "import random\n",
    "import platform\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import wfdb\n",
    "import fleetfmt\n",
    "import json\n",
    "import tqdm\n",
    "import dill\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import src.data\n",
    "import src.reader\n",
    "\n",
    "import sak\n",
    "import sak.signal.wavelet\n",
    "import sak.data\n",
    "import sak.data.augmentation\n",
    "import sak.visualization\n",
    "import sak.visualization.signal\n",
    "import sak.torch\n",
    "import sak.torch.nn\n",
    "import sak.torch.nn as nn\n",
    "import sak.torch.train\n",
    "import sak.torch.data\n",
    "import sak.data.preprocessing\n",
    "import sak.torch.models\n",
    "import sak.torch.models.lego\n",
    "import sak.torch.models.variational\n",
    "import sak.torch.models.classification\n",
    "\n",
    "from sak.signal import StandardHeader\n",
    "\n",
    "def smooth(x: np.ndarray, window_size: int, conv_mode: str = 'same'):\n",
    "    x = np.pad(np.copy(x),(window_size,window_size),'edge')\n",
    "    window = np.hamming(window_size)/(window_size//2)\n",
    "    x = np.convolve(x, window, mode=conv_mode)\n",
    "    x = x[window_size:-window_size]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Float32, 250Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing numpy content to a new Fleet file.\n",
      "MUSE dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10646/10646 [01:55<00:00, 92.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brugada dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [01:39<00:00, 14.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallot dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1941/1941 [00:19<00:00, 98.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HCM dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:01<00:00, 129.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenge dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41704/41704 [06:03<00:00, 114.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongQT dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 807/807 [00:07<00:00, 106.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THEW dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1700/1700 [2:48:23<00:00,  5.94s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepFake dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 36246/150000 [03:27<09:56, 190.64it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 70%|██████▉   | 14354/20618 [26:50<10:12, 10.22it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ishneholterlib\n",
    "import struct\n",
    "\n",
    "window = 2048\n",
    "max_size = window*10\n",
    "window_step = max_size-window//4\n",
    "target_fs = 250\n",
    "target_dtype = \"float32\"\n",
    "\n",
    "print(\"Writing numpy content to a new Fleet file.\")\n",
    "with pathlib.Path(f\"/media/guille/DADES/DADES/ECG/unsupervised_{target_dtype}_{target_fs}hz.fleet\").open('wb') as fhandle, fleetfmt.FileWriter(fhandle) as writer:\n",
    "    print(\"MUSE dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/ECGData/*.csv\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            tmp = pd.read_csv(file)\n",
    "            tmp = tmp.round(3)\n",
    "            tmp.columns = map(lambda x: str(x).upper(), tmp.columns)\n",
    "            fs = 500\n",
    "            hea = list(tmp.columns)\n",
    "            tmp = sak.signal.interpolate.interp1d(tmp.values,target_fs*tmp.shape[0]//fs,axis=0).T\n",
    "            for k,value in zip(hea,tmp): \n",
    "                if value.size < window: continue\n",
    "                writer.append(f\"MUSE/{fname[5:]}/{k}###0\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Brugada dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/Brugada/Databases/HUVR/*.ecg\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg = ishneholterlib.Holter(file)\n",
    "            ecg.load_data()\n",
    "            fs = ecg.sr\n",
    "            for lead in ecg.lead:\n",
    "                k = lead.spec_str().upper()\n",
    "                data = lead.data.copy()\n",
    "                data = sak.signal.interpolate.interp1d(data,target_fs*data.size//fs)\n",
    "                if data.size > max_size:\n",
    "                    data = skimage.util.view_as_windows(data,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    data = data[None,]\n",
    "                for i,value in enumerate(data):\n",
    "                    if i == 0:\n",
    "                        continue\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"Brugada/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Fallot dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/Fallot/ECGs/*.csv\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            tmp = pd.read_csv(file)\n",
    "            tmp = tmp.round(3)\n",
    "            tmp.columns = map(lambda x: str(x).upper(), tmp.columns)\n",
    "            hea = map(lambda x: str(x).upper(), tmp.columns)\n",
    "            fs = 1/np.unique(np.round(np.diff(tmp[\"TIME IN SEC.\"].values),5))\n",
    "            assert fs.size == 1, \"check fs!!!\"\n",
    "            fs = int(fs[0])\n",
    "            hea = list(tmp.columns)\n",
    "            tmp = sak.signal.interpolate.interp1d(tmp.values,target_fs*tmp.shape[0]//fs,axis=0).T\n",
    "            for k,value in zip(hea,tmp): \n",
    "                if k == \"TIME IN SEC.\": continue\n",
    "                if value.size < window: continue\n",
    "                writer.append(f\"Fallot/{fname}/{k}###0\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"HCM dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/HCMData/CSV/*.csv\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            tmp = pd.read_csv(file,header=None)[:-500]/1000\n",
    "            tmp.columns = map(lambda x: str(x).upper(), StandardHeader)\n",
    "            fs = 500\n",
    "            tmp = sak.signal.interpolate.interp1d(tmp.values,target_fs*tmp.shape[0]//fs,axis=0).T\n",
    "            for k,value in zip(StandardHeader,tmp): \n",
    "                if value.size < window: continue\n",
    "                writer.append(f\"HCM/{fname}/{k}###0\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Challenge dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/Challenge/*.mat\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            sig,hea = wfdb.rdsamp(os.path.join(root,fname),return_res=16)\n",
    "            hea[\"sig_name\"] = list(map(lambda x: str(x).upper(), hea[\"sig_name\"]))\n",
    "            fs = hea[\"fs\"]\n",
    "            sig = sak.signal.interpolate.interp1d(sig,target_fs*sig.shape[0]//fs,axis=0).T\n",
    "            for k,lead in zip(hea[\"sig_name\"],sig):\n",
    "                if lead.size > max_size:\n",
    "                    data = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    data = lead[None,]\n",
    "                for i,value in enumerate(data):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"Challenge/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"LongQT dataset\")\n",
    "    time.sleep(0.5)\n",
    "    import xml.etree.ElementTree as ET\n",
    "    ET.register_namespace(\"\",\"http://www3.medical.philips.com\")\n",
    "    all_files = (glob.glob(\"/home/guille/DADES/DADES/ECG/LONG_QT/LongQT_*/*.XML\") + \n",
    "                 glob.glob(\"/home/guille/DADES/DADES/ECG/LONG_QT/LongQT_*/*/*.XML\"))\n",
    "    head = \"{http://www3.medical.philips.com}\"\n",
    "    namespace = {\"xmlns:ns0\": \"http://www3.medical.philips.com\",\n",
    "                 \"xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\",\n",
    "                 \"xsi:schemaLocation\": \"http://www3.medical.philips.com PhilipsECG.xsd\"}\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            tree = ET.parse(file)\n",
    "            if len(tree.findall(\"FullDisclosure\")) != 0:\n",
    "                continue\n",
    "            fs = float(tree.findall(\"StripData/SampleRate\")[0].text)\n",
    "            ecg = {}\n",
    "            for lead in tree.findall(\"StripData/WaveformData\"):\n",
    "                ecg[lead.get(\"lead\").upper()] = np.array(lead.text.strip().split(\",\"),dtype=int)\n",
    "            ecg = np.array([ecg[k] for k in StandardHeader])\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg,round(ecg.shape[1]*250/fs))\n",
    "            for k,lead in zip(StandardHeader,ecg):\n",
    "                if lead.size > max_size:\n",
    "                    data = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    data = lead[None,]\n",
    "                for i,value in enumerate(data):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"LongQT/{fname}/{k}###{i}\", (value/1000).astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"THEW dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/THEWProject/*/*.ecg\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            rt,stu_id = os.path.split(root)\n",
    "            ecg = ishneholterlib.Holter(file,check_valid=False)\n",
    "            ecg.load_data()\n",
    "            fs = ecg.sr\n",
    "            for lead in ecg.lead:\n",
    "                k = lead.spec_str().upper()\n",
    "                data = lead.data.copy()\n",
    "                data = sak.signal.interpolate.interp1d(data,target_fs*data.size//fs)\n",
    "                if data.size > max_size:\n",
    "                    data = skimage.util.view_as_windows(data,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    data = data[None,]\n",
    "                for i,value in enumerate(data):\n",
    "                    if value.size < window: continue\n",
    "                    if np.all(np.abs(np.diff(value)) < 1e-6): continue\n",
    "                    writer.append(f\"THEW/{stu_id}_{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"DeepFake dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/DeepFake/*.asc\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg = pd.read_csv(file,header=None,sep=\" \")\n",
    "            fs = 500\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs)\n",
    "            for k,value in zip(StandardHeader,ecg):\n",
    "                k = k.upper()\n",
    "                if value.size < window: continue\n",
    "                writer.append(f\"DeepFake/{fname}/{k}###0\", (value/1000).astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"SoO dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/Delineator/SoO/RETAG/*.txt\")\n",
    "    db = pd.read_csv('/home/guille/DADES/DADES/ECG/Delineator/SoO/DATABASE_MANUAL.csv')\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg = pd.read_csv(file,header=0,sep=\",\",index_col=0)\n",
    "            hea = list(map(lambda x: str(x).upper(), ecg.columns))\n",
    "            filt_id = (db[\"ID\"] == int(fname.split(\"-\")[0]))\n",
    "            fs = db[filt_id][\"Sampling_Freq\"].values[0]\n",
    "            ecg = np.round(sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs))/(2**15)\n",
    "            for k,lead in zip(hea,ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"SoO/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"EDB dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/PhysioNet/EDB/*.dat\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg,hea = wfdb.rdsamp(file[:-4])\n",
    "            fs = hea[\"fs\"]\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs)\n",
    "            for k,lead in zip(hea[\"sig_name\"],ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"EDB/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"SVDB dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/PhysioNet/SVDB/*.dat\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg,hea = wfdb.rdsamp(file[:-4])\n",
    "            fs = hea[\"fs\"]\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs)\n",
    "            for k,lead in zip(hea[\"sig_name\"],ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"SVDB/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"PredictAF dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/PredictAF/*/*/*.NHS\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            rt,pat_id = os.path.split(root)\n",
    "            rt,stu_id = os.path.split(rt)\n",
    "            ecg,fs = src.reader.readNHS(file)\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg.T,int(target_fs*ecg.shape[0]//fs)).T\n",
    "            for k,lead in zip(StandardHeader,ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if i < 5:\n",
    "                        continue\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"PredictAF/{stu_id}_{pat_id}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Zhejiang dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/RubenDoste/ZhejiangDatabase/PVCVTRawECGData/*.csv\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            if fname in [\"onsets\",\"offsets\"]: continue\n",
    "            ecg = pd.read_csv(file,header=0)\n",
    "            hea = list(ecg.columns)\n",
    "            fs = 2000\n",
    "            ecg = np.round(sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs))/(2**13)\n",
    "            for k,lead in zip(hea,ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"Zhejiang/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"HUVR_CARTO dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/EGMDelineator/Databases/CARTOEXPORT/*/*ECG_Export.txt\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            rt,pat_id = os.path.split(root)\n",
    "            fs = 2000\n",
    "            signal,header = src.reader.Read_CARTO3_ECG(file)\n",
    "            channels = np.array(sak.map_upper([c.split(\"(\")[0].replace(\" \",\"\") for c in header[\"Channels\"]]))\n",
    "            idx_header = np.array([np.where(channels == c)[0][0] for c in StandardHeader])\n",
    "            signal = np.round(sak.signal.interpolate.interp1d(signal[idx_header],target_fs*signal.shape[1]//fs))/(2**10)\n",
    "            for k,lead in zip(StandardHeader,signal):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"HUVR_CARTO/{pat_id}_{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "time.sleep(0.5)\n",
    "print(\"Done.\")\n",
    "time.sleep(0.5)\n",
    "!du -sh /media/guille/DADES/DADES/ECG/unsupervised*.fleet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "116G\t/media/guille/DADES/DADES/ECG/unsupervised_float16_250hz.fleet\r\n",
      "195G\t/media/guille/DADES/DADES/ECG/unsupervised_float32_250hz.fleet\r\n",
      "390G\t/media/guille/DADES/DADES/ECG/unsupervised_float32_500hz.fleet\r\n"
     ]
    }
   ],
   "source": [
    "print(\"Done!\")\n",
    "!du -sh /media/guille/DADES/DADES/ECG/unsupervised*.fleet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Float32, 250Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing numpy content to a new Fleet file.\n",
      "MUSE dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10646/10646 [01:53<00:00, 93.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brugada dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:44<00:00, 23.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallot dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1941/1941 [00:19<00:00, 101.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HCM dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:01<00:00, 101.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenge dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 22547/41704 [03:15<02:28, 129.30it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 23%|██▎       | 4843/20618 [10:16<33:25,  7.87it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ishneholterlib\n",
    "import struct\n",
    "\n",
    "window = 2048\n",
    "max_size = window*10\n",
    "window_step = max_size-window//4\n",
    "target_fs = 500\n",
    "target_dtype = \"float32\"\n",
    "\n",
    "print(\"Writing numpy content to a new Fleet file.\")\n",
    "with pathlib.Path(f\"/media/guille/DADES/DADES/ECG/unsupervised_{target_dtype}_{target_fs}hz.fleet\").open('wb') as fhandle, fleetfmt.FileWriter(fhandle) as writer:\n",
    "    print(\"MUSE dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/ECGData/*.csv\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            tmp = pd.read_csv(file)\n",
    "            tmp = tmp.round(3)\n",
    "            tmp.columns = map(lambda x: str(x).upper(), tmp.columns)\n",
    "            fs = 500\n",
    "            hea = list(tmp.columns)\n",
    "            tmp = sak.signal.interpolate.interp1d(tmp.values,target_fs*tmp.shape[0]//fs,axis=0).T\n",
    "            for k,value in zip(hea,tmp): \n",
    "                if value.size < window: continue\n",
    "                writer.append(f\"MUSE/{fname[5:]}/{k}###0\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Brugada dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/Brugada/Databases/HUVR/*.ecg\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg = ishneholterlib.Holter(file)\n",
    "            ecg.load_data()\n",
    "            fs = ecg.sr\n",
    "            for lead in ecg.lead:\n",
    "                k = lead.spec_str().upper()\n",
    "                data = lead.data.copy()\n",
    "                data = sak.signal.interpolate.interp1d(data,target_fs*data.size//fs)\n",
    "                if data.size > max_size:\n",
    "                    data = skimage.util.view_as_windows(data,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    data = data[None,]\n",
    "                for i,value in enumerate(data):\n",
    "                    if i == 0:\n",
    "                        continue\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"Brugada/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Fallot dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/Fallot/ECGs/*.csv\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            tmp = pd.read_csv(file)\n",
    "            tmp = tmp.round(3)\n",
    "            tmp.columns = map(lambda x: str(x).upper(), tmp.columns)\n",
    "            hea = map(lambda x: str(x).upper(), tmp.columns)\n",
    "            fs = 1/np.unique(np.round(np.diff(tmp[\"TIME IN SEC.\"].values),5))\n",
    "            assert fs.size == 1, \"check fs!!!\"\n",
    "            fs = int(fs[0])\n",
    "            hea = list(tmp.columns)\n",
    "            tmp = sak.signal.interpolate.interp1d(tmp.values,target_fs*tmp.shape[0]//fs,axis=0).T\n",
    "            for k,value in zip(hea,tmp): \n",
    "                if k == \"TIME IN SEC.\": continue\n",
    "                if value.size < window: continue\n",
    "                writer.append(f\"Fallot/{fname}/{k}###0\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"HCM dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/HCMData/CSV/*.csv\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            tmp = pd.read_csv(file,header=None)[:-500]/1000\n",
    "            tmp.columns = map(lambda x: str(x).upper(), StandardHeader)\n",
    "            fs = 500\n",
    "            tmp = sak.signal.interpolate.interp1d(tmp.values,target_fs*tmp.shape[0]//fs,axis=0).T\n",
    "            for k,value in zip(StandardHeader,tmp): \n",
    "                if value.size < window: continue\n",
    "                writer.append(f\"HCM/{fname}/{k}###0\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Challenge dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/Challenge/*.mat\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            sig,hea = wfdb.rdsamp(os.path.join(root,fname),return_res=16)\n",
    "            hea[\"sig_name\"] = list(map(lambda x: str(x).upper(), hea[\"sig_name\"]))\n",
    "            fs = hea[\"fs\"]\n",
    "            sig = sak.signal.interpolate.interp1d(sig,target_fs*sig.shape[0]//fs,axis=0).T\n",
    "            for k,lead in zip(hea[\"sig_name\"],sig):\n",
    "                if lead.size > max_size:\n",
    "                    data = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    data = lead[None,]\n",
    "                for i,value in enumerate(data):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"Challenge/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"LongQT dataset\")\n",
    "    time.sleep(0.5)\n",
    "    import xml.etree.ElementTree as ET\n",
    "    ET.register_namespace(\"\",\"http://www3.medical.philips.com\")\n",
    "    all_files = (glob.glob(\"/home/guille/DADES/DADES/ECG/LONG_QT/LongQT_*/*.XML\") + \n",
    "                 glob.glob(\"/home/guille/DADES/DADES/ECG/LONG_QT/LongQT_*/*/*.XML\"))\n",
    "    head = \"{http://www3.medical.philips.com}\"\n",
    "    namespace = {\"xmlns:ns0\": \"http://www3.medical.philips.com\",\n",
    "                 \"xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\",\n",
    "                 \"xsi:schemaLocation\": \"http://www3.medical.philips.com PhilipsECG.xsd\"}\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            tree = ET.parse(file)\n",
    "            if len(tree.findall(\"FullDisclosure\")) != 0:\n",
    "                continue\n",
    "            fs = float(tree.findall(\"StripData/SampleRate\")[0].text)\n",
    "            ecg = {}\n",
    "            for lead in tree.findall(\"StripData/WaveformData\"):\n",
    "                ecg[lead.get(\"lead\").upper()] = np.array(lead.text.strip().split(\",\"),dtype=int)\n",
    "            ecg = np.array([ecg[k] for k in StandardHeader])\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg,round(ecg.shape[1]*250/fs))\n",
    "            for k,lead in zip(StandardHeader,ecg):\n",
    "                if lead.size > max_size:\n",
    "                    data = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    data = lead[None,]\n",
    "                for i,value in enumerate(data):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"LongQT/{fname}/{k}###{i}\", (value/1000).astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"THEW dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/THEWProject/*/*.ecg\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            rt,stu_id = os.path.split(root)\n",
    "            ecg = ishneholterlib.Holter(file,check_valid=False)\n",
    "            ecg.load_data()\n",
    "            fs = ecg.sr\n",
    "            for lead in ecg.lead:\n",
    "                k = lead.spec_str().upper()\n",
    "                data = lead.data.copy()\n",
    "                data = sak.signal.interpolate.interp1d(data,target_fs*data.size//fs)\n",
    "                if data.size > max_size:\n",
    "                    data = skimage.util.view_as_windows(data,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    data = data[None,]\n",
    "                for i,value in enumerate(data):\n",
    "                    if value.size < window: continue\n",
    "                    if np.all(np.abs(np.diff(value)) < 1e-6): continue\n",
    "                    writer.append(f\"THEW/{stu_id}_{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"DeepFake dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/DeepFake/*.asc\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg = pd.read_csv(file,header=None,sep=\" \")\n",
    "            fs = 500\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs)\n",
    "            for k,value in zip(StandardHeader,ecg):\n",
    "                k = k.upper()\n",
    "                if value.size < window: continue\n",
    "                writer.append(f\"DeepFake/{fname}/{k}###0\", (value/1000).astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"SoO dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/Delineator/SoO/RETAG/*.txt\")\n",
    "    db = pd.read_csv('/home/guille/DADES/DADES/ECG/Delineator/SoO/DATABASE_MANUAL.csv')\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg = pd.read_csv(file,header=0,sep=\",\",index_col=0)\n",
    "            hea = list(map(lambda x: str(x).upper(), ecg.columns))\n",
    "            filt_id = (db[\"ID\"] == int(fname.split(\"-\")[0]))\n",
    "            fs = db[filt_id][\"Sampling_Freq\"].values[0]\n",
    "            ecg = np.round(sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs))/(2**15)\n",
    "            for k,lead in zip(hea,ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"SoO/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"EDB dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/PhysioNet/EDB/*.dat\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg,hea = wfdb.rdsamp(file[:-4])\n",
    "            fs = hea[\"fs\"]\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs)\n",
    "            for k,lead in zip(hea[\"sig_name\"],ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"EDB/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"SVDB dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/PhysioNet/SVDB/*.dat\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg,hea = wfdb.rdsamp(file[:-4])\n",
    "            fs = hea[\"fs\"]\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs)\n",
    "            for k,lead in zip(hea[\"sig_name\"],ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"SVDB/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"PredictAF dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/PredictAF/*/*/*.NHS\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            rt,pat_id = os.path.split(root)\n",
    "            rt,stu_id = os.path.split(rt)\n",
    "            ecg,fs = src.reader.readNHS(file)\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg.T,int(target_fs*ecg.shape[0]//fs)).T\n",
    "            for k,lead in zip(StandardHeader,ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if i < 5:\n",
    "                        continue\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"PredictAF/{stu_id}_{pat_id}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Zhejiang dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/RubenDoste/ZhejiangDatabase/PVCVTRawECGData/*.csv\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            if fname in [\"onsets\",\"offsets\"]: continue\n",
    "            ecg = pd.read_csv(file,header=0)\n",
    "            hea = list(ecg.columns)\n",
    "            fs = 2000\n",
    "            ecg = np.round(sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs))/(2**13)\n",
    "            for k,lead in zip(hea,ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"Zhejiang/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"HUVR_CARTO dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/EGMDelineator/Databases/CARTOEXPORT/*/*ECG_Export.txt\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            rt,pat_id = os.path.split(root)\n",
    "            fs = 2000\n",
    "            signal,header = src.reader.Read_CARTO3_ECG(file)\n",
    "            channels = np.array(sak.map_upper([c.split(\"(\")[0].replace(\" \",\"\") for c in header[\"Channels\"]]))\n",
    "            idx_header = np.array([np.where(channels == c)[0][0] for c in StandardHeader])\n",
    "            signal = np.round(sak.signal.interpolate.interp1d(signal[idx_header],target_fs*signal.shape[1]//fs))/(2**10)\n",
    "            for k,lead in zip(StandardHeader,signal):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"HUVR_CARTO/{pat_id}_{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "time.sleep(0.5)\n",
    "print(\"Done.\")\n",
    "time.sleep(0.5)\n",
    "!du -sh /media/guille/DADES/DADES/ECG/unsupervised*.fleet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
