{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import skimage\n",
    "import skimage.segmentation\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import math\n",
    "import shutil\n",
    "import pathlib\n",
    "import glob\n",
    "import shutil\n",
    "import uuid\n",
    "import random\n",
    "import platform\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import wfdb\n",
    "import json\n",
    "import tqdm\n",
    "import dill\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats\n",
    "import cv2\n",
    "\n",
    "import src.data\n",
    "import src.metrics\n",
    "import sak\n",
    "import sak.signal.wavelet\n",
    "import sak.data\n",
    "import sak.data.augmentation\n",
    "import sak.data.preprocessing\n",
    "import sak.visualization\n",
    "import sak.visualization.signal\n",
    "import sak.torch\n",
    "import sak.torch.nn\n",
    "import sak.torch.nn as nn\n",
    "import sak.torch.nn\n",
    "import sak.torch.train\n",
    "import sak.torch.data\n",
    "import sak.torch.models\n",
    "import sak.torch.models.lego\n",
    "import sak.torch.models.variational\n",
    "import sak.torch.models.classification\n",
    "\n",
    "from sak.signal import StandardHeader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read all common files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/media/guille/DADES/DADES/Delineator/'\n",
    "\n",
    "with open('../list_files.txt') as f:\n",
    "    valid_models = f.read().splitlines()\n",
    "    \n",
    "folders = [os.path.join(basedir,'TrainedModels',model) for model in valid_models]\n",
    "all_databases = [\"qtdb_single\",\"qtdb_multi\",\"zhejiang\",\"ludb\"]\n",
    "all_waves = [\"P\",\"QRS\",\"T\"]\n",
    "all_12_leads = [\"zhejiang\", \"ludb\"]\n",
    "all_qtdb = [\"qtdb_single\",\"qtdb_multi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and homoginize results - extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "109it [00:03, 30.20it/s]\n"
     ]
    }
   ],
   "source": [
    "zhejiang = {}\n",
    "ludb = {}\n",
    "qtdb_single = {}\n",
    "qtdb_multi = {}\n",
    "qtdb_fold_single = {}\n",
    "qtdb_fold_multi = {}\n",
    "\n",
    "for counter,folder in tqdm.tqdm(enumerate(folders)):\n",
    "    if (\n",
    "        (not os.path.isfile(os.path.join(folder,'metrics_string.txt'))) or\n",
    "        (not os.path.isfile(os.path.join(folder,'zhejiang_metrics_string.txt'))) or\n",
    "        (not os.path.isfile(os.path.join(folder,'ludb_metrics_string.txt'))) or\n",
    "        (not os.path.isfile(os.path.join(folder,'metrics_fold_string.txt')))\n",
    "       ):\n",
    "        continue\n",
    "    \n",
    "    # Extract model name from folder path\n",
    "    root,model_name = os.path.split(folder)\n",
    "    \n",
    "    # Init metrics\n",
    "    zhejiang[model_name] = {'P': {}, 'QRS': {}, 'T': {}}\n",
    "    ludb[model_name] = {'P': {}, 'QRS': {}, 'T': {}}\n",
    "    qtdb_single[model_name] = {'P': {}, 'QRS': {}, 'T': {}}\n",
    "    qtdb_multi[model_name] = {'P': {}, 'QRS': {}, 'T': {}}\n",
    "    qtdb_fold_single[model_name] = {'P': {}, 'QRS': {}, 'T': {}}\n",
    "    qtdb_fold_multi[model_name] = {'P': {}, 'QRS': {}, 'T': {}}\n",
    "    \n",
    "    # Open metrics files\n",
    "    with open(os.path.join(folder,'metrics_string.txt')) as f:\n",
    "        metrics = f.read().splitlines()\n",
    "    with open(os.path.join(folder,'metrics_fold_string.txt')) as f:\n",
    "        metrics_fold = f.read().splitlines()\n",
    "    with open(os.path.join(folder,'zhejiang_metrics_string.txt')) as f:\n",
    "        metrics_zhejiang = f.read().splitlines()\n",
    "    with open(os.path.join(folder,'ludb_metrics_string.txt')) as f:\n",
    "        metrics_ludb = f.read().splitlines()\n",
    "    \n",
    "    # Agglomerate Zhejiang's and LUDB's metrics\n",
    "    for i,db in enumerate(all_12_leads):\n",
    "        dic = eval(db)\n",
    "        mtr = eval(f\"metrics_{db}\")\n",
    "        \n",
    "        for j,wave in enumerate(all_waves):\n",
    "            dic[model_name][wave][\"Precision\"] = float(mtr[4+j*10].split(':')[1][:-1])\n",
    "            dic[model_name][wave][\"Recall\"]    = float(mtr[5+j*10].split(':')[1][:-1])\n",
    "            dic[model_name][wave][\"F1 Score\"]  = float(mtr[6+j*10].split(':')[1][:-1])\n",
    "\n",
    "            onmstd = mtr[8+j*10].split(':')[1][:-2]\n",
    "            offmstd = mtr[9+j*10].split(':')[1][:-2]\n",
    "            dic[model_name][wave][\"Onset Error (Mean)\"] = float(onmstd.split('±')[0])\n",
    "            dic[model_name][wave][\"Onset Error (STD)\"] = float(onmstd.split('±')[1])\n",
    "            dic[model_name][wave][\"Offset Error (Mean)\"] = float(offmstd.split('±')[0])\n",
    "            dic[model_name][wave][\"Offset Error (STD)\"] = float(offmstd.split('±')[1])\n",
    "    \n",
    "    # Extract QTDB single and multi\n",
    "    for i,type in enumerate(all_qtdb):\n",
    "        dic = eval(type)\n",
    "        \n",
    "        for j,wave in enumerate(all_waves):\n",
    "            dic[model_name][wave][\"Precision\"] = float(metrics[(34*i)+(7+j*10)].split(':')[1][:-1])\n",
    "            dic[model_name][wave][\"Recall\"]    = float(metrics[(34*i)+(8+j*10)].split(':')[1][:-1])\n",
    "            dic[model_name][wave][\"F1 Score\"]  = float(metrics[(34*i)+(9+j*10)].split(':')[1][:-1])\n",
    "\n",
    "            onmstd = metrics[(34*i)+(11+j*10)].split(':')[1][:-2]\n",
    "            offmstd = metrics[(34*i)+(12+j*10)].split(':')[1][:-2]\n",
    "            dic[model_name][wave][\"Onset Error (Mean)\"] = float(onmstd.split('±')[0])\n",
    "            dic[model_name][wave][\"Onset Error (STD)\"] = float(onmstd.split('±')[1])\n",
    "            dic[model_name][wave][\"Offset Error (Mean)\"] = float(offmstd.split('±')[0])\n",
    "            dic[model_name][wave][\"Offset Error (STD)\"] = float(offmstd.split('±')[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results - best F1 score overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BEST ', 'UNet7LevelsConvDiceOnly_20201203124825', 99.38250000000001)\n",
      "('WORST', 'UNet7LevelsDiceOnly_20201202071040', 95.16250000000001)\n",
      "\n",
      "\n",
      "########## P wave ##########\n",
      "('BEST ', 'WNet5LevelsSelfAttentionDiceOnly_20201130125349', 98.99000000000001)\n",
      "('WORST', 'UNet7LevelsDiceOnly_20201202071040', 85.98750000000001)\n",
      "\n",
      "\n",
      "########## QRS wave ##########\n",
      "('BEST ', 'UNet7Levels_synth_20201208095004', 99.8475)\n",
      "('WORST', 'WNet5LevelsSelfAttention_synth_20201206012731', 99.35749999999999)\n",
      "\n",
      "\n",
      "########## T wave ##########\n",
      "('BEST ', 'UNet7LevelsDiceOnly_20201202071040', 99.69)\n",
      "('WORST', 'UNet5LevelsConvDiceOnly_real_20201204204042', 96.95750000000001)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "P_F1   = {}\n",
    "QRS_F1 = {}\n",
    "T_F1   = {}\n",
    "\n",
    "for wave in [\"P\", \"QRS\", \"T\"]:\n",
    "    output = eval(f\"{wave}_F1\")\n",
    "    \n",
    "    for db in [\"qtdb_single\",\"qtdb_multi\",\"zhejiang\",\"ludb\"]:\n",
    "        database = eval(db)\n",
    "        if db not in output:\n",
    "            output[db] = {}\n",
    "\n",
    "        for k in database:\n",
    "            output[db][k] = database[k][wave][\"F1 Score\"]\n",
    "            \n",
    "# Convert to DataFrame\n",
    "P_F1   = pd.DataFrame(P_F1)\n",
    "QRS_F1 = pd.DataFrame(QRS_F1)\n",
    "T_F1   = pd.DataFrame(T_F1)\n",
    "\n",
    "# Mean results across all databases and waves\n",
    "F1_overall = (P_F1.mean(axis=1)+QRS_F1.mean(axis=1)+T_F1.mean(axis=1))/3\n",
    "\n",
    "# Display results\n",
    "print((\"BEST \", F1_overall.idxmax(), F1_overall.max()))\n",
    "print((\"WORST\", F1_overall.idxmin(), F1_overall.min()))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Per wave\n",
    "for wave in all_waves:\n",
    "    wv = eval(f\"{wave}_F1\")\n",
    "    print(f\"########## {wave} wave ##########\")\n",
    "    print((\"BEST \", wv.mean(axis=1).idxmax(), wv.mean(axis=1).max()))\n",
    "    print((\"WORST\", wv.mean(axis=1).idxmin(), wv.mean(axis=1).min()))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results - minimum STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BEST ', 'WNet5LevelsSelfAttentionDiceOnly_20201130125349', 18.232083333333332)\n",
      "('WORST', 'UNet5LevelsConvDiceOnly_20201202003451', 56.17374999999999)\n",
      "\n",
      "\n",
      "########## P wave ##########\n",
      "('BEST ', 'WNet5LevelsSelfAttentionConvDiceOnly_20201130125349', 12.90875)\n",
      "('WORST', 'UNet5LevelsConvDiceOnly_20201202003451', 41.68)\n",
      "\n",
      "\n",
      "########## QRS wave ##########\n",
      "('BEST ', 'WNet5LevelsSelfAttentionDiceOnly_20201130125349', 10.479999999999999)\n",
      "('WORST', 'UNet5LevelsConvDiceOnly_20201202003451', 38.32)\n",
      "\n",
      "\n",
      "########## T wave ##########\n",
      "('BEST ', 'UNet7LevelsConvDiceOnly_20201203124825', 30.746250000000003)\n",
      "('WORST', 'UNet5LevelsConvDiceOnly_20201202003451', 88.52124999999998)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "P_STD_on    = {}\n",
    "P_STD_off   = {}\n",
    "QRS_STD_on  = {}\n",
    "QRS_STD_off = {}\n",
    "T_STD_on    = {}\n",
    "T_STD_off   = {}\n",
    "\n",
    "for wave in [\"P\", \"QRS\", \"T\"]:\n",
    "    output_on  = eval(f\"{wave}_STD_on\")\n",
    "    output_off = eval(f\"{wave}_STD_off\")\n",
    "    \n",
    "    for db in [\"qtdb_single\",\"qtdb_multi\",\"zhejiang\",\"ludb\"]:\n",
    "        database = eval(db)\n",
    "        if db not in output_on:\n",
    "            output_on[db] = {}\n",
    "        if db not in output_off:\n",
    "            output_off[db] = {}\n",
    "\n",
    "        for k in database:\n",
    "            output_on[db][k] = database[k][wave][\"Onset Error (STD)\"]\n",
    "            output_off[db][k] = database[k][wave][\"Offset Error (STD)\"]\n",
    "\n",
    "# Convert to DataFrame\n",
    "P_STD_on    = pd.DataFrame(P_STD_on)\n",
    "P_STD_off   = pd.DataFrame(P_STD_off)\n",
    "QRS_STD_on  = pd.DataFrame(QRS_STD_on)\n",
    "QRS_STD_off = pd.DataFrame(QRS_STD_off)\n",
    "T_STD_on    = pd.DataFrame(T_STD_on)\n",
    "T_STD_off   = pd.DataFrame(T_STD_off)\n",
    "\n",
    "# Mean results across all databases and waves\n",
    "onoff_overall = (\n",
    "    ((P_STD_on+P_STD_off)/2).mean(axis=1)+\n",
    "    ((QRS_STD_on+QRS_STD_off)/2).mean(axis=1)+\n",
    "    ((T_STD_on+T_STD_off)/2).mean(axis=1))/3\n",
    "\n",
    "# Display results\n",
    "print((\"BEST \", onoff_overall.idxmin(), onoff_overall.min()))\n",
    "print((\"WORST\", onoff_overall.idxmax(), onoff_overall.max()))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Per wave\n",
    "for wave in all_waves:\n",
    "    wv_on = eval(f\"{wave}_STD_on\")\n",
    "    wv_off = eval(f\"{wave}_STD_off\")\n",
    "    print(f\"########## {wave} wave ##########\")\n",
    "    print((\"BEST \", ((wv_on+wv_off)/2).mean(axis=1).idxmin(), ((wv_on+wv_off)/2).mean(axis=1).min()))\n",
    "    print((\"WORST\", ((wv_on+wv_off)/2).mean(axis=1).idxmax(), ((wv_on+wv_off)/2).mean(axis=1).max()))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real vs synthetic vs both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "real  = [model for model in valid_models if (\"_real_\" in model)]\n",
    "synth = [model for model in valid_models if (\"_synth_\" in model)]\n",
    "both  = [model for model in valid_models if ((model not in real) and (model not in synth))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BOTH VS  REAL', 0.6252500000000002)\n",
      "('BOTH VS SYNTH', 0.2774583333333335)\n",
      "('SYNTH VS REAL', 0.34779166666666683)\n",
      "\n",
      "\n",
      "########## P wave ##########\n",
      "('BOTH VS  REAL', 1.0267500000000012)\n",
      "('BOTH VS SYNTH', 0.7138750000000009)\n",
      "('SYNTH VS REAL', 0.31287500000000035)\n",
      "\n",
      "\n",
      "########## QRS wave ##########\n",
      "('BOTH VS  REAL', 0.07800000000000082)\n",
      "('BOTH VS SYNTH', -0.0025000000000000356)\n",
      "('SYNTH VS REAL', 0.08050000000000086)\n",
      "\n",
      "\n",
      "########## T wave ##########\n",
      "('BOTH VS  REAL', 0.7709999999999985)\n",
      "('BOTH VS SYNTH', 0.12099999999999937)\n",
      "('SYNTH VS REAL', 0.6499999999999991)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric = \"F1 Score\"\n",
    "\n",
    "real_as_metric = {}\n",
    "synth_as_metric = {}\n",
    "both_as_metric = {}\n",
    "\n",
    "for db in all_databases:\n",
    "    database = eval(db)\n",
    "    \n",
    "    for k in database:\n",
    "        value = {wave: database[k][wave][metric] for wave in all_waves}\n",
    "        unikey = k.split('_')[0]\n",
    "        \n",
    "        if k in real:\n",
    "            value = real_as_metric.get(unikey,{})\n",
    "            value.update({f\"{wave}_{db}\": database[k][wave][metric] for wave in all_waves})\n",
    "            real_as_metric[unikey] = value\n",
    "        elif k in synth:\n",
    "            value = synth_as_metric.get(unikey,{})\n",
    "            value.update({f\"{wave}_{db}\": database[k][wave][metric] for wave in all_waves})\n",
    "            synth_as_metric[unikey] = value\n",
    "        elif k in both:\n",
    "            value = both_as_metric.get(unikey,{})\n",
    "            value.update({f\"{wave}_{db}\": database[k][wave][metric] for wave in all_waves})\n",
    "            both_as_metric[unikey] = value\n",
    "        else:\n",
    "            raise ValueError(\"This should not have happened\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "real_as_metric = pd.DataFrame(real_as_metric)\n",
    "synth_as_metric = pd.DataFrame(synth_as_metric)\n",
    "both_as_metric = pd.DataFrame(both_as_metric)\n",
    "\n",
    "# Drop faulty training\n",
    "if \"UNet7LevelsDiceOnly\" in real_as_metric:\n",
    "    real_as_metric = real_as_metric.T.drop(\"UNet7LevelsDiceOnly\").drop(\"UNet6LevelsDiceOnly\").T\n",
    "if \"UNet7LevelsDiceOnly\" in synth_as_metric:\n",
    "    synth_as_metric = synth_as_metric.T.drop(\"UNet7LevelsDiceOnly\").drop(\"UNet6LevelsDiceOnly\").T\n",
    "if \"UNet7LevelsDiceOnly\" in both_as_metric:\n",
    "    both_as_metric = both_as_metric.T.drop(\"UNet7LevelsDiceOnly\").drop(\"UNet6LevelsDiceOnly\").T\n",
    "\n",
    "# Display results\n",
    "print((\"BOTH VS  REAL\", (both_as_metric.T-real_as_metric.T).mean(axis=1).mean()))\n",
    "print((\"BOTH VS SYNTH\", (both_as_metric.T-synth_as_metric.T).mean(axis=1).mean()))\n",
    "print((\"SYNTH VS REAL\", (synth_as_metric.T-real_as_metric.T).mean(axis=1).mean()))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Per wave\n",
    "for wave in all_waves:\n",
    "    real_mtr = eval(f\"real_as_metric\").T[[f'{wave}_qtdb_single',f'{wave}_qtdb_multi',f'{wave}_ludb',f'{wave}_zhejiang',]].T\n",
    "    synth_mtr = eval(f\"synth_as_metric\").T[[f'{wave}_qtdb_single',f'{wave}_qtdb_multi',f'{wave}_ludb',f'{wave}_zhejiang',]].T\n",
    "    both_mtr = eval(f\"both_as_metric\").T[[f'{wave}_qtdb_single',f'{wave}_qtdb_multi',f'{wave}_ludb',f'{wave}_zhejiang',]].T\n",
    "    print(f\"########## {wave} wave ##########\")\n",
    "    print((\"BOTH VS  REAL\", (both_mtr.T-real_mtr.T).mean(axis=1).mean()))\n",
    "    print((\"BOTH VS SYNTH\", (both_mtr.T-synth_mtr.T).mean(axis=1).mean()))\n",
    "    print((\"SYNTH VS REAL\", (synth_mtr.T-real_mtr.T).mean(axis=1).mean()))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net vs W-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('WNET VS UNET', 0.5127777777777768)\n",
      "\n",
      "\n",
      "########## P wave ##########\n",
      "('WNET VS UNET', 0.7370833333333321)\n",
      "\n",
      "\n",
      "########## QRS wave ##########\n",
      "('WNET VS UNET', 0.10319444444444395)\n",
      "\n",
      "\n",
      "########## T wave ##########\n",
      "('WNET VS UNET', 0.6980555555555544)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric = \"F1 Score\"\n",
    "\n",
    "unet = [model for model in valid_models if (\"UNet\" in model)]\n",
    "wnet = [model for model in valid_models if (\"WNet\" in model)]\n",
    "\n",
    "unet_as_metric = {}\n",
    "wnet_as_metric = {}\n",
    "\n",
    "for db in all_databases:\n",
    "    database = eval(db)\n",
    "    \n",
    "    for k in database:\n",
    "        value = {wave: database[k][wave][metric] for wave in all_waves}\n",
    "        unikey = \"_\".join(k.replace(\"UNet\",\"\").replace(\"WNet\",\"\").split(\"_\")[:-1])\n",
    "        \n",
    "        if k in unet:\n",
    "            value = unet_as_metric.get(unikey,{})\n",
    "            value.update({f\"{wave}_{db}\": database[k][wave][metric] for wave in all_waves})\n",
    "            unet_as_metric[unikey] = value\n",
    "        elif k in wnet:\n",
    "            value = wnet_as_metric.get(unikey,{})\n",
    "            value.update({f\"{wave}_{db}\": database[k][wave][metric] for wave in all_waves})\n",
    "            wnet_as_metric[unikey] = value\n",
    "        else:\n",
    "            raise ValueError(\"This should not have happened\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "unet_as_metric = pd.DataFrame(unet_as_metric)\n",
    "wnet_as_metric = pd.DataFrame(wnet_as_metric)\n",
    "\n",
    "# Drop faulty training\n",
    "if \"UNet7LevelsDiceOnly\" in unet_as_metric:\n",
    "    unet_as_metric = unet_as_metric.T.drop(\"UNet7LevelsDiceOnly\").drop(\"UNet6LevelsDiceOnly\").T\n",
    "if \"UNet7LevelsDiceOnly\" in wnet_as_metric:\n",
    "    wnet_as_metric = wnet_as_metric.T.drop(\"UNet7LevelsDiceOnly\").drop(\"UNet6LevelsDiceOnly\").T\n",
    "\n",
    "# Display results\n",
    "print((\"WNET VS UNET\", (wnet_as_metric.T-unet_as_metric.T).mean(axis=1).mean()))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Per wave\n",
    "for wave in all_waves:\n",
    "    unet_mtr = eval(f\"unet_as_metric\").T[[f'{wave}_qtdb_single',f'{wave}_qtdb_multi',f'{wave}_ludb',f'{wave}_zhejiang',]].T\n",
    "    wnet_mtr = eval(f\"wnet_as_metric\").T[[f'{wave}_qtdb_single',f'{wave}_qtdb_multi',f'{wave}_ludb',f'{wave}_zhejiang',]].T\n",
    "    print(f\"########## {wave} wave ##########\")\n",
    "    print((\"WNET VS UNET\", (wnet_mtr.T-unet_mtr.T).mean(axis=1).mean()))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dice vs F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1sc VS dice', 0.25262626262626253)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "########## P wave ##########\n",
      "('f1sc VS dice', 0.38643939393939364)\n",
      "\n",
      "\n",
      "########## QRS wave ##########\n",
      "('f1sc VS dice', 0.031363636363636475)\n",
      "\n",
      "\n",
      "########## T wave ##########\n",
      "('f1sc VS dice', 0.3400757575757576)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "########## qtdb_single database ##########\n",
      "('f1sc VS dice', 0.5213131313131317)\n",
      "\n",
      "\n",
      "########## qtdb_multi database ##########\n",
      "('f1sc VS dice', 0.5013131313131314)\n",
      "\n",
      "\n",
      "########## zhejiang database ##########\n",
      "('f1sc VS dice', 0.08898989898989831)\n",
      "\n",
      "\n",
      "########## ludb database ##########\n",
      "('f1sc VS dice', -0.10111111111111115)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric = \"F1 Score\"\n",
    "\n",
    "dice = [model for model in valid_models if (\"DiceOnly\" in model)]\n",
    "f1sc = [model for model in valid_models if (\"DiceOnly\" not in model)]\n",
    "\n",
    "dice_as_metric = {}\n",
    "f1sc_as_metric = {}\n",
    "\n",
    "for db in all_databases:\n",
    "    database = eval(db)\n",
    "    \n",
    "    for k in database:\n",
    "        value = {wave: database[k][wave][metric] for wave in all_waves}\n",
    "        unikey = \"_\".join(k.replace(\"DiceOnly\",\"\").split(\"_\")[:-1])\n",
    "        \n",
    "        if k in dice:\n",
    "            value = dice_as_metric.get(unikey,{})\n",
    "            value.update({f\"{wave}_{db}\": database[k][wave][metric] for wave in all_waves})\n",
    "            dice_as_metric[unikey] = value\n",
    "        elif k in f1sc:\n",
    "            value = f1sc_as_metric.get(unikey,{})\n",
    "            value.update({f\"{wave}_{db}\": database[k][wave][metric] for wave in all_waves})\n",
    "            f1sc_as_metric[unikey] = value\n",
    "        else:\n",
    "            raise ValueError(\"This should not have happened\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "dice_as_metric = pd.DataFrame(dice_as_metric)\n",
    "f1sc_as_metric = pd.DataFrame(f1sc_as_metric)\n",
    "\n",
    "# Display results\n",
    "print((\"f1sc VS dice\", (f1sc_as_metric.T-dice_as_metric.T).mean(axis=1).mean()))\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Per wave\n",
    "for wave in all_waves:\n",
    "    dice_mtr = dice_as_metric.T[[f'{wave}_qtdb_single',f'{wave}_qtdb_multi',f'{wave}_ludb',f'{wave}_zhejiang',]].T\n",
    "    f1sc_mtr = f1sc_as_metric.T[[f'{wave}_qtdb_single',f'{wave}_qtdb_multi',f'{wave}_ludb',f'{wave}_zhejiang',]].T\n",
    "    print(f\"########## {wave} wave ##########\")\n",
    "    print((\"f1sc VS dice\", (f1sc_mtr.T-dice_mtr.T).mean(axis=1).mean()))\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Per database\n",
    "for db in all_databases:\n",
    "    dice_mtr = dice_as_metric.T[[f\"P_{db}\",f\"QRS_{db}\",f\"T_{db}\"]].T\n",
    "    f1sc_mtr = f1sc_as_metric.T[[f\"P_{db}\",f\"QRS_{db}\",f\"T_{db}\"]].T\n",
    "    print(f\"########## {db} database ##########\")\n",
    "    print((\"f1sc VS dice\", (f1sc_mtr.T-dice_mtr.T).mean(axis=1).mean()))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SelfAttention vs Not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('noselfatt VS selfatt', 0.08263888888888853)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "########## P wave ##########\n",
      "('noselfatt VS selfatt', 0.06291666666666688)\n",
      "\n",
      "\n",
      "########## QRS wave ##########\n",
      "('noselfatt VS selfatt', 0.08333333333333155)\n",
      "\n",
      "\n",
      "########## T wave ##########\n",
      "('noselfatt VS selfatt', 0.10166666666666717)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "########## qtdb_single database ##########\n",
      "('noselfatt VS selfatt', 0.11111111111110876)\n",
      "\n",
      "\n",
      "########## qtdb_multi database ##########\n",
      "('noselfatt VS selfatt', 0.1772222222222237)\n",
      "\n",
      "\n",
      "########## zhejiang database ##########\n",
      "('noselfatt VS selfatt', 0.1494444444444443)\n",
      "\n",
      "\n",
      "########## ludb database ##########\n",
      "('noselfatt VS selfatt', -0.10722222222222261)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric = \"F1 Score\"\n",
    "\n",
    "selfatt = [model for model in valid_models if (\"SelfAttentionConv\" in model)]\n",
    "noselfatt = [model for model in valid_models if (\"SelfAttentionConv\" not in model)]\n",
    "\n",
    "selfatt_as_metric = {}\n",
    "noselfatt_as_metric = {}\n",
    "\n",
    "for db in all_databases:\n",
    "    database = eval(db)\n",
    "    \n",
    "    for k in database:\n",
    "        value = {wave: database[k][wave][metric] for wave in all_waves}\n",
    "        unikey = \"_\".join(k.replace(\"SelfAttentionConv\",\"\").split(\"_\")[:-1])\n",
    "        \n",
    "        if k in selfatt:\n",
    "            value = selfatt_as_metric.get(unikey,{})\n",
    "            value.update({f\"{wave}_{db}\": database[k][wave][metric] for wave in all_waves})\n",
    "            selfatt_as_metric[unikey] = value\n",
    "        elif k in noselfatt:\n",
    "            value = noselfatt_as_metric.get(unikey,{})\n",
    "            value.update({f\"{wave}_{db}\": database[k][wave][metric] for wave in all_waves})\n",
    "            noselfatt_as_metric[unikey] = value\n",
    "        else:\n",
    "            raise ValueError(\"This should not have happened\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "selfatt_as_metric = pd.DataFrame(selfatt_as_metric)\n",
    "noselfatt_as_metric = pd.DataFrame(noselfatt_as_metric)\n",
    "\n",
    "# Display results\n",
    "print((\"noselfatt VS selfatt\", (noselfatt_as_metric.T-selfatt_as_metric.T).mean(axis=1).mean()))\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Per wave\n",
    "for wave in all_waves:\n",
    "    selfatt_mtr = selfatt_as_metric.T[[f'{wave}_qtdb_single',f'{wave}_qtdb_multi',f'{wave}_ludb',f'{wave}_zhejiang',]].T\n",
    "    noselfatt_mtr = noselfatt_as_metric.T[[f'{wave}_qtdb_single',f'{wave}_qtdb_multi',f'{wave}_ludb',f'{wave}_zhejiang',]].T\n",
    "    print(f\"########## {wave} wave ##########\")\n",
    "    print((\"noselfatt VS selfatt\", (noselfatt_mtr.T-selfatt_mtr.T).mean(axis=1).mean()))\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Per database\n",
    "for db in all_databases:\n",
    "    selfatt_mtr = selfatt_as_metric.T[[f\"P_{db}\",f\"QRS_{db}\",f\"T_{db}\"]].T\n",
    "    noselfatt_mtr = noselfatt_as_metric.T[[f\"P_{db}\",f\"QRS_{db}\",f\"T_{db}\"]].T\n",
    "    print(f\"########## {db} database ##########\")\n",
    "    print((\"noselfatt VS selfatt\", (noselfatt_mtr.T-selfatt_mtr.T).mean(axis=1).mean()))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNet5LevelsConvDiceOnly_20201202003451',\n",
       " 'UNet5LevelsConvDiceOnly_real_20201204204042',\n",
       " 'UNet5LevelsConv_20201126031519',\n",
       " 'UNet5LevelsConv_real_20201128082143',\n",
       " 'UNet5LevelsConv_synth_20201207090502',\n",
       " 'UNet5LevelsDiceOnly_20201130125350',\n",
       " 'UNet5LevelsDiceOnly_real_20201203140435',\n",
       " 'UNet5LevelsDiceOnly_synth_20201209121447',\n",
       " 'UNet5Levels_20201124143433',\n",
       " 'UNet5Levels_real_20201127152623',\n",
       " 'UNet5Levels_synth_20201205163610',\n",
       " 'UNet6LevelsConvDiceOnly_20201202055324',\n",
       " 'UNet6LevelsConvDiceOnly_real_20201205031809',\n",
       " 'UNet6LevelsConv_20201126052228',\n",
       " 'UNet6LevelsConv_real_20201128200118',\n",
       " 'UNet6LevelsConv_synth_20201208041815',\n",
       " 'UNet6LevelsDiceOnly_20201202004750',\n",
       " 'UNet6LevelsDiceOnly_real_20201205001620',\n",
       " 'UNet6Levels_20201126042437',\n",
       " 'UNet6Levels_real_20201128150827',\n",
       " 'UNet6Levels_synth_20201207160352',\n",
       " 'UNet7LevelsConvDiceOnly_20201203124825',\n",
       " 'UNet7LevelsConvDiceOnly_real_20201205081529',\n",
       " 'UNet7LevelsConv_20201127152515',\n",
       " 'UNet7LevelsConv_real_20201129035943',\n",
       " 'UNet7LevelsConv_synth_20201209013204',\n",
       " 'UNet7LevelsDiceOnly_20201202071040',\n",
       " 'UNet7LevelsDiceOnly_real_20201205041709',\n",
       " 'UNet7Levels_20201126170003',\n",
       " 'UNet7Levels_real_20201129011017',\n",
       " 'UNet7Levels_synth_20201208095004',\n",
       " 'WNet5LevelsConvDiceOnly_20201130125349',\n",
       " 'WNet5LevelsConvDiceOnly_real_20201204020927',\n",
       " 'WNet5LevelsConv_20201124143432',\n",
       " 'WNet5LevelsConv_real_20201128064651',\n",
       " 'WNet5LevelsConv_synth_20201206145918',\n",
       " 'WNet5LevelsDiceOnly_20201130125349',\n",
       " 'WNet5LevelsDiceOnly_real_20201203185909',\n",
       " 'WNet5LevelsDiceOnly_synth_20201209185735',\n",
       " 'WNet5LevelsSelfAttentionConvDiceOnly_20201130125349',\n",
       " 'WNet5LevelsSelfAttentionConvDiceOnly_real_20201203231939',\n",
       " 'WNet5LevelsSelfAttentionConvDiceOnly_synth_20201210041533',\n",
       " 'WNet5LevelsSelfAttentionConv_20201124143433',\n",
       " 'WNet5LevelsSelfAttentionConv_real_20201128033608',\n",
       " 'WNet5LevelsSelfAttentionConv_synth_20201206085318',\n",
       " 'WNet5LevelsSelfAttentionDiceOnly_20201130125349',\n",
       " 'WNet5LevelsSelfAttentionDiceOnly_real_20201203192805',\n",
       " 'WNet5LevelsSelfAttentionDiceOnly_synth_20201209215314',\n",
       " 'WNet5LevelsSelfAttention_20201124143433',\n",
       " 'WNet5LevelsSelfAttention_real_20201127200934',\n",
       " 'WNet5LevelsSelfAttention_synth_20201206012731',\n",
       " 'WNet5Levels_20201124143434',\n",
       " 'WNet5Levels_real_20201127155141',\n",
       " 'WNet5Levels_synth_20201205201348',\n",
       " 'WNet6LevelsConvDiceOnly_20201202055222',\n",
       " 'WNet6LevelsConvDiceOnly_real_20201205031708',\n",
       " 'WNet6LevelsConv_20201126052125',\n",
       " 'WNet6LevelsConv_real_20201128200024',\n",
       " 'WNet6LevelsConv_synth_20201208041720',\n",
       " 'WNet6LevelsDiceOnly_20201202044521',\n",
       " 'WNet6LevelsDiceOnly_real_20201205030934',\n",
       " 'WNet6LevelsSelfAttentionConvDiceOnly_20201202055118',\n",
       " 'WNet6LevelsSelfAttentionConvDiceOnly_real_20201205031608',\n",
       " 'WNet6LevelsSelfAttentionConv_20201126052023',\n",
       " 'WNet6LevelsSelfAttentionConv_real_20201128195933',\n",
       " 'WNet6LevelsSelfAttentionConv_synth_20201208041625',\n",
       " 'WNet6LevelsSelfAttentionDiceOnly_20201202055015',\n",
       " 'WNet6LevelsSelfAttentionDiceOnly_real_20201205031509',\n",
       " 'WNet6LevelsSelfAttention_20201126051919',\n",
       " 'WNet6LevelsSelfAttention_real_20201128195835',\n",
       " 'WNet6LevelsSelfAttention_synth_20201208041428',\n",
       " 'WNet6Levels_20201126051114',\n",
       " 'WNet6Levels_real_20201128171738',\n",
       " 'WNet6Levels_synth_20201207220959',\n",
       " 'WNet7LevelsConvDiceOnly_20201203124716',\n",
       " 'WNet7LevelsConvDiceOnly_real_20201205081430',\n",
       " 'WNet7LevelsConv_20201127152446',\n",
       " 'WNet7LevelsConv_real_20201129035847',\n",
       " 'WNet7LevelsConv_synth_20201209013108',\n",
       " 'WNet7LevelsDiceOnly_20201203124233',\n",
       " 'WNet7LevelsDiceOnly_real_20201205081019',\n",
       " 'WNet7LevelsSelfAttentionConvDiceOnly_20201203124559',\n",
       " 'WNet7LevelsSelfAttentionConvDiceOnly_real_20201205081330',\n",
       " 'WNet7LevelsSelfAttentionConv_20201127152342',\n",
       " 'WNet7LevelsSelfAttentionConv_real_20201129035753',\n",
       " 'WNet7LevelsSelfAttentionConv_synth_20201209013013',\n",
       " 'WNet7LevelsSelfAttentionDiceOnly_20201203124450',\n",
       " 'WNet7LevelsSelfAttentionDiceOnly_real_20201205081230',\n",
       " 'WNet7LevelsSelfAttention_20201127152239',\n",
       " 'WNet7LevelsSelfAttention_real_20201129035701',\n",
       " 'WNet7LevelsSelfAttention_synth_20201209012917',\n",
       " 'WNet7Levels_20201127152043',\n",
       " 'WNet7Levels_real_20201129035609',\n",
       " 'WNet7Levels_synth_20201209012717',\n",
       " 'WNet6LevelsConvDiceOnly_synth_20201212001406',\n",
       " 'WNet6LevelsSelfAttentionDiceOnly_synth_20201212001100',\n",
       " 'WNet5LevelsConvDiceOnly_synth_20201210195924',\n",
       " 'UNet6LevelsConvDiceOnly_synth_20201212001457',\n",
       " 'WNet6LevelsDiceOnly_synth_20201211183622',\n",
       " 'WNet7LevelsDiceOnly_synth_20201213012150',\n",
       " 'WNet7LevelsSelfAttentionDiceOnly_synth_20201213012350',\n",
       " 'UNet7LevelsDiceOnly_synth_20201212143531',\n",
       " 'UNet7LevelsConvDiceOnly_synth_20201213012639',\n",
       " 'WNet6LevelsSelfAttentionConvDiceOnly_synth_20201212001309',\n",
       " 'UNet6LevelsDiceOnly_synth_20201211150555',\n",
       " 'UNet5LevelsConvDiceOnly_synth_20201211065325',\n",
       " 'WNet7LevelsSelfAttentionConvDiceOnly_synth_20201213012446',\n",
       " 'WNet7LevelsConvDiceOnly_synth_20201213012543',\n",
       " 'WNet5BoundLoss_20201026000000']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('level7 VS level5', -0.1432638888888897)\n",
      "('level7 VS level6', -0.2950694444444448)\n",
      "('level6 VS level5', 0.12189814814814809)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "########## P wave ##########\n",
      "('level7 VS level5', -1.8368750000000011)\n",
      "('level7 VS level6', -1.0029166666666676)\n",
      "('level6 VS level5', -0.5227777777777778)\n",
      "\n",
      "\n",
      "########## QRS wave ##########\n",
      "('level7 VS level5', 0.15999999999999956)\n",
      "('level7 VS level6', 0.04791666666666602)\n",
      "('level6 VS level5', 0.06833333333333375)\n",
      "\n",
      "\n",
      "########## T wave ##########\n",
      "('level7 VS level5', 1.2470833333333327)\n",
      "('level7 VS level6', 0.06979166666666714)\n",
      "('level6 VS level5', 0.8201388888888883)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "########## qtdb_single database ##########\n",
      "('level7 VS level5', -0.46277777777778084)\n",
      "('level7 VS level6', -0.6494444444444472)\n",
      "('level6 VS level5', 0.18759259259259306)\n",
      "\n",
      "\n",
      "########## qtdb_multi database ##########\n",
      "('level7 VS level5', -0.34111111111111186)\n",
      "('level7 VS level6', -0.4638888888888886)\n",
      "('level6 VS level5', 0.1364814814814805)\n",
      "\n",
      "\n",
      "########## zhejiang database ##########\n",
      "('level7 VS level5', 0.13055555555555567)\n",
      "('level7 VS level6', -0.09388888888888798)\n",
      "('level6 VS level5', 0.08796296296296245)\n",
      "\n",
      "\n",
      "########## ludb database ##########\n",
      "('level7 VS level5', 0.10027777777777853)\n",
      "('level7 VS level6', 0.026944444444444413)\n",
      "('level6 VS level5', 0.07555555555555632)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric = \"F1 Score\"\n",
    "\n",
    "level5 = [model for model in valid_models if (\"5Levels\" in model)]\n",
    "level6 = [model for model in valid_models if (\"6Levels\" in model)]\n",
    "level7 = [model for model in valid_models if (\"7Levels\" in model)]\n",
    "\n",
    "\n",
    "level5_as_metric = {}\n",
    "level6_as_metric = {}\n",
    "level7_as_metric = {}\n",
    "\n",
    "for db in all_databases:\n",
    "    database = eval(db)\n",
    "    \n",
    "    for k in database:\n",
    "        value = {wave: database[k][wave][metric] for wave in all_waves}\n",
    "        unikey = \"_\".join(k.split('_')[:-1]).replace(\"5Levels\",\"\").replace(\"6Levels\",\"\").replace(\"7Levels\",\"\")\n",
    "        \n",
    "        if k in level5:\n",
    "            value = level5_as_metric.get(unikey,{})\n",
    "            value.update({f\"{wave}_{db}\": database[k][wave][metric] for wave in all_waves})\n",
    "            level5_as_metric[unikey] = value\n",
    "        elif k in level6:\n",
    "            value = level6_as_metric.get(unikey,{})\n",
    "            value.update({f\"{wave}_{db}\": database[k][wave][metric] for wave in all_waves})\n",
    "            level6_as_metric[unikey] = value\n",
    "        elif k in level7:\n",
    "            value = level7_as_metric.get(unikey,{})\n",
    "            value.update({f\"{wave}_{db}\": database[k][wave][metric] for wave in all_waves})\n",
    "            level7_as_metric[unikey] = value\n",
    "        else:\n",
    "            raise ValueError(\"This should not have happened\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "level5_as_metric = pd.DataFrame(level5_as_metric)\n",
    "level6_as_metric = pd.DataFrame(level6_as_metric)\n",
    "level7_as_metric = pd.DataFrame(level7_as_metric)\n",
    "\n",
    "# Display results\n",
    "print((\"level7 VS level5\", (level7_as_metric.T-level5_as_metric.T).mean(axis=1).mean()))\n",
    "print((\"level7 VS level6\", (level7_as_metric.T-level6_as_metric.T).mean(axis=1).mean()))\n",
    "print((\"level6 VS level5\", (level6_as_metric.T-level5_as_metric.T).mean(axis=1).mean()))\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Per wave\n",
    "for wave in all_waves:\n",
    "    level5_mtr = eval(f\"level5_as_metric\").T[[f'{wave}_qtdb_single',f'{wave}_qtdb_multi',f'{wave}_ludb',f'{wave}_zhejiang',]].T\n",
    "    level6_mtr = eval(f\"level6_as_metric\").T[[f'{wave}_qtdb_single',f'{wave}_qtdb_multi',f'{wave}_ludb',f'{wave}_zhejiang',]].T\n",
    "    level7_mtr = eval(f\"level7_as_metric\").T[[f'{wave}_qtdb_single',f'{wave}_qtdb_multi',f'{wave}_ludb',f'{wave}_zhejiang',]].T\n",
    "    print(f\"########## {wave} wave ##########\")\n",
    "    print((\"level7 VS level5\", (level7_mtr.T-level5_mtr.T).mean(axis=1).mean()))\n",
    "    print((\"level7 VS level6\", (level7_mtr.T-level6_mtr.T).mean(axis=1).mean()))\n",
    "    print((\"level6 VS level5\", (level6_mtr.T-level5_mtr.T).mean(axis=1).mean()))\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Per database\n",
    "for db in all_databases:\n",
    "    level5_mtr = level5_as_metric.T[[f\"P_{db}\",f\"QRS_{db}\",f\"T_{db}\"]].T\n",
    "    level6_mtr = level6_as_metric.T[[f\"P_{db}\",f\"QRS_{db}\",f\"T_{db}\"]].T\n",
    "    level7_mtr = level7_as_metric.T[[f\"P_{db}\",f\"QRS_{db}\",f\"T_{db}\"]].T\n",
    "    print(f\"########## {db} database ##########\")\n",
    "    print((\"level7 VS level5\", (level7_mtr.T-level5_mtr.T).mean(axis=1).mean()))\n",
    "    print((\"level7 VS level6\", (level7_mtr.T-level6_mtr.T).mean(axis=1).mean()))\n",
    "    print((\"level6 VS level5\", (level6_mtr.T-level5_mtr.T).mean(axis=1).mean()))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Zhejiang's and LUDB's mistake at computing on/off error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth(basedir: str, database: str):\n",
    "    if database == 'ludb':\n",
    "        # Load data\n",
    "        P = sak.load_data(os.path.join(basedir,\"ludb\",\"P.csv\"))\n",
    "        QRS = sak.load_data(os.path.join(basedir,\"ludb\",\"QRS.csv\"))\n",
    "        T = sak.load_data(os.path.join(basedir,\"ludb\",\"T.csv\"))\n",
    "    elif database == 'zhejiang':\n",
    "        # Load data\n",
    "        P = sak.load_data(os.path.join(basedir,\"ZhejiangDB\",\"P.csv\"))\n",
    "        QRS = sak.load_data(os.path.join(basedir,\"ZhejiangDB\",\"QRS.csv\"))\n",
    "        T = sak.load_data(os.path.join(basedir,\"ZhejiangDB\",\"T.csv\"))\n",
    "        \n",
    "    # Divide into onsets/offsets\n",
    "    Pon    = {k: P[k][0::2] for k in P}\n",
    "    Poff   = {k: P[k][1::2] for k in P}\n",
    "    QRSon  = {k: QRS[k][0::2] for k in QRS}\n",
    "    QRSoff = {k: QRS[k][1::2] for k in QRS}\n",
    "    Ton    = {k: T[k][0::2] for k in T}\n",
    "    Toff   = {k: T[k][1::2] for k in T}\n",
    "\n",
    "    # Generate validity\n",
    "    validity = {\n",
    "        k: [\n",
    "            np.min(np.concatenate((P.get(k,[+np.inf]),QRS.get(k,[+np.inf]),T.get(k,[+np.inf])))),\n",
    "            np.max(np.concatenate((P.get(k,[-np.inf]),QRS.get(k,[-np.inf]),T.get(k,[-np.inf])))),\n",
    "        ] for k in QRS\n",
    "    }\n",
    "    return Pon,Poff,QRSon,QRSoff,Ton,Toff,validity\n",
    " \n",
    "def get_file_list(basedir: str, database: str):\n",
    "    if database == 'ludb':\n",
    "        files = glob.glob(os.path.join(basedir,'ludb','*.dat'))\n",
    "    elif database == 'zhejiang':\n",
    "        files = glob.glob(os.path.join(basedir,'ZhejiangDB','RAW','*.csv'))\n",
    "    return files\n",
    " \n",
    "def get_sample(file: str, database: str):\n",
    "    if database == 'ludb':\n",
    "        (signal, header) = wfdb.rdsamp(os.path.splitext(file)[0])\n",
    "        fs = header['fs']\n",
    "    elif database == 'zhejiang':\n",
    "        signal = pd.read_csv(file).values\n",
    "        fs = 2000.\n",
    "    return signal, fs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fs = {\"ludb\": 500, \"zhejiang\": 2000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:42<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "for folder in tqdm.tqdm(folders):\n",
    "    if (\n",
    "        (not os.path.isfile(os.path.join(folder,'metrics_string.txt'))) or\n",
    "        (not os.path.isfile(os.path.join(folder,'zhejiang_metrics_string.txt'))) or\n",
    "        (not os.path.isfile(os.path.join(folder,'ludb_metrics_string.txt'))) or\n",
    "        (not os.path.isfile(os.path.join(folder,'metrics_fold_string.txt')))\n",
    "       ):\n",
    "        continue\n",
    "    \n",
    "    for db in [\"zhejiang\", \"ludb\"]:\n",
    "        # Get predicted GT\n",
    "        pon    = sak.load_data(os.path.join(folder,f\"{db}_predicted_pon.csv\"))\n",
    "        poff   = sak.load_data(os.path.join(folder,f\"{db}_predicted_poff.csv\"))\n",
    "        qrson  = sak.load_data(os.path.join(folder,f\"{db}_predicted_qrson.csv\"))\n",
    "        qrsoff = sak.load_data(os.path.join(folder,f\"{db}_predicted_qrsoff.csv\"))\n",
    "        ton    = sak.load_data(os.path.join(folder,f\"{db}_predicted_ton.csv\"))\n",
    "        toff   = sak.load_data(os.path.join(folder,f\"{db}_predicted_toff.csv\"))\n",
    "\n",
    "        # Get true fiducials\n",
    "        Pon,Poff,QRSon,QRSoff,Ton,Toff,validity = get_ground_truth(basedir, db)\n",
    "\n",
    "        # Produce metrics\n",
    "        metrics = {}\n",
    "\n",
    "        for wave in ['p','qrs','t']:\n",
    "            metrics[wave] = {}\n",
    "            metrics[wave]['truepositives'] = {}\n",
    "            metrics[wave]['falsepositives'] = {}\n",
    "            metrics[wave]['falsenegatives'] = {}\n",
    "            metrics[wave]['onerrors'] = {}\n",
    "            metrics[wave]['offerrors'] = {}\n",
    "\n",
    "            input_on   = eval('{}on'.format(wave.lower()))\n",
    "            input_off  = eval('{}off'.format(wave.lower()))\n",
    "            target_on  = eval('{}on'.format(wave.upper()))\n",
    "            target_off = eval('{}off'.format(wave.upper()))\n",
    "\n",
    "            # Compute metrics\n",
    "            for k_input in input_on:\n",
    "                k_target = f\"{k_input}###I\"\n",
    "                try:\n",
    "                    # Refine input and output's regions w/ validity vectors\n",
    "                    (input_on[k_input],input_off[k_input]) = src.metrics.filter_valid(input_on[k_input],input_off[k_input], validity[k_target][0], validity[k_target][1],operation=\"or\")\n",
    "                    (target_on[k_target],target_off[k_target]) = src.metrics.filter_valid(target_on[k_target],target_off[k_target], validity[k_target][0], validity[k_target][1],operation=\"or\")\n",
    "                    tp,fp,fn,dice,onerror,offerror = src.metrics.compute_metrics(input_on[k_input],input_off[k_input],target_on[k_target],target_off[k_target])\n",
    "                except:\n",
    "                    continue\n",
    "                metrics[wave]['truepositives'][k_input] = tp\n",
    "                metrics[wave]['falsepositives'][k_input] = fp\n",
    "                metrics[wave]['falsenegatives'][k_input] = fn\n",
    "                metrics[wave]['onerrors'][k_input] = np.copy(np.array(onerror))\n",
    "                metrics[wave]['offerrors'][k_input] = np.copy(np.array(offerror))\n",
    "\n",
    "        #########################################################################\n",
    "        # Get stupid metric string\n",
    "        metrics_string = \"\"\n",
    "        metrics_string += \"\\n# {}\".format(model_name)\n",
    "\n",
    "        for wave in ['p','qrs','t']:\n",
    "            metrics_string += \"\\n######### {} wave #########\".format(wave.upper())\n",
    "            metrics_string += \"\\n\"\n",
    "            metrics_string += \"\\nPrecision:    {}%\".format(np.round(src.metrics.precision(sum(metrics[wave]['truepositives'].values()),sum(metrics[wave]['falsepositives'].values()),sum(metrics[wave]['falsenegatives'].values()))*100,decimals=2))\n",
    "            metrics_string += \"\\nRecall:       {}%\".format(np.round(src.metrics.recall(sum(metrics[wave]['truepositives'].values()),sum(metrics[wave]['falsepositives'].values()),sum(metrics[wave]['falsenegatives'].values()))*100,decimals=2))\n",
    "            metrics_string += \"\\nF1 score:     {}%\".format(np.round(src.metrics.f1_score(sum(metrics[wave]['truepositives'].values()),sum(metrics[wave]['falsepositives'].values()),sum(metrics[wave]['falsenegatives'].values()))*100,decimals=2))\n",
    "            metrics_string += \"\\n\"\n",
    "            metrics_string += \"\\nOnset Error:  {} ± {} ms\".format(np.round(np.mean([v for l in metrics[wave]['onerrors'].values() for v in l])/target_fs[db]*1000,decimals=2),np.round(np.std([v for l in metrics[wave]['onerrors'].values() for v in l])/target_fs[db]*1000,decimals=2))\n",
    "            metrics_string += \"\\nOffset Error: {} ± {} ms\".format(np.round(np.mean([v for l in metrics[wave]['offerrors'].values() for v in l])/target_fs[db]*1000,decimals=2),np.round(np.std([v for l in metrics[wave]['offerrors'].values() for v in l])/target_fs[db]*1000,decimals=2))\n",
    "            metrics_string += \"\\n\\n\"\n",
    "\n",
    "        metrics_string += \"\\n---\"\n",
    "\n",
    "        \n",
    "        # Save produced metrics\n",
    "        original_stdout = sys.stdout # Save a reference to the original standard output\n",
    "        with open(os.path.join(folder,f'{db}_metrics_string.txt'), 'w') as f:\n",
    "            sys.stdout = f # Change the standard output to the file we created.\n",
    "            print(metrics_string)\n",
    "\n",
    "        sys.stdout = original_stdout # Reset the standard output to its original value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted GT\n",
    "pon    = sak.load_data(os.path.join(folder,f\"{db}_predicted_pon.csv\"))\n",
    "poff   = sak.load_data(os.path.join(folder,f\"{db}_predicted_poff.csv\"))\n",
    "qrson  = sak.load_data(os.path.join(folder,f\"{db}_predicted_qrson.csv\"))\n",
    "qrsoff = sak.load_data(os.path.join(folder,f\"{db}_predicted_qrsoff.csv\"))\n",
    "ton    = sak.load_data(os.path.join(folder,f\"{db}_predicted_ton.csv\"))\n",
    "toff   = sak.load_data(os.path.join(folder,f\"{db}_predicted_toff.csv\"))\n",
    "\n",
    "# Get true fiducials\n",
    "Pon,Poff,QRSon,QRSoff,Ton,Toff,validity = get_ground_truth(basedir, 'zhejiang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce metrics\n",
    "metrics = {}\n",
    "\n",
    "for wave in ['p','qrs','t']:\n",
    "    metrics[wave] = {}\n",
    "    metrics[wave]['truepositives'] = {}\n",
    "    metrics[wave]['falsepositives'] = {}\n",
    "    metrics[wave]['falsenegatives'] = {}\n",
    "    metrics[wave]['onerrors'] = {}\n",
    "    metrics[wave]['offerrors'] = {}\n",
    "\n",
    "    input_on   = eval('{}on'.format(wave.lower()))\n",
    "    input_off  = eval('{}off'.format(wave.lower()))\n",
    "    target_on  = eval('{}on'.format(wave.upper()))\n",
    "    target_off = eval('{}off'.format(wave.upper()))\n",
    "\n",
    "    # Compute metrics\n",
    "    for k_input in input_on:\n",
    "        k_target = f\"{k_input}###I\"\n",
    "        try:\n",
    "            # Refine input and output's regions w/ validity vectors\n",
    "            (input_on[k_input],input_off[k_input]) = src.metrics.filter_valid(input_on[k_input],input_off[k_input], validity[k_target][0], validity[k_target][1],operation=\"or\")\n",
    "            (target_on[k_target],target_off[k_target]) = src.metrics.filter_valid(target_on[k_target],target_off[k_target], validity[k_target][0], validity[k_target][1],operation=\"or\")\n",
    "            tp,fp,fn,dice,onerror,offerror = src.metrics.compute_metrics(input_on[k_input],input_off[k_input],target_on[k_target],target_off[k_target])\n",
    "        except:\n",
    "            continue\n",
    "        metrics[wave]['truepositives'][k_input] = tp\n",
    "        metrics[wave]['falsepositives'][k_input] = fp\n",
    "        metrics[wave]['falsenegatives'][k_input] = fn\n",
    "        metrics[wave]['onerrors'][k_input] = np.copy(np.array(onerror))\n",
    "        metrics[wave]['offerrors'][k_input] = np.copy(np.array(offerror))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# Get stupid metric string\n",
    "metrics_string = \"\"\n",
    "metrics_string += \"\\n# {}\".format(model_name)\n",
    "\n",
    "for wave in ['p','qrs','t']:\n",
    "    metrics_string += \"\\n######### {} wave #########\".format(wave.upper())\n",
    "    metrics_string += \"\\n\"\n",
    "    metrics_string += \"\\nPrecision:    {}%\".format(np.round(src.metrics.precision(sum(metrics[wave]['truepositives'].values()),sum(metrics[wave]['falsepositives'].values()),sum(metrics[wave]['falsenegatives'].values()))*100,decimals=2))\n",
    "    metrics_string += \"\\nRecall:       {}%\".format(np.round(src.metrics.recall(sum(metrics[wave]['truepositives'].values()),sum(metrics[wave]['falsepositives'].values()),sum(metrics[wave]['falsenegatives'].values()))*100,decimals=2))\n",
    "    metrics_string += \"\\nF1 score:     {}%\".format(np.round(src.metrics.f1_score(sum(metrics[wave]['truepositives'].values()),sum(metrics[wave]['falsepositives'].values()),sum(metrics[wave]['falsenegatives'].values()))*100,decimals=2))\n",
    "    metrics_string += \"\\n\"\n",
    "    metrics_string += \"\\nOnset Error:  {} ± {} ms\".format(np.round(np.mean([v for l in metrics[wave]['onerrors'].values() for v in l])/target_fs[db]*1000,decimals=2),np.round(np.std([v for l in metrics[wave]['onerrors'].values() for v in l])/target_fs[db]*1000,decimals=2))\n",
    "    metrics_string += \"\\nOffset Error: {} ± {} ms\".format(np.round(np.mean([v for l in metrics[wave]['offerrors'].values() for v in l])/target_fs[db]*1000,decimals=2),np.round(np.std([v for l in metrics[wave]['offerrors'].values() for v in l])/target_fs[db]*1000,decimals=2))\n",
    "    metrics_string += \"\\n\\n\"\n",
    "\n",
    "metrics_string += \"\\n---\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zhejiang'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/guille/DADES/DADES/Delineator/TrainedModels/UNet5LevelsConvDiceOnly_real_20201204204042'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(folder,f'{db}_metrics_string.txt'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/guille/DADES/DADES/Delineator/TrainedModels/UNet5LevelsConvDiceOnly_real_20201204204042/zhejiang_metrics_string_pre_fix.txt'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy(os.path.join(folder,f'{db}_metrics_string.txt'),os.path.join(folder,f'{db}_metrics_string_pre_fix.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# UNet5LevelsConvDiceOnly_synth_20201211065325\n",
      "######### P wave #########\n",
      "\n",
      "Precision:    98.35%\n",
      "Recall:       94.96%\n",
      "F1 score:     96.62%\n",
      "\n",
      "Onset Error:  3.3 ± 14.38 ms\n",
      "Offset Error: 9.79 ± 12.63 ms\n",
      "\n",
      "\n",
      "######### QRS wave #########\n",
      "\n",
      "Precision:    99.13%\n",
      "Recall:       99.66%\n",
      "F1 score:     99.4%\n",
      "\n",
      "Onset Error:  -0.29 ± 13.13 ms\n",
      "Offset Error: 4.82 ± 11.68 ms\n",
      "\n",
      "\n",
      "######### T wave #########\n",
      "\n",
      "Precision:    94.85%\n",
      "Recall:       98.85%\n",
      "F1 score:     96.81%\n",
      "\n",
      "Onset Error:  0.94 ± 34.25 ms\n",
      "Offset Error: -3.23 ± 35.53 ms\n",
      "\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(metrics_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth(basedir: str, database: str):\n",
    "    if database == 'ludb':\n",
    "        # Load data\n",
    "        P = sak.load_data(os.path.join(basedir,\"ludb\",\"P.csv\"))\n",
    "        QRS = sak.load_data(os.path.join(basedir,\"ludb\",\"QRS.csv\"))\n",
    "        T = sak.load_data(os.path.join(basedir,\"ludb\",\"T.csv\"))\n",
    "    elif database == 'zhejiang':\n",
    "        # Load data\n",
    "        P = sak.load_data(os.path.join(basedir,\"ZhejiangDB\",\"P.csv\"))\n",
    "        QRS = sak.load_data(os.path.join(basedir,\"ZhejiangDB\",\"QRS.csv\"))\n",
    "        T = sak.load_data(os.path.join(basedir,\"ZhejiangDB\",\"T.csv\"))\n",
    "        \n",
    "    # Divide into onsets/offsets\n",
    "    Pon    = {k: P[k][0::2] for k in P}\n",
    "    Poff   = {k: P[k][1::2] for k in P}\n",
    "    QRSon  = {k: QRS[k][0::2] for k in QRS}\n",
    "    QRSoff = {k: QRS[k][1::2] for k in QRS}\n",
    "    Ton    = {k: T[k][0::2] for k in T}\n",
    "    Toff   = {k: T[k][1::2] for k in T}\n",
    "\n",
    "    # Generate validity\n",
    "    validity = {\n",
    "        k: [\n",
    "            np.min(np.concatenate((P.get(k,[+np.inf]),QRS.get(k,[+np.inf]),T.get(k,[+np.inf])))),\n",
    "            np.max(np.concatenate((P.get(k,[-np.inf]),QRS.get(k,[-np.inf]),T.get(k,[-np.inf])))),\n",
    "        ] for k in QRS\n",
    "    }\n",
    "    return Pon,Poff,QRSon,QRSoff,Ton,Toff,validity\n",
    " \n",
    "def get_file_list(basedir: str, database: str):\n",
    "    if database == 'ludb':\n",
    "        files = glob.glob(os.path.join(basedir,'ludb','*.dat'))\n",
    "    elif database == 'zhejiang':\n",
    "        files = glob.glob(os.path.join(basedir,'ZhejiangDB','RAW','*.csv'))\n",
    "    return files\n",
    " \n",
    "def get_sample(file: str, database: str):\n",
    "    if database == 'ludb':\n",
    "        (signal, header) = wfdb.rdsamp(os.path.splitext(file)[0])\n",
    "        fs = header['fs']\n",
    "    elif database == 'zhejiang':\n",
    "        signal = pd.read_csv(file).values\n",
    "        fs = 2000.\n",
    "    return signal, fs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/media/guille/DADES/DADES/Delineator/'\n",
    "_,_,QRSon,QRSoff,_,_,_ = get_ground_truth(basedir,\"ludb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "QRSon  = sak.load_data(os.path.join(basedir, 'QTDB', 'QRSonNew.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3295"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([QRSon[k].size for k in QRSon])//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for k in QRSon:\n",
    "    if k.endswith(\"###I\"):\n",
    "        counter += QRSon[k].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for file in glob.glob('/media/guille/DADES/DADES/PhysioNet/QTDB/RAW/*.q1c'):\n",
    "    fname,ext = os.path.splitext(file)\n",
    "    ext = ext.replace('.','')\n",
    "    \n",
    "    ann = wfdb.rdann(fname,ext)\n",
    "    counter += (np.array(ann.symbol) == 'N').sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3528"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1830"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "21966//12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "611"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = get_file_list(basedir,\"zhejiang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "for file in file_list:\n",
    "    sample,fs = get_sample(file,\"zhejiang\")\n",
    "    \n",
    "    samples.append(sample.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5791"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.622"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(samples)/fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning3",
   "language": "python",
   "name": "deeplearning3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
