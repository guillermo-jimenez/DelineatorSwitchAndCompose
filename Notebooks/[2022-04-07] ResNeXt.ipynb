{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import skimage\n",
    "import skimage.segmentation\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import math\n",
    "import shutil\n",
    "import pathlib\n",
    "import glob\n",
    "import shutil\n",
    "import uuid\n",
    "import random\n",
    "import platform\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import wfdb\n",
    "import json\n",
    "import tqdm\n",
    "import dill\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import src.data\n",
    "import sak\n",
    "import sak.signal.wavelet\n",
    "import sak.data\n",
    "import sak.data.augmentation\n",
    "import sak.visualization\n",
    "import sak.visualization.signal\n",
    "import sak.torch\n",
    "import sak.torch.nn\n",
    "import sak.torch.nn as nn\n",
    "import sak.torch.train\n",
    "import sak.torch.data\n",
    "import sak.data.preprocessing\n",
    "import sak.torch.models\n",
    "import sak.torch.models.lego\n",
    "import sak.torch.models.variational\n",
    "import sak.torch.models.classification\n",
    "\n",
    "from sak.signal import StandardHeader\n",
    "\n",
    "def smooth(x: np.ndarray, window_size: int, conv_mode: str = 'same'):\n",
    "    x = np.pad(np.copy(x),(window_size,window_size),'edge')\n",
    "    window = np.hamming(window_size)/(window_size//2)\n",
    "    x = np.convolve(x, window, mode=conv_mode)\n",
    "    x = x[window_size:-window_size]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./configurations/WNeXt6Levels.json', 'r') as f:\n",
    "    execution = json.load(f)\n",
    "# Define model\n",
    "model = sak.from_dict(execution[\"model\"]).float()\n",
    "# plt.figure(figsize=(50,50));model.draw_networkx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:00<00:00, 2520.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue with file sel35_0, continuing...\n",
      "Issue with file sel35_1, continuing...\n",
      "################# FOLD 1 #################\n"
     ]
    }
   ],
   "source": [
    "input_files = './pickle/'\n",
    "\n",
    "##### 2. Load synthetic dataset #####\n",
    "# 2.1. Load individual segments\n",
    "P = sak.pickleload(os.path.join(input_files,\"Psignal_new.pkl\"))\n",
    "PQ = sak.pickleload(os.path.join(input_files,\"PQsignal_new.pkl\"))\n",
    "QRS = sak.pickleload(os.path.join(input_files,\"QRSsignal_new.pkl\"))\n",
    "ST = sak.pickleload(os.path.join(input_files,\"STsignal_new.pkl\"))\n",
    "T = sak.pickleload(os.path.join(input_files,\"Tsignal_new.pkl\"))\n",
    "TP = sak.pickleload(os.path.join(input_files,\"TPsignal_new.pkl\"))\n",
    "\n",
    "Pamplitudes = sak.pickleload(os.path.join(input_files,\"Pamplitudes_new.pkl\"))\n",
    "PQamplitudes = sak.pickleload(os.path.join(input_files,\"PQamplitudes_new.pkl\"))\n",
    "QRSamplitudes = sak.pickleload(os.path.join(input_files,\"QRSamplitudes_new.pkl\"))\n",
    "STamplitudes = sak.pickleload(os.path.join(input_files,\"STamplitudes_new.pkl\"))\n",
    "Tamplitudes = sak.pickleload(os.path.join(input_files,\"Tamplitudes_new.pkl\"))\n",
    "TPamplitudes = sak.pickleload(os.path.join(input_files,\"TPamplitudes_new.pkl\"))\n",
    "\n",
    "# 2.2. Get amplitude distribution\n",
    "Pdistribution   = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(Pamplitudes.values()))))\n",
    "PQdistribution  = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(PQamplitudes.values()))))\n",
    "QRSdistribution = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.hstack((np.array(list(QRSamplitudes.values())), 2-np.array(list(QRSamplitudes.values()))))))\n",
    "STdistribution  = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(STamplitudes.values()))))\n",
    "Tdistribution   = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(Tamplitudes.values()))))\n",
    "TPdistribution  = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(TPamplitudes.values()))))\n",
    "\n",
    "# 2.3. Smooth all\n",
    "window = 5\n",
    "P   = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth(  P[k],window)),metric=sak.signal.abs_max) for k in   P}\n",
    "PQ  = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth( PQ[k],window)),metric=sak.signal.abs_max) for k in  PQ}\n",
    "QRS = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth(QRS[k],window)),metric=sak.signal.abs_max) for k in QRS}\n",
    "ST  = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth( ST[k],window)),metric=sak.signal.abs_max) for k in  ST}\n",
    "T   = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth(  T[k],window)),metric=sak.signal.abs_max) for k in   T}\n",
    "TP  = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth( TP[k],window)),metric=sak.signal.abs_max) for k in  TP}\n",
    "\n",
    "\n",
    "##### 3. Load QTDB #####\n",
    "dataset             = pd.read_csv(os.path.join(input_files,'QTDB','Dataset.csv'), index_col=0)\n",
    "dataset             = dataset.sort_index(axis=1)\n",
    "labels              = np.asarray(list(dataset)) # In case no data augmentation is applied\n",
    "description         = dataset.describe()\n",
    "group               = {k: '_'.join(k.split('_')[:-1]) for k in dataset}\n",
    "unique_ids          = list(set([k.split('_')[0] for k in dataset]))\n",
    "\n",
    "# Load validity\n",
    "validity            = sak.load_data(os.path.join(input_files,'QTDB','validity.csv'))\n",
    "\n",
    "# Load fiducials\n",
    "Pon_QTDB            = sak.load_data(os.path.join(input_files,'QTDB','PonNew.csv'))\n",
    "Poff_QTDB           = sak.load_data(os.path.join(input_files,'QTDB','PoffNew.csv'))\n",
    "QRSon_QTDB          = sak.load_data(os.path.join(input_files,'QTDB','QRSonNew.csv'))\n",
    "QRSoff_QTDB         = sak.load_data(os.path.join(input_files,'QTDB','QRSoffNew.csv'))\n",
    "Ton_QTDB            = sak.load_data(os.path.join(input_files,'QTDB','TonNew.csv'))\n",
    "Toff_QTDB           = sak.load_data(os.path.join(input_files,'QTDB','ToffNew.csv'))\n",
    "\n",
    "# Generate masks & signals\n",
    "signal_QTDB = {}\n",
    "segmentation_QTDB = {}\n",
    "for k in tqdm.tqdm(QRSon_QTDB):\n",
    "    # Check file exists and all that\n",
    "    if k not in validity:\n",
    "        print(\"Issue with file {}, continuing...\".format(k))\n",
    "        continue\n",
    "\n",
    "    # Store signal\n",
    "    signal = dataset[k][validity[k][0]:validity[k][1]].values\n",
    "    signal = sak.signal.on_off_correction(signal)\n",
    "    amplitude = np.median(sak.signal.moving_lambda(signal,200,sak.signal.abs_max))\n",
    "    signal = signal/amplitude\n",
    "    signal_QTDB[k] = signal[None,]\n",
    "\n",
    "    # Generate boolean mask\n",
    "    segmentation = np.zeros((3,dataset.shape[0]),dtype=bool)\n",
    "    if k in Pon_QTDB:\n",
    "        for on,off in zip(Pon_QTDB[k],Poff_QTDB[k]):\n",
    "            segmentation[0,on:off] = True\n",
    "    if k in QRSon_QTDB:\n",
    "        for on,off in zip(QRSon_QTDB[k],QRSoff_QTDB[k]):\n",
    "            segmentation[1,on:off] = True\n",
    "    if k in Ton_QTDB:\n",
    "        for on,off in zip(Ton_QTDB[k],Toff_QTDB[k]):\n",
    "            segmentation[2,on:off] = True\n",
    "\n",
    "    segmentation_QTDB[k] = segmentation[:,validity[k][0]:validity[k][1]]\n",
    "\n",
    "\n",
    "##### 4. Generate random splits #####\n",
    "# 4.1. Split into train and test\n",
    "all_keys_synthetic = {}\n",
    "for k in list(P) + list(PQ) + list(QRS) + list(ST) + list(T) + list(TP):\n",
    "    uid = k.split(\"###\")[0].split(\"_\")[0].split(\"-\")[0]\n",
    "    if uid not in all_keys_synthetic:\n",
    "        all_keys_synthetic[uid] = [k]\n",
    "    else:\n",
    "        all_keys_synthetic[uid].append(k)\n",
    "\n",
    "all_keys_real = {}\n",
    "for k in list(signal_QTDB) + list(segmentation_QTDB):\n",
    "    uid = k.split(\"###\")[0].split(\"_\")[0].split(\"-\")[0]\n",
    "    if uid not in all_keys_real:\n",
    "        all_keys_real[uid] = [k]\n",
    "    else:\n",
    "        all_keys_real[uid].append(k)\n",
    "\n",
    "# 4.2. Get database and file\n",
    "filenames = []\n",
    "database = []\n",
    "for k in all_keys_synthetic:\n",
    "    filenames.append(k)\n",
    "    if k.startswith(\"SOO\"):\n",
    "        database.append(0)\n",
    "    elif k.startswith(\"sel\"):\n",
    "        database.append(1)\n",
    "    else:\n",
    "        database.append(2)\n",
    "filenames = np.array(filenames)\n",
    "database = np.array(database)\n",
    "\n",
    "# Set random seed for the execution and perform train/test splitting\n",
    "random.seed(execution[\"seed\"])\n",
    "np.random.seed(execution[\"seed\"])\n",
    "torch.random.manual_seed(execution[\"seed\"])\n",
    "splitter = sklearn.model_selection.StratifiedKFold(5).split(filenames,database)\n",
    "splits = list(splitter)\n",
    "indices_train = [s[0] for s in splits]\n",
    "indices_valid = [s[1] for s in splits]\n",
    "\n",
    "##### 5. Train folds #####\n",
    "# 5.1. Save model-generating files\n",
    "target_path = execution[\"save_directory\"] # Store original output path for future usage\n",
    "original_length = execution[\"dataset\"][\"length\"]\n",
    "\n",
    "# 5.2. Save folds of valid files\n",
    "all_folds_test = {\"fold_{}\".format(i+1): np.array(filenames)[ix_valid] for i,ix_valid in enumerate(indices_valid)}\n",
    "\n",
    "# 5.3. Iterate over folds\n",
    "for i,(ix_train,ix_valid) in enumerate(zip(indices_train,indices_valid)):\n",
    "    print(\"################# FOLD {} #################\".format(i+1))\n",
    "    # Synthetic keys\n",
    "    train_keys_synthetic, valid_keys_synthetic = ([],[])\n",
    "    for k in np.array(filenames)[ix_train]: \n",
    "        train_keys_synthetic += all_keys_synthetic[k]\n",
    "    for k in np.array(filenames)[ix_valid]: \n",
    "        valid_keys_synthetic += all_keys_synthetic[k]\n",
    "\n",
    "    # Real keys\n",
    "    train_keys_real, valid_keys_real = ([],[])\n",
    "    for k in np.array(filenames)[ix_train]: \n",
    "        if k in all_keys_real: train_keys_real += all_keys_real[k]\n",
    "    for k in np.array(filenames)[ix_valid]: \n",
    "        if k in all_keys_real: valid_keys_real += all_keys_real[k]\n",
    "\n",
    "    # Avoid repetitions\n",
    "    train_keys_synthetic = list(set(train_keys_synthetic))\n",
    "    valid_keys_synthetic = list(set(valid_keys_synthetic))\n",
    "    train_keys_real = list(set(train_keys_real))\n",
    "    valid_keys_real = list(set(valid_keys_real))\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~ Refine synthetic set ~~~~~~~~~~~~~~~~~~~~\n",
    "    # Divide train/valid segments\n",
    "    Ptrain   = {k:   P[k] for k in   P if k in train_keys_synthetic}\n",
    "    PQtrain  = {k:  PQ[k] for k in  PQ if k in train_keys_synthetic}\n",
    "    QRStrain = {k: QRS[k] for k in QRS if k in train_keys_synthetic}\n",
    "    STtrain  = {k:  ST[k] for k in  ST if k in train_keys_synthetic}\n",
    "    Ttrain   = {k:   T[k] for k in   T if k in train_keys_synthetic}\n",
    "    TPtrain  = {k:  TP[k] for k in  TP if k in train_keys_synthetic}\n",
    "\n",
    "    Pvalid   = {k:   P[k] for k in   P if k in valid_keys_synthetic}\n",
    "    PQvalid  = {k:  PQ[k] for k in  PQ if k in valid_keys_synthetic}\n",
    "    QRSvalid = {k: QRS[k] for k in QRS if k in valid_keys_synthetic}\n",
    "    STvalid  = {k:  ST[k] for k in  ST if k in valid_keys_synthetic}\n",
    "    Tvalid   = {k:   T[k] for k in   T if k in valid_keys_synthetic}\n",
    "    TPvalid  = {k:  TP[k] for k in  TP if k in valid_keys_synthetic}\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~ Refine real set ~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    signal_QTDB_train       = {k:       signal_QTDB[k] for k in       signal_QTDB if k in train_keys_real}\n",
    "    signal_QTDB_valid       = {k:       signal_QTDB[k] for k in       signal_QTDB if k in valid_keys_real}\n",
    "    segmentation_QTDB_train = {k: segmentation_QTDB[k] for k in segmentation_QTDB if k in train_keys_real}\n",
    "    segmentation_QTDB_valid = {k: segmentation_QTDB[k] for k in segmentation_QTDB if k in valid_keys_real}\n",
    "\n",
    "    # Define synthetic datasets\n",
    "    dataset_train_synthetic = src.data.Dataset(Ptrain, QRStrain, Ttrain, PQtrain, STtrain, TPtrain, \n",
    "                                               Pdistribution, QRSdistribution, Tdistribution, PQdistribution, \n",
    "                                               STdistribution, TPdistribution, **execution[\"dataset\"])\n",
    "    execution[\"dataset\"][\"length\"] = execution[\"dataset\"][\"length\"]//4 # On synthetic data, not so useful to do intensive validation\n",
    "    dataset_valid_synthetic = src.data.Dataset(Pvalid, QRSvalid, Tvalid, PQvalid, STvalid, TPvalid, \n",
    "                                               Pdistribution, QRSdistribution, Tdistribution, PQdistribution, \n",
    "                                               STdistribution, TPdistribution, **execution[\"dataset\"])\n",
    "    execution[\"dataset\"][\"length\"] = original_length # On synthetic data, not so useful to do intensive validation\n",
    "\n",
    "    # Define real datasets\n",
    "    dataset_train_real = src.data.DatasetQTDB(signal_QTDB_train,segmentation_QTDB_train,execution[\"dataset\"][\"N\"],128)\n",
    "    dataset_valid_real = src.data.DatasetQTDB(signal_QTDB_valid,segmentation_QTDB_valid,execution[\"dataset\"][\"N\"],128)\n",
    "\n",
    "    # Define merging dataset\n",
    "    dataset_train = sak.torch.data.UniformMultiDataset((dataset_train_synthetic,dataset_train_real),[10,1],[1,10],return_weights=True)\n",
    "    sampler_train = sak.torch.data.UniformMultiSampler(dataset_train)\n",
    "    dataset_valid = sak.torch.data.UniformMultiDataset((dataset_valid_synthetic,dataset_valid_real),[10,1],[1,10],return_weights=True)\n",
    "    sampler_valid = sak.torch.data.UniformMultiSampler(dataset_valid)\n",
    "\n",
    "    # Create dataloaders\n",
    "    loader_train = torch.utils.data.DataLoader(dataset_train, sampler=sampler_train, **execution[\"loader\"])\n",
    "    loader_valid = torch.utils.data.DataLoader(dataset_valid, sampler=sampler_valid, **execution[\"loader\"])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tmp in loader_train:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import timm.models\n",
    "import timm.models.layers\n",
    "import timm.models.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from sak.__ops import required\n",
    "from sak.__ops import check_required\n",
    "from sak import class_selector\n",
    "\n",
    "class ConvNeXtBlockNd(torch.nn.Module):\n",
    "    def __init__(self, in_channels: int = required, layer_scale_init_value: float = 1e-6, \n",
    "                 drop_path: float = 0., dim: int = required, **kwargs: dict):\n",
    "        super(ConvNeXtBlockNd, self).__init__()\n",
    "        # Check required inputs\n",
    "        check_required(self, {\"in_channels\":in_channels, \"dim\":dim})\n",
    "\n",
    "        # Establish default inputs\n",
    "        kwargs[\"groups\"] = in_channels\n",
    "        kwargs[\"kernel_size\"] = kwargs.get(\"kernel_size\",7)\n",
    "        kwargs[\"padding\"] = kwargs.get(\"padding\", (kwargs[\"kernel_size\"]-1)//2)\n",
    "        activation = kwargs.pop(\"activation\",\"torch.nn.GELU\")\n",
    "        initializer = kwargs.pop(\"initializer\",\"timm.models.layers.trunc_normal_\")\n",
    "\n",
    "        # Declare operations\n",
    "        if   dim == 1: \n",
    "            self.depthwise_conv = torch.nn.Conv1d(in_channels, in_channels, **kwargs)\n",
    "            self.permute_in,self.permute_out = [0,2,1],     [0,2,1]\n",
    "        elif dim == 2: \n",
    "            self.depthwise_conv = torch.nn.Conv2d(in_channels, in_channels, **kwargs)\n",
    "            self.permute_in,self.permute_out = [0,2,3,1],   [0,3,1,2]\n",
    "        elif dim == 3: \n",
    "            self.depthwise_conv = torch.nn.Conv3d(in_channels, in_channels, **kwargs)\n",
    "            self.permute_in,self.permute_out = [0,2,3,4,1], [0,4,1,2,3]\n",
    "        else: raise ValueError(\"Invalid number of dimensions: {}\".format(dim))\n",
    "        self.normalization    = torch.nn.LayerNorm(in_channels,eps=1e-6)\n",
    "        self.pointwise_conv_1 = torch.nn.Linear(in_channels, 4*in_channels)\n",
    "        self.activation       = class_selector(activation)()\n",
    "        self.pointwise_conv_2 = torch.nn.Linear(4*in_channels, in_channels)\n",
    "        self.gamma            = nn.Parameter(layer_scale_init_value * torch.ones((in_channels,)), \n",
    "                                             requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path        = timm.models.layers.DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        \n",
    "        # Initialize weights values\n",
    "        initializer = class_selector(initializer)\n",
    "        initializer(self.depthwise_conv.weight)\n",
    "        initializer(self.pointwise_conv_1.weight)\n",
    "        initializer(self.pointwise_conv_2.weight)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x_prev = x\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = x.permute(*self.permute_in) # (N, C, ...) -> (N, ..., C)\n",
    "        x = self.normalization(x)\n",
    "        x = self.pointwise_conv_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.pointwise_conv_2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = x.permute(*self.permute_out) # (N, ..., C) -> (N, C, ...)\n",
    "        return x_prev + self.drop_path(x) # Residual\n",
    "\n",
    "class ConvNeXtBlock1d(ConvNeXtBlockNd):\n",
    "    def __init__(self, in_channels: int = required, layer_scale_init_value: float = 1e-6, drop_path: float = 0., **kwargs):\n",
    "        super(ConvNeXtBlock1d, self).__init__(in_channels, layer_scale_init_value, drop_path, dim=1, **kwargs)\n",
    "\n",
    "class ConvNeXtBlock2d(ConvNeXtBlockNd):\n",
    "    def __init__(self, in_channels: int = required, layer_scale_init_value: float = 1e-6, drop_path: float = 0., **kwargs):\n",
    "        super(ConvNeXtBlock2d, self).__init__(in_channels, layer_scale_init_value, drop_path, dim=2, **kwargs)\n",
    "\n",
    "class ConvNeXtBlock3d(ConvNeXtBlockNd):\n",
    "    def __init__(self, in_channels: int = required, layer_scale_init_value: float = 1e-6, drop_path: float = 0., **kwargs):\n",
    "        super(ConvNeXtBlock3d, self).__init__(in_channels, layer_scale_init_value, drop_path, dim=3, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXtBlock1d(\n",
       "  (depthwise_conv): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,), groups=32)\n",
       "  (normalization): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "  (pointwise_conv_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "  (activation): GELU()\n",
       "  (pointwise_conv_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (drop_path): Identity()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvNeXtBlock1d(32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import exp\n",
    "from torch import ones\n",
    "from torch import ones_like\n",
    "from torch import log\n",
    "from torch.nn import Module\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Identity\n",
    "from torch.nn import BatchNorm1d\n",
    "from torch.nn import BatchNorm2d\n",
    "from torch.nn import BatchNorm3d\n",
    "from torch.nn import Dropout2d\n",
    "from torch.nn import Dropout3d\n",
    "from torch.nn import Conv1d\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Conv3d\n",
    "from torch.nn import ConvTranspose1d\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import ConvTranspose3d\n",
    "from torch.nn import AdaptiveAvgPool1d\n",
    "from torch.nn import AdaptiveAvgPool2d\n",
    "from torch.nn import AdaptiveAvgPool3d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "from torch.nn import init\n",
    "\n",
    "from timm.models.layers import DropPath\n",
    "\n",
    "from torch.nn.functional import interpolate\n",
    "from sak.torch.nn.modules.utils import Concatenate\n",
    "from sak import class_selector\n",
    "from sak import from_dict\n",
    "from sak.__ops import required\n",
    "from sak.__ops import check_required\n",
    "from sak.torch.nn import DepthwiseConv1d,DepthwiseConv2d,DepthwiseConv3d,PointwiseConv1d,PointwiseConv2d,PointwiseConv3d\n",
    "\n",
    "class ConvNeXtBlockNd(Module):\n",
    "    \"\"\"Partially based on: \n",
    "    * https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/convnext.py\n",
    "    * https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/convnext.py\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = required, hidden_channels: int = None, gamma_init: float = 1e-6, \n",
    "                 drop_proba: float = 0., dim: int = required, **kwargs: dict):\n",
    "        super(ConvNeXtBlockNd, self).__init__()\n",
    "        # Check required inputs\n",
    "        check_required(self, {\"in_channels\":in_channels, \"dim\":dim})\n",
    "        hidden_channels = hidden_channels or 4*in_channels\n",
    "\n",
    "        # Establish default inputs\n",
    "        kwargs[\"groups\"] = in_channels\n",
    "        kwargs[\"kernel_size\"] = kwargs.get(\"kernel_size\",7)\n",
    "        kwargs[\"padding\"] = kwargs.get(\"padding\", (kwargs[\"kernel_size\"]-1)//2)\n",
    "        kwargs[\"initializer\"] = kwargs.get(\"initializer\", \"timm.models.layers.trunc_normal_\")\n",
    "        activation = kwargs.pop(\"activation\",{\"class\": \"torch.nn.GELU\", \"arguments\": {}})\n",
    "\n",
    "        # Declare operations\n",
    "        if   dim == 1: depth_op,norm_op,point_op = DepthwiseConv1d,LayerNorm1d,PointwiseConv1d\n",
    "        elif dim == 2: depth_op,norm_op,point_op = DepthwiseConv2d,LayerNorm2d,PointwiseConv2d\n",
    "        elif dim == 3: depth_op,norm_op,point_op = DepthwiseConv3d,LayerNorm3d,PointwiseConv3d\n",
    "        else: raise ValueError(\"Invalid number of dimensions: {}\".format(dim))\n",
    "        self.depthwise_conv   = depth_op(in_channels, **kwargs)\n",
    "        self.normalization    = norm_op(in_channels)\n",
    "        self.pointwise_conv_1 = point_op(in_channels, hidden_channels, **kwargs)\n",
    "        self.activation       = from_dict(activation)\n",
    "        self.pointwise_conv_2 = point_op(hidden_channels, in_channels, **kwargs)\n",
    "        self.gamma            = Parameter(gamma_init * ones((in_channels,)), requires_grad=True).reshape(1, -1, *[1]*dim) if gamma_init > 0 else None\n",
    "        self.drop_path        = DropPath(drop_proba) if drop_proba > 0. else lambda x: x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        residual = x\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.normalization(x)\n",
    "        x = self.pointwise_conv_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.pointwise_conv_2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        return self.drop_path(x) + residual\n",
    "\n",
    "class ConvNeXtBlock1d(ConvNeXtBlockNd):\n",
    "    def __init__(self, in_channels: int = required, hidden_channels: int = None, gamma_init: float = 1e-6, drop_path: float = 0., **kwargs):\n",
    "        super(ConvNeXtBlock1d, self).__init__(in_channels, hidden_channels, gamma_init, drop_path, dim=1, **kwargs)\n",
    "\n",
    "class ConvNeXtBlock2d(ConvNeXtBlockNd):\n",
    "    def __init__(self, in_channels: int = required, hidden_channels: int = None, gamma_init: float = 1e-6, drop_path: float = 0., **kwargs):\n",
    "        super(ConvNeXtBlock2d, self).__init__(in_channels, hidden_channels, gamma_init, drop_path, dim=2, **kwargs)\n",
    "\n",
    "class ConvNeXtBlock3d(ConvNeXtBlockNd):\n",
    "    def __init__(self, in_channels: int = required, hidden_channels: int = None, gamma_init: float = 1e-6, drop_path: float = 0., **kwargs):\n",
    "        super(ConvNeXtBlock3d, self).__init__(in_channels, hidden_channels, gamma_init, drop_path, dim=3, **kwargs)\n",
    "\n",
    "        \n",
    "class LayerNormNd(torch.nn.LayerNorm):\n",
    "    r\"\"\" LayerNorm for channels_first tensors with 2d spatial dimensions (ie N, C, H, W).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalized_shape: Union[int, List[int], torch.Size], dim: int = required, **kwargs):\n",
    "        kwargs[\"eps\"] = kwargs.get(\"eps\",1e-6) # Fix epsilon\n",
    "        super().__init__(normalized_shape, **kwargs)\n",
    "        if   dim == 1: self.permute_in,self.permute_out = [0,2,1],     [0,2,1]\n",
    "        elif dim == 2: self.permute_in,self.permute_out = [0,2,3,1],   [0,3,1,2]\n",
    "        elif dim == 3: self.permute_in,self.permute_out = [0,2,3,4,1], [0,4,1,2,3]\n",
    "        else: raise ValueError(\"Invalid number of dimensions: {}\".format(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.is_contiguous():\n",
    "            return F.layer_norm(\n",
    "                x.permute(*self.permute_in), self.normalized_shape, self.weight, self.bias, self.eps).permute(*self.permute_in)\n",
    "        else:\n",
    "            s, u = torch.var_mean(x, dim=1, unbiased=False, keepdim=True)\n",
    "            x = (x - u) * torch.rsqrt(s + self.eps)\n",
    "            x = x * self.weight[:, None, None] + self.bias[:, None, None]\n",
    "            return x\n",
    "        \n",
    "\n",
    "class LayerNorm1d(LayerNormNd):\n",
    "    def __init__(self, normalized_shape: Union[int, List[int], torch.Size], **kwargs):\n",
    "        super().__init__(normalized_shape, dim=1, **kwargs)\n",
    "\n",
    "class LayerNorm2d(LayerNormNd):\n",
    "    def __init__(self, normalized_shape: Union[int, List[int], torch.Size], **kwargs):\n",
    "        super().__init__(normalized_shape, dim=2, **kwargs)\n",
    "\n",
    "class LayerNorm3d(LayerNormNd):\n",
    "    def __init__(self, normalized_shape: Union[int, List[int], torch.Size], **kwargs):\n",
    "        super().__init__(normalized_shape, dim=3, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXtBlock1d(\n",
       "  (depthwise_conv): DepthwiseConv1d(\n",
       "    (depthwise_conv): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,), groups=32)\n",
       "  )\n",
       "  (normalization): LayerNorm1d((32,), eps=1e-06, elementwise_affine=True)\n",
       "  (pointwise_conv_1): PointwiseConv1d(\n",
       "    (pointwise_conv): Conv1d(32, 128, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (activation): GELU()\n",
       "  (pointwise_conv_2): PointwiseConv1d(\n",
       "    (pointwise_conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_channels = None\n",
    "in_channels = 32\n",
    "\n",
    "ConvNeXtBlock1d(32,initializer=\"timm.models.layers.trunc_normal_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "class LayerNormNd(torch.nn.LayerNorm):\n",
    "    r\"\"\" LayerNorm for channels_first tensors with 2d spatial dimensions (ie N, C, H, W).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalized_shape: Union[int, List[int], torch.Size], dim: int = required, **kwargs):\n",
    "        kwargs[\"eps\"] = kwargs.get(\"eps\",1e-6) # Fix epsilon\n",
    "        super().__init__(normalized_shape, **kwargs)\n",
    "        if   dim == 1: self.permute_in,self.permute_out = [0,2,1],     [0,2,1]\n",
    "        elif dim == 2: self.permute_in,self.permute_out = [0,2,3,1],   [0,3,1,2]\n",
    "        elif dim == 3: self.permute_in,self.permute_out = [0,2,3,4,1], [0,4,1,2,3]\n",
    "        else: raise ValueError(\"Invalid number of dimensions: {}\".format(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.is_contiguous():\n",
    "            return F.layer_norm(\n",
    "                x.permute(*self.permute_in), self.normalized_shape, self.weight, self.bias, self.eps).permute(*self.permute_in)\n",
    "        else:\n",
    "            s, u = torch.var_mean(x, dim=1, unbiased=False, keepdim=True)\n",
    "            x = (x - u) * torch.rsqrt(s + self.eps)\n",
    "            x = x * self.weight[:, None, None] + self.bias[:, None, None]\n",
    "            return x\n",
    "        \n",
    "\n",
    "class LayerNorm1d(LayerNormNd):\n",
    "    def __init__(self, normalized_shape: Union[int, List[int], torch.Size], **kwargs):\n",
    "        super().__init__(normalized_shape, dim=1, **kwargs)\n",
    "\n",
    "class LayerNorm2d(LayerNormNd):\n",
    "    def __init__(self, normalized_shape: Union[int, List[int], torch.Size], **kwargs):\n",
    "        super().__init__(normalized_shape, dim=2, **kwargs)\n",
    "\n",
    "class LayerNorm3d(LayerNormNd):\n",
    "    def __init__(self, normalized_shape: Union[int, List[int], torch.Size], **kwargs):\n",
    "        super().__init__(normalized_shape, dim=3, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = LayerNorm1d(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtBlockNd(torch.nn.Module):\n",
    "    r\"\"\"https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py\n",
    "    \n",
    "    ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(channels, channels, kernel_size=7, padding=3, groups=channels) # depthwise conv\n",
    "        self.norm = torch.nn.LayerNorm(channels, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(channels, 4 * channels) # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(4 * channels, channels)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((channels)), \n",
    "                                    requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = timm.models.layers.DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "            print(f\"x = self.gamma * x: \\n\\t{x.shape}\\n\")\n",
    "        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtBlockNd(torch.nn.Module):\n",
    "    r\"\"\"https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py\n",
    "    \n",
    "    ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(channels, channels, kernel_size=7, padding=3, groups=channels) # depthwise conv\n",
    "        self.norm = torch.nn.LayerNorm(channels, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(channels, 4 * channels) # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(4 * channels, channels)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((channels)), \n",
    "                                    requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = timm.models.layers.DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "            print(f\"x = self.gamma * x: \\n\\t{x.shape}\\n\")\n",
    "        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "class Block(nn.Module):\n",
    "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n",
    "        self.norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n",
    "                                    requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x\n",
    "\n",
    "class ConvNeXt(nn.Module):\n",
    "    r\"\"\" ConvNeXt\n",
    "        A PyTorch impl of : `A ConvNet for the 2020s`  -\n",
    "          https://arxiv.org/pdf/2201.03545.pdf\n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
    "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans=3, num_classes=1000, \n",
    "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., \n",
    "                 layer_scale_init_value=1e-6, head_init_scale=1.,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n",
    "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n",
    "                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.head.weight.data.mul_(head_init_scale)\n",
    "        self.head.bias.data.mul_(head_init_scale)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "        return self.norm(x.mean([-2, -1])) # global average pooling, (N, C, H, W) -> (N, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError \n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "            return x\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    \"convnext_tiny_1k\":    \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n",
    "    \"convnext_small_1k\":   \"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\",\n",
    "    \"convnext_base_1k\":    \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\",\n",
    "    \"convnext_large_1k\":   \"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\",\n",
    "    \"convnext_tiny_22k\":   \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth\",\n",
    "    \"convnext_small_22k\":  \"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\",\n",
    "    \"convnext_base_22k\":   \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\",\n",
    "    \"convnext_large_22k\":  \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\",\n",
    "    \"convnext_xlarge_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\",\n",
    "}\n",
    "\n",
    "@register_model\n",
    "def convnext_tiny(pretrained=False,in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)\n",
    "    if pretrained:\n",
    "        url = model_urls['convnext_tiny_22k'] if in_22k else model_urls['convnext_tiny_1k']\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def convnext_small(pretrained=False,in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], **kwargs)\n",
    "    if pretrained:\n",
    "        url = model_urls['convnext_small_22k'] if in_22k else model_urls['convnext_small_1k']\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def convnext_base(pretrained=False, in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs)\n",
    "    if pretrained:\n",
    "        url = model_urls['convnext_base_22k'] if in_22k else model_urls['convnext_base_1k']\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def convnext_large(pretrained=False, in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], **kwargs)\n",
    "    if pretrained:\n",
    "        url = model_urls['convnext_large_22k'] if in_22k else model_urls['convnext_large_1k']\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def convnext_xlarge(pretrained=False, in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048], **kwargs)\n",
    "    if pretrained:\n",
    "        assert in_22k, \"only ImageNet-22K pre-trained ConvNeXt-XL is available; please set in_22k=True\"\n",
    "        url = model_urls['convnext_xlarge_22k']\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (downsample_layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): LayerNorm()\n",
       "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LayerNorm()\n",
       "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): LayerNorm()\n",
       "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (stages): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Block(\n",
       "        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Block(\n",
       "        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Block(\n",
       "        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvNeXt()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
