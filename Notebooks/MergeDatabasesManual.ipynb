{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import random\n",
    "import uuid\n",
    "import os\n",
    "import os.path\n",
    "import skimage\n",
    "import utils\n",
    "import sak.wavelet\n",
    "import sak.data\n",
    "import sak.data.augmentation\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import networkx.algorithms.approximation\n",
    "import wfdb\n",
    "import json\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sak.signal import StandardHeader\n",
    "\n",
    "# Data loader to un-clutter code    \n",
    "def load_data(filepath):\n",
    "    dic = dict()\n",
    "    with open(filepath) as f:\n",
    "        text = list(f)\n",
    "    for line in text:\n",
    "        line = line.replace(' ','').replace('\\n','').replace(',,','')\n",
    "        if line[-1] == ',': line = line[:-1]\n",
    "        head = line.split(',')[0]\n",
    "        tail = line.split(',')[1:]\n",
    "        if tail == ['']:\n",
    "            tail = np.asarray([])\n",
    "        else:\n",
    "            tail = np.asarray(tail).astype(int)\n",
    "\n",
    "        dic[head] = tail\n",
    "    return dic\n",
    "\n",
    "\n",
    "def trailonset(sig,on):\n",
    "    on = on-sig[0]\n",
    "    off = on-sig[0]+sig[-1]\n",
    "    sig = sig+np.linspace(on,off,sig.size)\n",
    "    \n",
    "    return sig\n",
    "\n",
    "def getcorr(segments):\n",
    "    if len(segments) > 0:\n",
    "        length = 2*max([seg.size for seg in segments])\n",
    "    else:\n",
    "        return np.zeros((0,0))\n",
    "\n",
    "    corr = np.zeros((len(segments),len(segments)))\n",
    "\n",
    "    for i,seg1 in enumerate(segments):\n",
    "        for j,seg2 in enumerate(segments):\n",
    "            if i != j:\n",
    "                if seg1.size != seg2.size:\n",
    "                    if seg1.size != 1:\n",
    "                        x1 = sp.interpolate.interp1d(np.linspace(0,1,len(seg1)),seg1)(np.linspace(0,1,length))\n",
    "                    else:\n",
    "                        x1 = np.full((length,),seg1[0])\n",
    "                    if seg2.size != 1:\n",
    "                        x2 = sp.interpolate.interp1d(np.linspace(0,1,len(seg2)),seg2)(np.linspace(0,1,length))\n",
    "                    else:\n",
    "                        x2 = np.full((length,),seg2[0])\n",
    "                else:\n",
    "                    x1 = seg1\n",
    "                    x2 = seg2\n",
    "                if (x1.size == 1) and (x2.size == 1):\n",
    "                    corr[i,j] = 1\n",
    "                else:\n",
    "                    c,_ = sak.signal.xcorr(x1,x2)\n",
    "                    corr[i,j] = np.max(np.abs(c))\n",
    "            else:\n",
    "                corr[i,j] = 1\n",
    "                \n",
    "    return corr\n",
    "\n",
    "def order_fiducials(qrs, other, mode='smaller'):\n",
    "    # just in case\n",
    "    other = np.copy(other)\n",
    "\n",
    "    if mode=='smaller':\n",
    "        filt = (other[None,:] < qrs[:,None])\n",
    "    elif mode=='bigger':\n",
    "        filt = (qrs[:,None] < other[None,:])\n",
    "    else:\n",
    "        raise ValueError(\"Mode must be {smaller, bigger}\")\n",
    "        \n",
    "    if other.size == 0:\n",
    "        return np.full_like(qrs,-1,dtype=int)\n",
    "        \n",
    "    for i in range(qrs.size):\n",
    "        if i >= filt.shape[1]:\n",
    "            newcol = np.zeros((qrs.size,),dtype=bool)\n",
    "            if mode=='smaller':\n",
    "                newcol[i:] = True\n",
    "            elif mode=='bigger':\n",
    "                newcol[:i+1] = True\n",
    "            other = np.insert(other,i,-1)\n",
    "            filt = np.insert(filt,i,newcol,axis=1)\n",
    "        else:\n",
    "            if mode=='smaller':\n",
    "                j = np.nonzero(filt[:,i])[0][0]\n",
    "            elif mode=='bigger':\n",
    "                j = qrs.size-np.nonzero(np.flip(filt[:,i]))[0][0]-1\n",
    "\n",
    "            if i != j:\n",
    "                newcol = np.zeros((qrs.size,),dtype=bool)\n",
    "                if mode=='smaller':\n",
    "                    newcol[i:] = True\n",
    "                elif mode=='bigger':\n",
    "                    newcol[:i+1] = True\n",
    "                other = np.insert(other,i,-1)\n",
    "                filt = np.insert(filt,i,newcol,axis=1)\n",
    "\n",
    "    return other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set base directory\n",
    "if platform.system() in ['Linux', 'Linux2']:\n",
    "    basedir = '/media/guille/DADES/DADES/Delineator'\n",
    "else:\n",
    "    basedir = r'C:\\Users\\Emilio\\Documents\\DADES\\DADES\\Delineator'\n",
    "    \n",
    "# Output data structures\n",
    "Psignal = {}\n",
    "PQsignal = {}\n",
    "QRSsignal = {}\n",
    "STsignal = {}\n",
    "Tsignal = {}\n",
    "TPsignal = {}\n",
    "\n",
    "Pamplitudes = {}\n",
    "PQamplitudes = {}\n",
    "QRSamplitudes = {}\n",
    "STamplitudes = {}\n",
    "Tamplitudes = {}\n",
    "TPamplitudes = {}\n",
    "\n",
    "# Filter out samples\n",
    "threshold = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LUDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 63.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load LUDB's manual delineations\n",
    "P_LUDB   = sak.load_data(os.path.join(basedir,'ludb','P.csv'))\n",
    "QRS_LUDB = sak.load_data(os.path.join(basedir,'ludb','QRS.csv'))\n",
    "T_LUDB   = sak.load_data(os.path.join(basedir,'ludb','T.csv'))\n",
    "\n",
    "# Halve to accomodate for 250hz sampling rate\n",
    "P_LUDB   = {k: P_LUDB[k]//2   for k in P_LUDB}\n",
    "QRS_LUDB = {k: QRS_LUDB[k]//2 for k in QRS_LUDB}\n",
    "T_LUDB   = {k: T_LUDB[k]//2   for k in T_LUDB}\n",
    "\n",
    "# Retrieve onsets\n",
    "Pon_LUDB    = {k: P_LUDB[k][::2]    for k in P_LUDB}\n",
    "QRSon_LUDB  = {k: QRS_LUDB[k][::2]  for k in QRS_LUDB}\n",
    "Ton_LUDB    = {k: T_LUDB[k][::2]    for k in T_LUDB}\n",
    "\n",
    "# Retrieve offsets\n",
    "Poff_LUDB   = {k: P_LUDB[k][1::2]   for k in P_LUDB}\n",
    "QRSoff_LUDB = {k: QRS_LUDB[k][1::2] for k in QRS_LUDB}\n",
    "Toff_LUDB   = {k: T_LUDB[k][1::2]   for k in T_LUDB}\n",
    "\n",
    "# Metrics for normalization\n",
    "metric_intralead = np.max\n",
    "metric_amplitude = sak.signal.abs_max\n",
    "\n",
    "# Iterate over signals\n",
    "for id_file in tqdm.tqdm(range(1,201)):\n",
    "    if '{}###I'.format(id_file) not in  QRSon_LUDB: continue\n",
    "    # Load singal\n",
    "    (signal, header) = wfdb.rdsamp(os.path.join(basedir,'ludb','{}'.format(id_file)))\n",
    "    \n",
    "    # Obtain segmentation code\n",
    "    seg_code = str(id_file)+'###I'\n",
    "    \n",
    "    # Sanity check for downsampling\n",
    "    if header['fs'] != 500:\n",
    "        print(header['fs'])\n",
    "\n",
    "    # 0. Order leads as standard header\n",
    "    sig_name = sak.map_upper(header['sig_name'])\n",
    "    sort = sak.argsort_as(sig_name, StandardHeader)\n",
    "    signal = signal[:,sort]\n",
    "    \n",
    "    # 1. Downsample and filter\n",
    "    signal = sp.signal.decimate(signal,2,axis=0)\n",
    "    signal = sp.signal.filtfilt(*sp.signal.butter(4,   0.5/250., 'high'),signal.T).T\n",
    "    signal = sp.signal.filtfilt(*sp.signal.butter(4, 125.0/250.,  'low'),signal.T).T\n",
    "    \n",
    "    # 2. Get segmentation centers and locate w.r.t. QRS wave\n",
    "    # Reference, qrs wave\n",
    "    qrs_on  = QRSon_LUDB.get(seg_code,[]).astype(int)\n",
    "    qrs_center = (QRS_LUDB.get(seg_code,[])[::2] + QRS_LUDB.get(seg_code,[])[1::2])//2 # Reference\n",
    "    qrs_off = QRSoff_LUDB.get(seg_code,[]).astype(int)\n",
    "    # p wave\n",
    "    p_on    = order_fiducials(qrs_center,Pon_LUDB.get(seg_code,[]),mode='smaller').astype(int)\n",
    "    p_off   = order_fiducials(qrs_center,Poff_LUDB.get(seg_code,[]),mode='smaller').astype(int)\n",
    "    # t wave\n",
    "    t_on    = order_fiducials(qrs_center,Ton_LUDB.get(seg_code,[]),mode='bigger').astype(int)\n",
    "    t_off   = order_fiducials(qrs_center,Toff_LUDB.get(seg_code,[]),mode='bigger').astype(int)\n",
    "    \n",
    "    # 3. Inter-lead: Compute the QRS amplitude w.r.t. other QRS in signal\n",
    "    normalizing_factor = -np.inf\n",
    "    for i,lead in enumerate(StandardHeader):\n",
    "        for j in range(qrs_center.size):\n",
    "            qrs_segment = sak.signal.on_off_correction(signal[qrs_on[j]:qrs_off[j],i])\n",
    "            qrs_amplitude = metric_amplitude(qrs_segment)\n",
    "            if qrs_amplitude > normalizing_factor:\n",
    "                normalizing_factor = qrs_amplitude\n",
    "                \n",
    "    # 4. Per-lead: 1) on/off correct, 2) crop and 3) find relative amplitude\n",
    "    for i,lead in enumerate(StandardHeader):\n",
    "        for j in range(qrs_center.size):\n",
    "            # Unique code\n",
    "            segment_code = '{}_{}###{}'.format(id_file,lead,j)\n",
    "            \n",
    "            # Segments\n",
    "            p_segment   = signal[p_on[j]:p_off[j],i]\n",
    "            qrs_segment = signal[qrs_on[j]:qrs_off[j],i]\n",
    "            t_segment   = signal[t_on[j]:t_off[j],i]\n",
    "            if p_segment.size != 0:   p_segment   = sak.signal.on_off_correction(p_segment)\n",
    "            if qrs_segment.size != 0: qrs_segment = sak.signal.on_off_correction(qrs_segment)\n",
    "            if t_segment.size != 0:   t_segment   = sak.signal.on_off_correction(t_segment)\n",
    "            \n",
    "            pq_segment  = signal[p_off[j]:qrs_on[j],i] if p_off[j] != -1 else np.array([])\n",
    "            st_segment  = signal[qrs_off[j]:t_on[j],i] if (t_on[j] != -1) else np.array([])\n",
    "            if j in range(qrs_center.size-1):\n",
    "                if (p_on[j+1] != -1):\n",
    "                    if (t_off[j] != -1): tp_segment = signal[t_off[j]:p_on[j+1],i]\n",
    "                    else:                tp_segment = signal[qrs_off[j]:p_on[j+1],i]\n",
    "                elif (t_off[j] != -1):   tp_segment = signal[t_off[j]:qrs_on[j+1],i]\n",
    "                else:                    tp_segment = np.array([])\n",
    "            \n",
    "            # Amplitudes calculation\n",
    "            if p_segment.size != 0:   Pamplitudes[segment_code]   = metric_amplitude(p_segment)/metric_amplitude(qrs_segment)\n",
    "            if qrs_segment.size != 0: QRSamplitudes[segment_code] = metric_amplitude(qrs_segment)/normalizing_factor\n",
    "            if t_segment.size != 0:   Tamplitudes[segment_code]   = metric_amplitude(t_segment)/metric_amplitude(qrs_segment)\n",
    "            if pq_segment.size != 0:  PQamplitudes[segment_code]  = metric_amplitude(pq_segment)/metric_amplitude(qrs_segment)\n",
    "            if st_segment.size != 0:  STamplitudes[segment_code]  = metric_amplitude(st_segment)/metric_amplitude(qrs_segment)\n",
    "            if tp_segment.size != 0:  TPamplitudes[segment_code]  = metric_amplitude(tp_segment)/metric_amplitude(qrs_segment)\n",
    "            \n",
    "            # Normalize segments\n",
    "            if p_segment.size != 0:   p_segment = sak.data.ball_scaling(p_segment, metric=sak.signal.abs_max)\n",
    "            if qrs_segment.size != 0: qrs_segment = sak.data.ball_scaling(qrs_segment, metric=sak.signal.abs_max)\n",
    "            if t_segment.size != 0:   t_segment = sak.data.ball_scaling(t_segment, metric=sak.signal.abs_max)\n",
    "            if pq_segment.size != 0:  pq_segment = sak.data.ball_scaling(pq_segment, metric=sak.signal.abs_max)\n",
    "            if st_segment.size != 0:  st_segment = sak.data.ball_scaling(st_segment, metric=sak.signal.abs_max)\n",
    "            if tp_segment.size != 0:  tp_segment = sak.data.ball_scaling(tp_segment, metric=sak.signal.abs_max)\n",
    "            \n",
    "            # Store signals\n",
    "            if p_segment.size != 0:   Psignal[segment_code] = p_segment\n",
    "            if pq_segment.size != 0:  PQsignal[segment_code] = pq_segment\n",
    "            if qrs_segment.size != 0: QRSsignal[segment_code] = qrs_segment\n",
    "            if st_segment.size != 0:  STsignal[segment_code] = st_segment\n",
    "            if t_segment.size != 0:   Tsignal[segment_code] = t_segment\n",
    "            if tp_segment.size != 0:  TPsignal[segment_code] = tp_segment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter signals by length\n",
    "for k in list(TPsignal.keys()):\n",
    "    if isinstance(TPsignal[k],float):\n",
    "        TPamplitudes.pop(k)\n",
    "        TPsignal.pop(k)\n",
    "    elif len(TPsignal[k]) > 250:\n",
    "        TPamplitudes.pop(k)\n",
    "        TPsignal.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6336\n",
      "6324\n",
      "7332\n",
      "7284\n",
      "7284\n",
      "7260\n"
     ]
    }
   ],
   "source": [
    "print(len(Psignal))\n",
    "print(len(PQsignal))\n",
    "print(len(QRSsignal))\n",
    "print(len(STsignal))\n",
    "print(len(Tsignal))\n",
    "print(len(TPsignal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [04:15<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# Filter out redundant segments\n",
    "for id_file in tqdm.tqdm(range(1,201)):\n",
    "    # P signal\n",
    "    p_ids = [a for a in list(Psignal) if a.startswith(\"{}_\".format(id_file))]\n",
    "    p_segments = [Psignal[id] for id in p_ids]\n",
    "    corr = getcorr(p_segments)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    segments_exclude = [p for i,p in enumerate(p_ids) if i not in nodesclique]\n",
    "    for seg in segments_exclude:\n",
    "        Pamplitudes.pop(seg)\n",
    "        Psignal.pop(seg)\n",
    "        \n",
    "    # QRS signal\n",
    "    qrs_ids = [a for a in list(QRSsignal) if a.startswith(\"{}_\".format(id_file))]\n",
    "    qrs_segments = [QRSsignal[id] for id in qrs_ids]\n",
    "    corr = getcorr(qrs_segments)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    segments_exclude = [p for i,p in enumerate(qrs_ids) if i not in nodesclique]\n",
    "    for seg in segments_exclude:\n",
    "        QRSamplitudes.pop(seg)\n",
    "        QRSsignal.pop(seg)\n",
    "        \n",
    "    # T signal\n",
    "    t_ids = [a for a in list(Tsignal) if a.startswith(\"{}_\".format(id_file))]\n",
    "    t_segments = [Tsignal[id] for id in t_ids]\n",
    "    corr = getcorr(t_segments)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    segments_exclude = [p for i,p in enumerate(t_ids) if i not in nodesclique]\n",
    "    for seg in segments_exclude:\n",
    "        Tamplitudes.pop(seg)\n",
    "        Tsignal.pop(seg)\n",
    "        \n",
    "    # PQ signal\n",
    "    pq_ids = [a for a in list(PQsignal) if a.startswith(\"{}_\".format(id_file))]\n",
    "    pq_segments = [PQsignal[id] for id in pq_ids]\n",
    "    corr = getcorr(pq_segments)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    segments_exclude = [p for i,p in enumerate(pq_ids) if i not in nodesclique]\n",
    "    for seg in segments_exclude:\n",
    "        PQamplitudes.pop(seg)\n",
    "        PQsignal.pop(seg)\n",
    "        \n",
    "    # ST signal\n",
    "    st_ids = [a for a in list(STsignal) if a.startswith(\"{}_\".format(id_file))]\n",
    "    st_segments = [STsignal[id] for id in st_ids]\n",
    "    corr = getcorr(st_segments)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    segments_exclude = [p for i,p in enumerate(st_ids) if i not in nodesclique]\n",
    "    for seg in segments_exclude:\n",
    "        STamplitudes.pop(seg)\n",
    "        STsignal.pop(seg)\n",
    "        \n",
    "    # TP signal\n",
    "    tp_ids = [a for a in list(TPsignal) if a.startswith(\"{}_\".format(id_file))]\n",
    "    tp_segments = [TPsignal[id] for id in tp_ids]\n",
    "    corr = getcorr(tp_segments)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    segments_exclude = [p for i,p in enumerate(tp_ids) if i not in nodesclique]\n",
    "    for seg in segments_exclude:\n",
    "        TPamplitudes.pop(seg)\n",
    "        TPsignal.pop(seg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3923\n",
      "2541\n",
      "3276\n",
      "4230\n",
      "2494\n",
      "3860\n"
     ]
    }
   ],
   "source": [
    "print(len(Psignal))\n",
    "print(len(PQsignal))\n",
    "print(len(QRSsignal))\n",
    "print(len(STsignal))\n",
    "print(len(Tsignal))\n",
    "print(len(TPsignal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLD AMOUNTS OF SEGMENTS\n",
    "\n",
    "P:   13295\n",
    "PQ:  8875\n",
    "QRS: 10953\n",
    "ST:  13167\n",
    "T:   12267\n",
    "TP:  15949"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load QT db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:03<00:00, 30.67it/s]\n"
     ]
    }
   ],
   "source": [
    "#### LOAD DATASETS ####\n",
    "dataset             = pd.read_csv(os.path.join(basedir,'QTDB','Dataset.csv'), index_col=0)\n",
    "dataset             = dataset.sort_index(axis=1)\n",
    "labels              = np.asarray(list(dataset)) # In case no data augmentation is applied\n",
    "description         = dataset.describe()\n",
    "group               = {k: '_'.join(k.split('_')[:-1]) for k in dataset}\n",
    "unique_ids          = list(set([k.split('_')[0] for k in dataset]))\n",
    "\n",
    "# Load fiducials\n",
    "Pon_QTDB = load_data(os.path.join(basedir,'QTDB','PonNew.csv'))\n",
    "Poff_QTDB = load_data(os.path.join(basedir,'QTDB','PoffNew.csv'))\n",
    "QRSon_QTDB = load_data(os.path.join(basedir,'QTDB','QRSonNew.csv'))\n",
    "QRSoff_QTDB = load_data(os.path.join(basedir,'QTDB','QRSoffNew.csv'))\n",
    "Ton_QTDB = load_data(os.path.join(basedir,'QTDB','TonNew.csv'))\n",
    "Toff_QTDB = load_data(os.path.join(basedir,'QTDB','ToffNew.csv'))\n",
    "\n",
    "for i,id_file in enumerate(tqdm.tqdm(unique_ids)):\n",
    "    # Buggy files\n",
    "    if id_file in ['sel35','sel36','sel103','sel232','sel310']: continue\n",
    "\n",
    "    # Load singal\n",
    "    signal = np.vstack((dataset[id_file+'_0'],dataset[id_file+'_1'])).T\n",
    "    \n",
    "    # Obtain segmentation code\n",
    "    seg_code = '{}_0'.format(id_file)\n",
    "    \n",
    "    # 1. Downsample and filter\n",
    "    signal = sp.signal.filtfilt(*sp.signal.butter(4,   0.5/250., 'high'),signal.T).T\n",
    "    signal = sp.signal.filtfilt(*sp.signal.butter(4, 125.0/250.,  'low'),signal.T).T\n",
    "    \n",
    "    # 2. Get segmentation centers and locate w.r.t. QRS wave\n",
    "    # Reference, qrs wave\n",
    "    qrs_on  = QRSon_QTDB.get(seg_code,[]).astype(int)\n",
    "    qrs_off = QRSoff_QTDB.get(seg_code,[]).astype(int)\n",
    "    qrs_center = (qrs_on+qrs_off)//2\n",
    "    # p wave\n",
    "    p_on    = order_fiducials(qrs_center,Pon_QTDB.get(seg_code,[]),mode='smaller').astype(int)\n",
    "    p_off   = order_fiducials(qrs_center,Poff_QTDB.get(seg_code,[]),mode='smaller').astype(int)\n",
    "    # t wave\n",
    "    t_on    = order_fiducials(qrs_center,Ton_QTDB.get(seg_code,[]),mode='bigger').astype(int)\n",
    "    t_off   = order_fiducials(qrs_center,Toff_QTDB.get(seg_code,[]),mode='bigger').astype(int)\n",
    "    \n",
    "    # 3. Inter-lead: Compute the QRS amplitude w.r.t. other QRS in signal\n",
    "    normalizing_factor = -np.inf\n",
    "    for i in range(2):\n",
    "        for j in range(qrs_center.size):\n",
    "            qrs_segment = sak.signal.on_off_correction(signal[qrs_on[j]:qrs_off[j],i])\n",
    "            qrs_amplitude = metric_amplitude(qrs_segment)\n",
    "            if qrs_amplitude > normalizing_factor:\n",
    "                normalizing_factor = qrs_amplitude\n",
    "                \n",
    "    # 4. Per-lead: 1) on/off correct, 2) crop and 3) find relative amplitude\n",
    "    for i in range(2):\n",
    "        for j in range(qrs_center.size):\n",
    "            # Unique code\n",
    "            segment_code = '{}_{}###{}'.format(id_file,i,j)\n",
    "            \n",
    "            # Segments\n",
    "            p_segment   = signal[p_on[j]:p_off[j],i]\n",
    "            qrs_segment = signal[qrs_on[j]:qrs_off[j],i]\n",
    "            t_segment   = signal[t_on[j]:t_off[j],i]\n",
    "            if p_segment.size != 0:   p_segment   = sak.signal.on_off_correction(p_segment)\n",
    "            if qrs_segment.size != 0: qrs_segment = sak.signal.on_off_correction(qrs_segment)\n",
    "            if t_segment.size != 0:   t_segment   = sak.signal.on_off_correction(t_segment)\n",
    "                \n",
    "            if qrs_segment.size < 10: aghkjsghkj\n",
    "            \n",
    "            pq_segment  = signal[p_off[j]:qrs_on[j],i] if p_off[j] != -1 else np.array([])\n",
    "            st_segment  = signal[qrs_off[j]:t_on[j],i] if (t_on[j] != -1) else np.array([])\n",
    "            if j in range(qrs_center.size-1):\n",
    "                if (p_on[j+1] != -1):\n",
    "                    if (t_off[j] != -1): tp_segment = signal[t_off[j]:p_on[j+1],i]\n",
    "                    else:                tp_segment = signal[qrs_off[j]:p_on[j+1],i]\n",
    "                elif (t_off[j] != -1):   tp_segment = signal[t_off[j]:qrs_on[j+1],i]\n",
    "                else:                    tp_segment = np.array([])\n",
    "            \n",
    "            # Amplitudes calculation\n",
    "            if p_segment.size != 0:   Pamplitudes[segment_code]   = metric_amplitude(p_segment)/metric_amplitude(qrs_segment)\n",
    "            if qrs_segment.size != 0: QRSamplitudes[segment_code] = metric_amplitude(qrs_segment)/normalizing_factor\n",
    "            if t_segment.size != 0:   Tamplitudes[segment_code]   = metric_amplitude(t_segment)/metric_amplitude(qrs_segment)\n",
    "            if pq_segment.size != 0:  PQamplitudes[segment_code]  = metric_amplitude(pq_segment)/metric_amplitude(qrs_segment)\n",
    "            if st_segment.size != 0:  STamplitudes[segment_code]  = metric_amplitude(st_segment)/metric_amplitude(qrs_segment)\n",
    "            if tp_segment.size != 0:  TPamplitudes[segment_code]  = metric_amplitude(tp_segment)/metric_amplitude(qrs_segment)\n",
    "            \n",
    "            # Normalize segments\n",
    "            if p_segment.size != 0:   p_segment = sak.data.ball_scaling(p_segment, metric=sak.signal.abs_max)\n",
    "            if qrs_segment.size != 0: qrs_segment = sak.data.ball_scaling(qrs_segment, metric=sak.signal.abs_max)\n",
    "            if t_segment.size != 0:   t_segment = sak.data.ball_scaling(t_segment, metric=sak.signal.abs_max)\n",
    "            if pq_segment.size != 0:  pq_segment = sak.data.ball_scaling(pq_segment, metric=sak.signal.abs_max)\n",
    "            if st_segment.size != 0:  st_segment = sak.data.ball_scaling(st_segment, metric=sak.signal.abs_max)\n",
    "            if tp_segment.size != 0:  tp_segment = sak.data.ball_scaling(tp_segment, metric=sak.signal.abs_max)\n",
    "            \n",
    "            # Store signals\n",
    "            if p_segment.size != 0:   Psignal[segment_code] = p_segment\n",
    "            if pq_segment.size != 0:  PQsignal[segment_code] = pq_segment\n",
    "            if qrs_segment.size != 0: QRSsignal[segment_code] = qrs_segment\n",
    "            if st_segment.size != 0:  STsignal[segment_code] = st_segment\n",
    "            if t_segment.size != 0:   Tsignal[segment_code] = t_segment\n",
    "            if tp_segment.size != 0:  TPsignal[segment_code] = tp_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter signals by length\n",
    "for k in list(TPsignal.keys()):\n",
    "    if isinstance(TPsignal[k],float):\n",
    "        TPamplitudes.pop(k)\n",
    "        TPsignal.pop(k)\n",
    "    elif len(TPsignal[k]) > 250:\n",
    "        TPamplitudes.pop(k)\n",
    "        TPsignal.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9701\n",
      "8313\n",
      "9692\n",
      "10226\n",
      "8642\n",
      "10152\n"
     ]
    }
   ],
   "source": [
    "print(len(Psignal))\n",
    "print(len(PQsignal))\n",
    "print(len(QRSsignal))\n",
    "print(len(STsignal))\n",
    "print(len(Tsignal))\n",
    "print(len(TPsignal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [10:04<00:00,  5.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# Filter out redundant segments\n",
    "for i,id_file in enumerate(tqdm.tqdm(unique_ids)):\n",
    "    # P signal\n",
    "    p_ids = [a for a in list(Psignal) if a.startswith(\"{}_\".format(id_file))]\n",
    "    p_segments = [Psignal[id] for id in p_ids]\n",
    "    corr = getcorr(p_segments)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    segments_exclude = [p for i,p in enumerate(p_ids) if i not in nodesclique]\n",
    "    for seg in segments_exclude:\n",
    "        Pamplitudes.pop(seg)\n",
    "        Psignal.pop(seg)\n",
    "        \n",
    "    # QRS signal\n",
    "    qrs_ids = [a for a in list(QRSsignal) if a.startswith(\"{}_\".format(id_file))]\n",
    "    qrs_segments = [QRSsignal[id] for id in qrs_ids]\n",
    "    corr = getcorr(qrs_segments)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    segments_exclude = [p for i,p in enumerate(qrs_ids) if i not in nodesclique]\n",
    "    for seg in segments_exclude:\n",
    "        QRSamplitudes.pop(seg)\n",
    "        QRSsignal.pop(seg)\n",
    "        \n",
    "    # T signal\n",
    "    t_ids = [a for a in list(Tsignal) if a.startswith(\"{}_\".format(id_file))]\n",
    "    t_segments = [Tsignal[id] for id in t_ids]\n",
    "    corr = getcorr(t_segments)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    segments_exclude = [p for i,p in enumerate(t_ids) if i not in nodesclique]\n",
    "    for seg in segments_exclude:\n",
    "        Tamplitudes.pop(seg)\n",
    "        Tsignal.pop(seg)\n",
    "        \n",
    "    # PQ signal\n",
    "    pq_ids = [a for a in list(PQsignal) if a.startswith(\"{}_\".format(id_file))]\n",
    "    pq_segments = [PQsignal[id] for id in pq_ids]\n",
    "    corr = getcorr(pq_segments)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    segments_exclude = [p for i,p in enumerate(pq_ids) if i not in nodesclique]\n",
    "    for seg in segments_exclude:\n",
    "        PQamplitudes.pop(seg)\n",
    "        PQsignal.pop(seg)\n",
    "        \n",
    "    # ST signal\n",
    "    st_ids = [a for a in list(STsignal) if a.startswith(\"{}_\".format(id_file))]\n",
    "    st_segments = [STsignal[id] for id in st_ids]\n",
    "    corr = getcorr(st_segments)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    segments_exclude = [p for i,p in enumerate(st_ids) if i not in nodesclique]\n",
    "    for seg in segments_exclude:\n",
    "        STamplitudes.pop(seg)\n",
    "        STsignal.pop(seg)\n",
    "        \n",
    "    # TP signal\n",
    "    tp_ids = [a for a in list(TPsignal) if a.startswith(\"{}_\".format(id_file))]\n",
    "    tp_segments = [TPsignal[id] for id in tp_ids]\n",
    "    corr = getcorr(tp_segments)\n",
    "    g = networkx.convert_matrix.from_numpy_matrix(corr < threshold)\n",
    "    nodesclique = networkx.algorithms.approximation.max_clique(g)\n",
    "    segments_exclude = [p for i,p in enumerate(tp_ids) if i not in nodesclique]\n",
    "    for seg in segments_exclude:\n",
    "        TPamplitudes.pop(seg)\n",
    "        TPsignal.pop(seg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6885\n",
      "4770\n",
      "5662\n",
      "6592\n",
      "4009\n",
      "8105\n"
     ]
    }
   ],
   "source": [
    "print(len(Psignal))\n",
    "print(len(PQsignal))\n",
    "print(len(QRSsignal))\n",
    "print(len(STsignal))\n",
    "print(len(Tsignal))\n",
    "print(len(TPsignal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:15<00:00, 18.79it/s]\n"
     ]
    }
   ],
   "source": [
    "Files = os.listdir(os.path.join(basedir,'SoO','RETAG'))\n",
    "Files = [os.path.splitext(f)[0] for f in Files if os.path.splitext(f)[1] == '.txt']\n",
    "Segmentations = pd.read_csv(os.path.join(basedir,'SoO','SEGMENTATIONS.csv'),index_col=0,header=None).T\n",
    "Keys = Segmentations.keys().tolist()\n",
    "Keys = [k for k in Keys if '-'.join(k.split('-')[:2]) in Files]\n",
    "database = pd.read_csv(os.path.join(basedir,'SoO','DATABASE_MANUAL.csv'))\n",
    "\n",
    "# Data storage\n",
    "QRSsignalSoO = dict()\n",
    "QRSgroupSoO = dict()\n",
    "\n",
    "for k in tqdm.tqdm(Keys):\n",
    "    # Retrieve general information\n",
    "    fname = '-'.join(k.split('-')[:2]) + '.txt'\n",
    "    ID = int(k.split('-')[0])\n",
    "    \n",
    "    # Read signal and segmentation\n",
    "    Signal = pd.read_csv(os.path.join(basedir,'SoO','RETAG',fname),index_col=0).values\n",
    "    (son,soff) = Segmentations[k]\n",
    "    fs = database['Sampling_Freq'][database['ID'] == int(ID)].values[0]\n",
    "    \n",
    "    # Check correct segmentation\n",
    "    if son > soff:\n",
    "        print(\"(!!!) Check file   {:>10s} has onset ({:d}) > offset ({:d})\".format(k, son, soff))\n",
    "        continue\n",
    "\n",
    "    # Up/downsample to 1000 Hz\n",
    "    factor = int(fs/250)\n",
    "    Signal = np.round(sp.signal.decimate(Signal.T, factor)).T\n",
    "    fs = fs/factor\n",
    "    son = int(son/factor)\n",
    "    soff = int(soff/factor)\n",
    "    \n",
    "    # Filter baseline wander and high freq. noise\n",
    "    Signal = sp.signal.filtfilt(*sp.signal.butter(4,   0.5/fs, 'high'),Signal.T).T\n",
    "    Signal = sp.signal.filtfilt(*sp.signal.butter(4, 125.0/fs,  'low'),Signal.T).T\n",
    "    \n",
    "    # 3. Inter-lead: Compute the QRS amplitude w.r.t. other QRS in signal\n",
    "    normalizing_factor = -np.inf\n",
    "    for i,lead in enumerate(StandardHeader):\n",
    "        qrs_segment = sak.signal.on_off_correction(Signal[son:soff,i])\n",
    "        qrs_amplitude = metric_amplitude(qrs_segment)\n",
    "        if qrs_amplitude > normalizing_factor:\n",
    "            normalizing_factor = qrs_amplitude\n",
    "\n",
    "    for i,lead in enumerate(StandardHeader):\n",
    "        qrs_segment = sak.signal.on_off_correction(Signal[son:soff,i])\n",
    "        QRSamplitudes['SOO{}_{}###{}'.format(k,lead,0)] = metric_amplitude(qrs_segment)/normalizing_factor\n",
    "        qrs_segment = sak.data.ball_scaling(qrs_segment, metric=sak.signal.abs_max)\n",
    "        QRSsignal['SOO{}_{}###{}'.format(k,lead,0)] = qrs_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6885\n",
      "4770\n",
      "7846\n",
      "6592\n",
      "4009\n",
      "8105\n"
     ]
    }
   ],
   "source": [
    "print(len(Psignal))\n",
    "print(len(PQsignal))\n",
    "print(len(QRSsignal))\n",
    "print(len(STsignal))\n",
    "print(len(Tsignal))\n",
    "print(len(TPsignal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete too short or too long signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal lengths\n",
    "Plength = {k: len(Psignal[k]) for k in Psignal.keys() if not isinstance(Psignal[k],float)}\n",
    "PQlength = {k: len(PQsignal[k]) for k in PQsignal.keys() if not isinstance(PQsignal[k],float)}\n",
    "QRSlength = {k: len(QRSsignal[k]) for k in QRSsignal.keys() if not isinstance(QRSsignal[k],float)}\n",
    "STlength = {k: len(STsignal[k]) for k in STsignal.keys() if not isinstance(STsignal[k],float)}\n",
    "Tlength = {k: len(Tsignal[k]) for k in Tsignal.keys() if not isinstance(Tsignal[k],float)}\n",
    "TPlength = {k: len(TPsignal[k]) for k in TPsignal.keys() if not isinstance(TPsignal[k],float)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter signals by length\n",
    "for k in list(Psignal.keys()):\n",
    "    if isinstance(Psignal[k],float):                                Psignal.pop(k); Pamplitudes.pop(k)\n",
    "    elif not ((len(Psignal[k]) > 2) and (len(Psignal[k]) < 45)):    Psignal.pop(k); Pamplitudes.pop(k)\n",
    "for k in list(PQsignal.keys()):\n",
    "    if isinstance(PQsignal[k],float):                               PQsignal.pop(k); PQamplitudes.pop(k)\n",
    "    elif not ((len(PQsignal[k]) > 1) and (len(PQsignal[k]) < 35)):  PQsignal.pop(k); PQamplitudes.pop(k)\n",
    "for k in list(QRSsignal.keys()):\n",
    "    if isinstance(QRSsignal[k],float):                              QRSsignal.pop(k); QRSamplitudes.pop(k)\n",
    "    elif not ((len(QRSsignal[k]) > 10)):                            QRSsignal.pop(k); QRSamplitudes.pop(k)\n",
    "for k in list(STsignal.keys()):\n",
    "    if isinstance(STsignal[k],float):                               STsignal.pop(k); STamplitudes.pop(k)\n",
    "    elif not ((len(STsignal[k]) > 1) and (len(STsignal[k]) < 65)):  STsignal.pop(k); STamplitudes.pop(k)\n",
    "for k in list(Tsignal.keys()):\n",
    "    if isinstance(Tsignal[k],float):                                Tsignal.pop(k); Tamplitudes.pop(k)\n",
    "    elif not ((len(Tsignal[k]) > 10) and (len(Tsignal[k]) < 100)):  Tsignal.pop(k); Tamplitudes.pop(k)\n",
    "for k in list(TPsignal.keys()):\n",
    "    if isinstance(TPsignal[k],float):                               TPsignal.pop(k); TPamplitudes.pop(k)\n",
    "    elif not ((len(TPsignal[k]) > 2) and (len(TPsignal[k]) < 250)): TPsignal.pop(k); TPamplitudes.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Psignal))\n",
    "print(len(PQsignal))\n",
    "print(len(QRSsignal))\n",
    "print(len(STsignal))\n",
    "print(len(Tsignal))\n",
    "print(len(TPsignal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sak.pickledump(Psignal,os.path.join('.','pickle','Psignal_new.pkl'))\n",
    "sak.pickledump(Pamplitudes,os.path.join('.','pickle','Pamplitudes_new.pkl'))\n",
    "sak.pickledump(PQsignal,os.path.join('.','pickle','PQsignal_new.pkl'))\n",
    "sak.pickledump(PQamplitudes,os.path.join('.','pickle','PQamplitudes_new.pkl'))\n",
    "sak.pickledump(QRSsignal,os.path.join('.','pickle','QRSsignal_new.pkl'))\n",
    "sak.pickledump(QRSamplitudes,os.path.join('.','pickle','QRSamplitudes_new.pkl'))\n",
    "sak.pickledump(STsignal,os.path.join('.','pickle','STsignal_new.pkl'))\n",
    "sak.pickledump(STamplitudes,os.path.join('.','pickle','STamplitudes_new.pkl'))\n",
    "sak.pickledump(Tsignal,os.path.join('.','pickle','Tsignal_new.pkl'))\n",
    "sak.pickledump(Tamplitudes,os.path.join('.','pickle','Tamplitudes_new.pkl'))\n",
    "sak.pickledump(TPsignal,os.path.join('.','pickle','TPsignal_new.pkl'))\n",
    "sak.pickledump(TPamplitudes,os.path.join('.','pickle','TPamplitudes_new.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getcorr_old(segments):\n",
    "#     if len(segments) > 0:\n",
    "#         length = 2*max([segments[i][2].size for i in range(len(segments))])\n",
    "#     else:\n",
    "#         return np.zeros((0,0))\n",
    "# \n",
    "#     corr = np.zeros((len(segments),len(segments)))\n",
    "# \n",
    "#     for i in range(len(segments)):\n",
    "#         for j in range(len(segments)):\n",
    "#             if i != j:\n",
    "#                 if segments[i][2].size != segments[j][2].size:\n",
    "#                     if segments[i][2].size != 1:\n",
    "#                         x1 = sp.interpolate.interp1d(np.linspace(0,1,len(segments[i][2])),segments[i][2])(np.linspace(0,1,length))\n",
    "#                     else:\n",
    "#                         x1 = np.full((length,),segments[i][2][0])\n",
    "#                     if segments[j][2].size != 1:\n",
    "#                         x2 = sp.interpolate.interp1d(np.linspace(0,1,len(segments[j][2])),segments[j][2])(np.linspace(0,1,length))\n",
    "#                     else:\n",
    "#                         x2 = np.full((length,),segments[j][2][0])\n",
    "#                 else:\n",
    "#                     x1 = segments[i][2]\n",
    "#                     x2 = segments[j][2]\n",
    "#                 if (x1.size == 1) and (x2.size == 1):\n",
    "#                     corr[i,j] = 1\n",
    "#                 else:\n",
    "#                     c,_ = sak.signal.xcorr(x1,x2)\n",
    "#                     corr[i,j] = np.max(np.abs(c))\n",
    "#             else:\n",
    "#                 corr[i,j] = 1\n",
    "# \n",
    "#     return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #### LOAD DATASETS ####\n",
    "# # dataset             = pd.read_csv(os.path.join(basedir,'QTDB','Dataset.csv'), index_col=0)\n",
    "# # dataset             = dataset.sort_index(axis=1)\n",
    "# # labels              = np.asarray(list(dataset)) # In case no data augmentation is applied\n",
    "# # description         = dataset.describe()\n",
    "# # group               = {k: '_'.join(k.split('_')[:-1]) for k in dataset}\n",
    "# # unique_ids          = list(set([k.split('_')[0] for k in dataset]))\n",
    "\n",
    "# # Load fiducials\n",
    "# Pon_QTDB = load_data(os.path.join(basedir,'QTDB','PonNew.csv'))\n",
    "# Poff_QTDB = load_data(os.path.join(basedir,'QTDB','PoffNew.csv'))\n",
    "# QRSon_QTDB = load_data(os.path.join(basedir,'QTDB','QRSonNew.csv'))\n",
    "# QRSoff_QTDB = load_data(os.path.join(basedir,'QTDB','QRSoffNew.csv'))\n",
    "# Ton_QTDB = load_data(os.path.join(basedir,'QTDB','TonNew.csv'))\n",
    "# Toff_QTDB = load_data(os.path.join(basedir,'QTDB','ToffNew.csv'))\n",
    "\n",
    "# path_SVG = '/home/guille/Escritorio/QTDB/'\n",
    "# f,ax = plt.subplots(2,1,figsize=(40,6))\n",
    "# for id_file in unique_ids:\n",
    "#     if id_file in ['sel35','sel36','sel232']: continue\n",
    "#     if id_file != 'sele0406': continue\n",
    "#     w = 100\n",
    "#     id0 = id_file + '_0'\n",
    "#     id1 = id_file + '_1'\n",
    "#     ax[0].plot(dataset[id0][QRSon_QTDB[id0][0]-w:QRSoff_QTDB[id0][-1]+w])\n",
    "#     ax[0].set_xlim([QRSon_QTDB[id0][0]-w,QRSoff_QTDB[id0][-1]+w])\n",
    "#     if id0 in Pon_QTDB:\n",
    "#         [ax[0].axvspan(Pon_QTDB[id0][j],Poff_QTDB[id0][j], color='r', alpha=0.15) for j in range(len(Pon_QTDB[id0]))]\n",
    "#     [ax[0].axvspan(QRSon_QTDB[id0][j],QRSoff_QTDB[id0][j], color='g', alpha=0.15) for j in range(len(QRSon_QTDB[id0]))]\n",
    "#     [ax[0].axvspan(Ton_QTDB[id0][j],Toff_QTDB[id0][j], color='m', alpha=0.15) for j in range(len(Ton_QTDB[id0]))]\n",
    "#     ax[1].plot(dataset[id1][QRSon_QTDB[id1][0]-w:QRSoff_QTDB[id1][-1]+w])\n",
    "#     ax[1].set_xlim([QRSon_QTDB[id1][0]-w,QRSoff_QTDB[id1][-1]+w])\n",
    "#     if id1 in Pon_QTDB:\n",
    "#         [ax[1].axvspan(Pon_QTDB[id1][j],Poff_QTDB[id1][j], color='r', alpha=0.15) for j in range(len(Pon_QTDB[id1]))]\n",
    "#     [ax[1].axvspan(QRSon_QTDB[id1][j],QRSoff_QTDB[id1][j], color='g', alpha=0.15) for j in range(len(QRSon_QTDB[id1]))]\n",
    "#     [ax[1].axvspan(Ton_QTDB[id1][j],Toff_QTDB[id1][j], color='m', alpha=0.15) for j in range(len(Ton_QTDB[id1]))]\n",
    "#     f.tight_layout()\n",
    "#     f.subplots_adjust(hspace=0.00,wspace=0.05)\n",
    "#     f.savefig(os.path.join(path_SVG,id_file+'.svg'))\n",
    "#     [ax[i].clear() for i in range(2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(seg_code)\n",
    "# seg_code = seg_code.split('_')[0] + '_1'\n",
    "# print(j)\n",
    "# print(QRSon_QTDB[seg_code][j])\n",
    "# print(QRSoff_QTDB[seg_code][j])\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(dataset[seg_code][QRSon_QTDB[seg_code][j]-100:QRSon_QTDB[seg_code][j]+100])\n",
    "# plt.gca().axvspan(xmin=QRSon_QTDB[seg_code][j],xmax=QRSoff_QTDB[seg_code][j],alpha=0.15)\n",
    "# seg_code = seg_code.split('_')[0] + '_1'\n",
    "# plt.figure()\n",
    "# plt.plot(dataset[seg_code][QRSon_QTDB[seg_code][j]-100:QRSon_QTDB[seg_code][j]+100])\n",
    "# plt.gca().axvspan(xmin=QRSon_QTDB[seg_code][j],xmax=QRSoff_QTDB[seg_code][j],alpha=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pon_QTDB = load_data(os.path.join(basedir,'QTDB','PonNew.csv'))\n",
    "# Poff_QTDB = load_data(os.path.join(basedir,'QTDB','PoffNew.csv'))\n",
    "# QRSon_QTDB = load_data(os.path.join(basedir,'QTDB','QRSonNew.csv'))\n",
    "# QRSoff_QTDB = load_data(os.path.join(basedir,'QTDB','QRSoffNew.csv'))\n",
    "# Ton_QTDB = load_data(os.path.join(basedir,'QTDB','TonNew.csv'))\n",
    "# Toff_QTDB = load_data(os.path.join(basedir,'QTDB','ToffNew.csv'))\n",
    "\n",
    "# seg_code = 'sele0124_1'\n",
    "# j = 48\n",
    "# print(QRSon_QTDB[seg_code][j])\n",
    "# print(QRSoff_QTDB[seg_code][j])\n",
    "# print(\"\")\n",
    "\n",
    "\n",
    "# if 0:\n",
    "#     mod_on = -7\n",
    "#     mod_off = 9\n",
    "\n",
    "#     QRSon_QTDB[seg_code.split('_')[0] + '_0'][j]  += mod_on\n",
    "#     QRSoff_QTDB[seg_code.split('_')[0] + '_0'][j] += mod_off\n",
    "#     QRSon_QTDB[seg_code.split('_')[0] + '_1'][j]  += mod_on\n",
    "#     QRSoff_QTDB[seg_code.split('_')[0] + '_1'][j] += mod_off\n",
    "\n",
    "#     print(QRSon_QTDB[seg_code][j])\n",
    "#     print(QRSoff_QTDB[seg_code][j])\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(dataset[seg_code][QRSon_QTDB[seg_code][j]-100:QRSon_QTDB[seg_code][j]+100])\n",
    "# plt.gca().axvspan(xmin=QRSon_QTDB[seg_code][j],xmax=QRSoff_QTDB[seg_code][j],alpha=0.15)\n",
    "# seg_code = seg_code.split('_')[0] + '_1'\n",
    "# plt.figure()\n",
    "# plt.plot(dataset[seg_code][QRSon_QTDB[seg_code][j]-100:QRSon_QTDB[seg_code][j]+100])\n",
    "# plt.gca().axvspan(xmin=QRSon_QTDB[seg_code][j],xmax=QRSoff_QTDB[seg_code][j],alpha=0.15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning3",
   "language": "python",
   "name": "deeplearning3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
