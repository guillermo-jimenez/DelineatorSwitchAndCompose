{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import random\n",
    "import uuid\n",
    "import math\n",
    "import os\n",
    "import os.path\n",
    "import skimage\n",
    "import sak\n",
    "import sak.wavelet\n",
    "import sak.data\n",
    "import sak.data.augmentation\n",
    "import sak.data.preprocessing\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import networkx.algorithms.approximation\n",
    "import wfdb\n",
    "import json\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sak.signal import StandardHeader\n",
    "\n",
    "# Data loader to un-clutter code    \n",
    "def load_data(filepath):\n",
    "    dic = dict()\n",
    "    with open(filepath) as f:\n",
    "        text = list(f)\n",
    "    for line in text:\n",
    "        line = line.replace(' ','').replace('\\n','').replace(',,','')\n",
    "        if line[-1] == ',': line = line[:-1]\n",
    "        head = line.split(',')[0]\n",
    "        tail = line.split(',')[1:]\n",
    "        if tail == ['']:\n",
    "            tail = np.asarray([])\n",
    "        else:\n",
    "            tail = np.asarray(tail).astype(int)\n",
    "\n",
    "        dic[head] = tail\n",
    "    return dic\n",
    "\n",
    "\n",
    "def trailonset(sig,on):\n",
    "    on = on-sig[0]\n",
    "    off = on-sig[0]+sig[-1]\n",
    "    sig = sig+np.linspace(on,off,sig.size)\n",
    "    \n",
    "    return sig\n",
    "\n",
    "def getcorr(segments):\n",
    "    if len(segments) > 0:\n",
    "        length = 2*max([seg.size for seg in segments])\n",
    "    else:\n",
    "        return np.zeros((0,0))\n",
    "\n",
    "    corr = np.zeros((len(segments),len(segments)))\n",
    "\n",
    "    for i,seg1 in enumerate(segments):\n",
    "        for j,seg2 in enumerate(segments):\n",
    "            if i != j:\n",
    "                if seg1.size != seg2.size:\n",
    "                    if seg1.size != 1:\n",
    "                        x1 = sp.interpolate.interp1d(np.linspace(0,1,len(seg1)),seg1)(np.linspace(0,1,length))\n",
    "                    else:\n",
    "                        x1 = np.full((length,),seg1[0])\n",
    "                    if seg2.size != 1:\n",
    "                        x2 = sp.interpolate.interp1d(np.linspace(0,1,len(seg2)),seg2)(np.linspace(0,1,length))\n",
    "                    else:\n",
    "                        x2 = np.full((length,),seg2[0])\n",
    "                else:\n",
    "                    x1 = seg1\n",
    "                    x2 = seg2\n",
    "                if (x1.size == 1) and (x2.size == 1):\n",
    "                    corr[i,j] = 1\n",
    "                else:\n",
    "                    c,_ = sak.signal.xcorr(x1,x2)\n",
    "                    corr[i,j] = np.max(np.abs(c))\n",
    "            else:\n",
    "                corr[i,j] = 1\n",
    "                \n",
    "    return corr\n",
    "\n",
    "def order_fiducials(qrs, other, mode='smaller'):\n",
    "    # just in case\n",
    "    other = np.copy(other)\n",
    "\n",
    "    if mode=='smaller':\n",
    "        filt = (other[None,:] < qrs[:,None])\n",
    "    elif mode=='bigger':\n",
    "        filt = (qrs[:,None] < other[None,:])\n",
    "    else:\n",
    "        raise ValueError(\"Mode must be {smaller, bigger}\")\n",
    "        \n",
    "    if other.size == 0:\n",
    "        return np.full_like(qrs,-1,dtype=int)\n",
    "        \n",
    "    for i in range(qrs.size):\n",
    "        if i >= filt.shape[1]:\n",
    "            newcol = np.zeros((qrs.size,),dtype=bool)\n",
    "            if mode=='smaller':\n",
    "                newcol[i:] = True\n",
    "            elif mode=='bigger':\n",
    "                newcol[:i+1] = True\n",
    "            other = np.insert(other,i,-1)\n",
    "            filt = np.insert(filt,i,newcol,axis=1)\n",
    "        else:\n",
    "            if mode=='smaller':\n",
    "                j = np.nonzero(filt[:,i])[0][0]\n",
    "            elif mode=='bigger':\n",
    "                j = qrs.size-np.nonzero(np.flip(filt[:,i]))[0][0]-1\n",
    "\n",
    "            if i != j:\n",
    "                newcol = np.zeros((qrs.size,),dtype=bool)\n",
    "                if mode=='smaller':\n",
    "                    newcol[i:] = True\n",
    "                elif mode=='bigger':\n",
    "                    newcol[:i+1] = True\n",
    "                other = np.insert(other,i,-1)\n",
    "                filt = np.insert(filt,i,newcol,axis=1)\n",
    "\n",
    "    return other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/media/guille/DADES/DADES/Delineator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD DATASETS ####\n",
    "dataset             = pd.read_csv(os.path.join(basedir,'QTDB','Dataset.csv'), index_col=0)\n",
    "dataset             = dataset.sort_index(axis=1)\n",
    "labels              = np.asarray(list(dataset)) # In case no data augmentation is applied\n",
    "description         = dataset.describe()\n",
    "group               = {k: '_'.join(k.split('_')[:-1]) for k in dataset}\n",
    "unique_ids          = list(set([k.split('_')[0] for k in dataset]))\n",
    "\n",
    "# Load validity\n",
    "validity            = sak.load_data(os.path.join(basedir,'QTDB','validity.csv'))\n",
    "\n",
    "# Load fiducials\n",
    "Pon_QTDB            = load_data(os.path.join(basedir,'QTDB','PonNew.csv'))\n",
    "Poff_QTDB           = load_data(os.path.join(basedir,'QTDB','PoffNew.csv'))\n",
    "QRSon_QTDB          = load_data(os.path.join(basedir,'QTDB','QRSonNew.csv'))\n",
    "QRSoff_QTDB         = load_data(os.path.join(basedir,'QTDB','QRSoffNew.csv'))\n",
    "Ton_QTDB            = load_data(os.path.join(basedir,'QTDB','TonNew.csv'))\n",
    "Toff_QTDB           = load_data(os.path.join(basedir,'QTDB','ToffNew.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate masks & signals\n",
    "x = {}\n",
    "y = {}\n",
    "for k in tqdm.tqdm(QRSon_QTDB):\n",
    "    # Check file exists and all that\n",
    "    if k not in validity:\n",
    "        print(\"Issue with file {}, continuing...\".format(k))\n",
    "        continue\n",
    "\n",
    "    # Store signal\n",
    "    signal = dataset[k][validity[k][0]:validity[k][1]].values\n",
    "    signal = sak.signal.on_off_correction(signal)\n",
    "    amplitude = np.median(sak.signal.moving_lambda(signal,200,sak.signal.abs_max))\n",
    "    signal = signal/amplitude\n",
    "    x[k] = signal[None,]\n",
    "    \n",
    "    # Generate boolean mask\n",
    "    segmentation = np.zeros((3,dataset.shape[0]),dtype=bool)\n",
    "    if k in Pon_QTDB:\n",
    "        for on,off in zip(Pon_QTDB[k],Poff_QTDB[k]):\n",
    "            segmentation[0,on:off] = True\n",
    "    if k in QRSon_QTDB:\n",
    "        for on,off in zip(QRSon_QTDB[k],QRSoff_QTDB[k]):\n",
    "            segmentation[1,on:off] = True\n",
    "    if k in Ton_QTDB:\n",
    "        for on,off in zip(Ton_QTDB[k],Toff_QTDB[k]):\n",
    "            segmentation[2,on:off] = True\n",
    "    \n",
    "    y[k] = segmentation[:,validity[k][0]:validity[k][1]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils\n",
    "\n",
    "class DataQTDB(torch.utils.data.Dataset):\n",
    "    '''Generates data for PyTorch'''\n",
    "\n",
    "    def __init__(self, x, y, window, stride, dtype='float32'):\n",
    "        '''Initialization'''\n",
    "        assert set(x.keys()) == set(y.keys())\n",
    "        # Store inputs\n",
    "        self.window = window\n",
    "        self.stride = stride\n",
    "        self.dtype = dtype\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.keys = list(x)\n",
    "\n",
    "        # Compute size\n",
    "        self.window_distribution = np.cumsum([0] + [(x[k].size - window + stride)//stride for k in x])\n",
    "        self.num_windows = self.window_distribution[-1] # Extremely small performance gain\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Denotes the number of batches per epoch'''\n",
    "        return self.num_windows\n",
    "    \n",
    "    def __get_key_window(self, i):\n",
    "        \"\"\"Retrieve an index's key and number of window\"\"\"\n",
    "        loc = np.argmax(i < self.window_distribution)\n",
    "        key = self.keys[loc-1]\n",
    "        win = i-self.window_distribution[loc-1]\n",
    "        \n",
    "        return key,win\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        '''Generates one datapoint''' \n",
    "        # Retrieve window location\n",
    "        key,n_window = self.__get_key_window(i)\n",
    "\n",
    "        # Compute onsets and offsets for localization\n",
    "        on  = n_window*self.stride\n",
    "        off = on + self.window\n",
    "        \n",
    "        # Retrieve data\n",
    "        x = self.x[key][:,on:off]\n",
    "        y = self.y[key][:,on:off]\n",
    "        \n",
    "        if i == self.num_windows:\n",
    "            raise StopIteration\n",
    "        \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = DataQTDB(x,y,window,stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8070/8070 [00:00<00:00, 139328.44it/s]\n"
     ]
    }
   ],
   "source": [
    "save = []\n",
    "for i,out in enumerate(tqdm.tqdm(self)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum([(x[k].size - 2048 + 128)//128 for k in x])\n",
    "\n",
    "for i in range(total):\n",
    "    # Retrieve single window\n",
    "    numRecord = int(i)//int(self.num_windows)\n",
    "    key       = self.x.keys()[numRecord]\n",
    "    n_window  = (int(i)-numRecord*self.num_windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desambiguate data augmentation\n",
    "j         = i//(self.num_windows*len(self.x))\n",
    "i         = i%(self.num_windows*len(self.x))\n",
    "numRecord = int(i)//int(self.num_windows)\n",
    "key       = self.x.keys()[numRecord]\n",
    "n_window  = (int(i)-numRecord*self.num_windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8968)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[k].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x[k].size-2048+128)//128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning3",
   "language": "python",
   "name": "deeplearning3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
