{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import skimage\n",
    "import skimage.segmentation\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import math\n",
    "import shutil\n",
    "import pathlib\n",
    "import glob\n",
    "import shutil\n",
    "import uuid\n",
    "import random\n",
    "import platform\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import wfdb\n",
    "import copy\n",
    "import fleetfmt\n",
    "import json\n",
    "import tqdm\n",
    "import dill\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import src.data\n",
    "import src.reader\n",
    "\n",
    "import sak\n",
    "import sak.signal.wavelet\n",
    "import sak.data\n",
    "import sak.data.augmentation\n",
    "import sak.visualization\n",
    "import sak.visualization.signal\n",
    "import sak.torch\n",
    "import sak.torch.nn\n",
    "import sak.torch.nn as nn\n",
    "import sak.torch.train\n",
    "import sak.torch.data\n",
    "import sak.data.preprocessing\n",
    "import sak.torch.models\n",
    "import sak.torch.models.lego\n",
    "import sak.torch.models.variational\n",
    "import sak.torch.models.classification\n",
    "\n",
    "from sak.signal import StandardHeader\n",
    "\n",
    "def smooth(x: np.ndarray, window_size: int, conv_mode: str = 'same'):\n",
    "    x = np.pad(np.copy(x),(window_size,window_size),'edge')\n",
    "    window = np.hamming(window_size)/(window_size//2)\n",
    "    x = np.convolve(x, window, mode=conv_mode)\n",
    "    x = x[window_size:-window_size]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From train_multi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:00<00:00, 2522.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue with file sel35_0, continuing...\n",
      "Issue with file sel35_1, continuing...\n",
      "################# FOLD 1 #################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guille/GitHub/DelineatorSwitchAndCompose/Notebooks/sak/torch/nn/modules/composers.py:58: UserWarning: Nodes set() are leafs but not marked as outputs. Check the provided config file\n",
      "  warnings.warn(\"Nodes {} are leafs but not marked as outputs. Check the provided config file\".format(terminal_nodes-terminal_outputs))\n"
     ]
    }
   ],
   "source": [
    "bool_hpc    = False\n",
    "model_name  = \"TestSemiSupervisedLearning\"\n",
    "config_file = './configurations/MPLWNet5LevelsSelfAttention.json'\n",
    "input_files = './pickle/'\n",
    "\n",
    "##### 1. Load configuration file #####\n",
    "with open(config_file, \"r\") as f:\n",
    "    execution = json.load(f)\n",
    "\n",
    "execution[\"root_directory\"] = os.path.expanduser(execution[\"root_directory\"])\n",
    "execution[\"save_directory\"] = os.path.expanduser(execution[\"save_directory\"])\n",
    "\n",
    "# NO ITERATOR FOR HPC, WASTE OF MEMORY\n",
    "if bool_hpc:\n",
    "    execution[\"iterator\"] = \"none\"\n",
    "\n",
    "##### 2. Load synthetic dataset #####\n",
    "# 2.1. Load individual segments\n",
    "P = sak.pickleload(os.path.join(input_files,\"Psignal_new.pkl\"))\n",
    "PQ = sak.pickleload(os.path.join(input_files,\"PQsignal_new.pkl\"))\n",
    "QRS = sak.pickleload(os.path.join(input_files,\"QRSsignal_new.pkl\"))\n",
    "ST = sak.pickleload(os.path.join(input_files,\"STsignal_new.pkl\"))\n",
    "T = sak.pickleload(os.path.join(input_files,\"Tsignal_new.pkl\"))\n",
    "TP = sak.pickleload(os.path.join(input_files,\"TPsignal_new.pkl\"))\n",
    "\n",
    "Pamplitudes = sak.pickleload(os.path.join(input_files,\"Pamplitudes_new.pkl\"))\n",
    "PQamplitudes = sak.pickleload(os.path.join(input_files,\"PQamplitudes_new.pkl\"))\n",
    "QRSamplitudes = sak.pickleload(os.path.join(input_files,\"QRSamplitudes_new.pkl\"))\n",
    "STamplitudes = sak.pickleload(os.path.join(input_files,\"STamplitudes_new.pkl\"))\n",
    "Tamplitudes = sak.pickleload(os.path.join(input_files,\"Tamplitudes_new.pkl\"))\n",
    "TPamplitudes = sak.pickleload(os.path.join(input_files,\"TPamplitudes_new.pkl\"))\n",
    "\n",
    "# 2.2. Get amplitude distribution\n",
    "Pdistribution   = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(Pamplitudes.values()))))\n",
    "PQdistribution  = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(PQamplitudes.values()))))\n",
    "QRSdistribution = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.hstack((np.array(list(QRSamplitudes.values())), 2-np.array(list(QRSamplitudes.values()))))))\n",
    "STdistribution  = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(STamplitudes.values()))))\n",
    "Tdistribution   = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(Tamplitudes.values()))))\n",
    "TPdistribution  = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(TPamplitudes.values()))))\n",
    "\n",
    "# 2.3. Smooth all\n",
    "window = 5\n",
    "P   = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth(  P[k],window)),metric=sak.signal.abs_max) for k in   P}\n",
    "PQ  = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth( PQ[k],window)),metric=sak.signal.abs_max) for k in  PQ}\n",
    "QRS = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth(QRS[k],window)),metric=sak.signal.abs_max) for k in QRS}\n",
    "ST  = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth( ST[k],window)),metric=sak.signal.abs_max) for k in  ST}\n",
    "T   = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth(  T[k],window)),metric=sak.signal.abs_max) for k in   T}\n",
    "TP  = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth( TP[k],window)),metric=sak.signal.abs_max) for k in  TP}\n",
    "\n",
    "\n",
    "##### 3. Load QTDB #####\n",
    "dataset             = pd.read_csv(os.path.join(input_files,'QTDB','Dataset.csv'), index_col=0)\n",
    "dataset             = dataset.sort_index(axis=1)\n",
    "labels              = np.asarray(list(dataset)) # In case no data augmentation is applied\n",
    "description         = dataset.describe()\n",
    "group               = {k: '_'.join(k.split('_')[:-1]) for k in dataset}\n",
    "unique_ids          = list(set([k.split('_')[0] for k in dataset]))\n",
    "\n",
    "# Load validity\n",
    "validity            = sak.load_data(os.path.join(input_files,'QTDB','validity.csv'))\n",
    "\n",
    "# Load fiducials\n",
    "Pon_QTDB            = sak.load_data(os.path.join(input_files,'QTDB','PonNew.csv'))\n",
    "Poff_QTDB           = sak.load_data(os.path.join(input_files,'QTDB','PoffNew.csv'))\n",
    "QRSon_QTDB          = sak.load_data(os.path.join(input_files,'QTDB','QRSonNew.csv'))\n",
    "QRSoff_QTDB         = sak.load_data(os.path.join(input_files,'QTDB','QRSoffNew.csv'))\n",
    "Ton_QTDB            = sak.load_data(os.path.join(input_files,'QTDB','TonNew.csv'))\n",
    "Toff_QTDB           = sak.load_data(os.path.join(input_files,'QTDB','ToffNew.csv'))\n",
    "\n",
    "# Generate masks & signals\n",
    "signal_QTDB = {}\n",
    "segmentation_QTDB = {}\n",
    "for k in tqdm.tqdm(QRSon_QTDB):\n",
    "    # Check file exists and all that\n",
    "    if k not in validity:\n",
    "        print(\"Issue with file {}, continuing...\".format(k))\n",
    "        continue\n",
    "\n",
    "    # Store signal\n",
    "    signal = dataset[k][validity[k][0]:validity[k][1]].values\n",
    "    signal = sak.signal.on_off_correction(signal)\n",
    "    amplitude = np.median(sak.signal.moving_lambda(signal,200,sak.signal.abs_max))\n",
    "    signal = signal/amplitude\n",
    "    signal_QTDB[k] = signal[None,]\n",
    "\n",
    "    # Generate boolean mask\n",
    "    segmentation = np.zeros((3,dataset.shape[0]),dtype=bool)\n",
    "    if k in Pon_QTDB:\n",
    "        for on,off in zip(Pon_QTDB[k],Poff_QTDB[k]):\n",
    "            segmentation[0,on:off] = True\n",
    "    if k in QRSon_QTDB:\n",
    "        for on,off in zip(QRSon_QTDB[k],QRSoff_QTDB[k]):\n",
    "            segmentation[1,on:off] = True\n",
    "    if k in Ton_QTDB:\n",
    "        for on,off in zip(Ton_QTDB[k],Toff_QTDB[k]):\n",
    "            segmentation[2,on:off] = True\n",
    "\n",
    "    segmentation_QTDB[k] = segmentation[:,validity[k][0]:validity[k][1]]\n",
    "\n",
    "\n",
    "##### 4. Generate random splits #####\n",
    "# 4.1. Split into train and test\n",
    "all_keys_synthetic = {}\n",
    "for k in list(P) + list(PQ) + list(QRS) + list(ST) + list(T) + list(TP):\n",
    "    uid = k.split(\"###\")[0].split(\"_\")[0].split(\"-\")[0]\n",
    "    if uid not in all_keys_synthetic:\n",
    "        all_keys_synthetic[uid] = [k]\n",
    "    else:\n",
    "        all_keys_synthetic[uid].append(k)\n",
    "\n",
    "all_keys_real = {}\n",
    "for k in list(signal_QTDB) + list(segmentation_QTDB):\n",
    "    uid = k.split(\"###\")[0].split(\"_\")[0].split(\"-\")[0]\n",
    "    if uid not in all_keys_real:\n",
    "        all_keys_real[uid] = [k]\n",
    "    else:\n",
    "        all_keys_real[uid].append(k)\n",
    "\n",
    "# 4.2. Get database and file\n",
    "filenames = []\n",
    "database = []\n",
    "for k in all_keys_synthetic:\n",
    "    filenames.append(k)\n",
    "    if k.startswith(\"SOO\"):\n",
    "        database.append(0)\n",
    "    elif k.startswith(\"sel\"):\n",
    "        database.append(1)\n",
    "    else:\n",
    "        database.append(2)\n",
    "filenames = np.array(filenames)\n",
    "database = np.array(database)\n",
    "\n",
    "# Set random seed for the execution and perform train/test splitting\n",
    "random.seed(execution[\"seed\"])\n",
    "np.random.seed(execution[\"seed\"])\n",
    "torch.random.manual_seed(execution[\"seed\"])\n",
    "splitter = sklearn.model_selection.StratifiedKFold(5).split(filenames,database)\n",
    "splits = list(splitter)\n",
    "indices_train = [s[0] for s in splits]\n",
    "indices_valid = [s[1] for s in splits]\n",
    "\n",
    "##### 5. Train folds #####\n",
    "# 5.1. Save model-generating files\n",
    "target_path = execution[\"save_directory\"] # Store original output path for future usage\n",
    "original_length = execution[\"dataset\"][\"length\"]\n",
    "if not os.path.isdir(os.path.join(target_path,model_name)):\n",
    "    pathlib.Path(os.path.join(target_path,model_name)).mkdir(parents=True, exist_ok=True)\n",
    "# shutil.copyfile(\"./train_multi.py\",os.path.join(target_path,model_name,\"train_multi.py\"))\n",
    "shutil.copyfile(\"./src/data.py\",os.path.join(target_path,model_name,\"data.py\"))\n",
    "shutil.copyfile(\"./src/metrics.py\",os.path.join(target_path,model_name,\"metrics.py\"))\n",
    "# shutil.copyfile(\"./sak/torch/nn/modules/loss.py\",os.path.join(target_path,model_name,\"loss.py\"))\n",
    "shutil.copyfile(config_file,os.path.join(target_path,model_name,os.path.split(config_file)[1]))\n",
    "\n",
    "# 5.2. Save folds of valid files\n",
    "all_folds_test = {\"fold_{}\".format(i+1): np.array(filenames)[ix_valid] for i,ix_valid in enumerate(indices_valid)}\n",
    "sak.save_data(all_folds_test,os.path.join(target_path,model_name,\"validation_files.csv\"))\n",
    "\n",
    "# 5.3. Iterate over folds\n",
    "for i,(ix_train,ix_valid) in enumerate(zip(indices_train,indices_valid)):\n",
    "    print(\"################# FOLD {} #################\".format(i+1))\n",
    "    # Synthetic keys\n",
    "    train_keys_synthetic, valid_keys_synthetic = ([],[])\n",
    "    for k in np.array(filenames)[ix_train]: \n",
    "        train_keys_synthetic += all_keys_synthetic[k]\n",
    "    for k in np.array(filenames)[ix_valid]: \n",
    "        valid_keys_synthetic += all_keys_synthetic[k]\n",
    "\n",
    "    # Real keys\n",
    "    train_keys_real, valid_keys_real = ([],[])\n",
    "    for k in np.array(filenames)[ix_train]: \n",
    "        if k in all_keys_real: train_keys_real += all_keys_real[k]\n",
    "    for k in np.array(filenames)[ix_valid]: \n",
    "        if k in all_keys_real: valid_keys_real += all_keys_real[k]\n",
    "\n",
    "    # Avoid repetitions\n",
    "    train_keys_synthetic = list(set(train_keys_synthetic))\n",
    "    valid_keys_synthetic = list(set(valid_keys_synthetic))\n",
    "    train_keys_real = list(set(train_keys_real))\n",
    "    valid_keys_real = list(set(valid_keys_real))\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~ Refine synthetic set ~~~~~~~~~~~~~~~~~~~~\n",
    "    # Divide train/valid segments\n",
    "    Ptrain   = {k:   P[k] for k in   P if k in train_keys_synthetic}\n",
    "    PQtrain  = {k:  PQ[k] for k in  PQ if k in train_keys_synthetic}\n",
    "    QRStrain = {k: QRS[k] for k in QRS if k in train_keys_synthetic}\n",
    "    STtrain  = {k:  ST[k] for k in  ST if k in train_keys_synthetic}\n",
    "    Ttrain   = {k:   T[k] for k in   T if k in train_keys_synthetic}\n",
    "    TPtrain  = {k:  TP[k] for k in  TP if k in train_keys_synthetic}\n",
    "\n",
    "    Pvalid   = {k:   P[k] for k in   P if k in valid_keys_synthetic}\n",
    "    PQvalid  = {k:  PQ[k] for k in  PQ if k in valid_keys_synthetic}\n",
    "    QRSvalid = {k: QRS[k] for k in QRS if k in valid_keys_synthetic}\n",
    "    STvalid  = {k:  ST[k] for k in  ST if k in valid_keys_synthetic}\n",
    "    Tvalid   = {k:   T[k] for k in   T if k in valid_keys_synthetic}\n",
    "    TPvalid  = {k:  TP[k] for k in  TP if k in valid_keys_synthetic}\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~ Refine real set ~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    signal_QTDB_train       = {k:       signal_QTDB[k] for k in       signal_QTDB if k in train_keys_real}\n",
    "    signal_QTDB_valid       = {k:       signal_QTDB[k] for k in       signal_QTDB if k in valid_keys_real}\n",
    "    segmentation_QTDB_train = {k: segmentation_QTDB[k] for k in segmentation_QTDB if k in train_keys_real}\n",
    "    segmentation_QTDB_valid = {k: segmentation_QTDB[k] for k in segmentation_QTDB if k in valid_keys_real}\n",
    "\n",
    "\n",
    "    # Prepare folders\n",
    "    execution[\"save_directory\"] = os.path.join(target_path, model_name, \"fold_{}\".format(i+1))\n",
    "    if not os.path.isdir(execution[\"save_directory\"]):\n",
    "        pathlib.Path(execution[\"save_directory\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define synthetic datasets\n",
    "    dataset_train_synthetic = src.data.Dataset(Ptrain, QRStrain, Ttrain, PQtrain, STtrain, TPtrain, \n",
    "                                               Pdistribution, QRSdistribution, Tdistribution, PQdistribution, \n",
    "                                               STdistribution, TPdistribution, **execution[\"dataset\"])\n",
    "    execution[\"dataset\"][\"length\"] = execution[\"dataset\"][\"length\"]//4 # On synthetic data, not so useful to do intensive validation\n",
    "    dataset_valid_synthetic = src.data.Dataset(Pvalid, QRSvalid, Tvalid, PQvalid, STvalid, TPvalid, \n",
    "                                               Pdistribution, QRSdistribution, Tdistribution, PQdistribution, \n",
    "                                               STdistribution, TPdistribution, **execution[\"dataset\"])\n",
    "    execution[\"dataset\"][\"length\"] = original_length # On synthetic data, not so useful to do intensive validation\n",
    "\n",
    "    # Define real datasets\n",
    "    dataset_train_real = src.data.DatasetQTDB(signal_QTDB_train,segmentation_QTDB_train,execution[\"dataset\"][\"N\"],128)\n",
    "    dataset_valid_real = src.data.DatasetQTDB(signal_QTDB_valid,segmentation_QTDB_valid,execution[\"dataset\"][\"N\"],128)\n",
    "\n",
    "    # Define merging dataset\n",
    "    dataset_train = sak.torch.data.UniformMultiDataset((dataset_train_synthetic,dataset_train_real),[10,1],[1,10],return_weights=True)\n",
    "    sampler_train = sak.torch.data.UniformMultiSampler(dataset_train)\n",
    "    dataset_valid = sak.torch.data.UniformMultiDataset((dataset_valid_synthetic,dataset_valid_real),[10,1],[1,10],return_weights=True)\n",
    "    sampler_valid = sak.torch.data.UniformMultiSampler(dataset_valid)\n",
    "\n",
    "    # Create dataloaders\n",
    "    loader_train = torch.utils.data.DataLoader(dataset_train, sampler=sampler_train, **execution[\"loader\"])\n",
    "    loader_valid = torch.utils.data.DataLoader(dataset_valid, sampler=sampler_valid, **execution[\"loader\"])\n",
    "\n",
    "    # Define model\n",
    "    model_teacher = sak.from_dict(execution[\"model\"]).float().cuda()\n",
    "    model_student = sak.from_dict(execution[\"model\"]).float().cuda()\n",
    "\n",
    "    # Train model\n",
    "    state = {\n",
    "        \"epoch\"             : 0,\n",
    "        \"device\"            : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        \"optimizer_teacher\" : sak.class_selector(execution[\"optimizer\"][\"class\"])(model_teacher.parameters(), **execution[\"optimizer\"][\"arguments\"]),\n",
    "        \"optimizer_student\" : sak.class_selector(execution[\"optimizer\"][\"class\"])(model_student.parameters(), **execution[\"optimizer\"][\"arguments\"]),\n",
    "        \"root_dir\"          : \"./\"\n",
    "    }\n",
    "    if \"scheduler\" in execution:\n",
    "        dict_scheduler = copy.deepcopy(execution[\"scheduler\"])\n",
    "        dict_scheduler[\"arguments\"][\"optimizer\"] = state[\"optimizer_teacher\"]\n",
    "        state[\"scheduler_teacher\"] = sak.from_dict(dict_scheduler)\n",
    "        dict_scheduler = copy.deepcopy(execution[\"scheduler\"])\n",
    "        dict_scheduler[\"arguments\"][\"optimizer\"] = state[\"optimizer_student\"]\n",
    "        state[\"scheduler_student\"] = sak.from_dict(dict_scheduler)\n",
    "    \n",
    "    if \"lr_scheduler\" in execution:\n",
    "        dict_lr_scheduler = copy.deepcopy(execution[\"lr_scheduler\"])\n",
    "        dict_lr_scheduler[\"arguments\"][\"optimizer\"] = state[\"optimizer_teacher\"]\n",
    "        state[\"lr_scheduler_teacher\"] = sak.from_dict(dict_lr_scheduler)\n",
    "        dict_lr_scheduler = copy.deepcopy(execution[\"lr_scheduler\"])\n",
    "        dict_lr_scheduler[\"arguments\"][\"optimizer\"] = state[\"optimizer_student\"]\n",
    "        state[\"lr_scheduler_student\"] = sak.from_dict(dict_lr_scheduler)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2254/2254 [00:39<00:00, 57.53it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm.tqdm(loader_valid):\n",
    "    if torch.isnan(batch[\"x\"]).sum() != 0:\n",
    "        kljagkla\n",
    "    if torch.isnan(batch[\"y\"]).sum() != 0:\n",
    "        auigyhuaisdg\n",
    "    if torch.isnan(batch[\"weights\"]).sum() != 0:\n",
    "        mashgkjah\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9078, 15887)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import src.train\n",
    "\n",
    "dataset_mpl = sak.from_dict(execution[\"dataset_unsupervised\"])\n",
    "loader_mpl  = torch.utils.data.DataLoader(dataset_mpl, **execution[\"loader_unsupervised\"])\n",
    "generator   = src.train.get_batch_ssl(loader_train,loader_mpl)\n",
    "\n",
    "len(loader_train),len(loader_mpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_teacher = sak.from_dict(execution[\"model\"]).float()\n",
    "model_student = sak.from_dict(execution[\"model\"]).float()\n",
    "\n",
    "inputs_teacher  = {\"x\": torch.rand(1,1,2048).float(), \"y\": torch.ge(torch.rand(1,3,2048),0.5).float()}\n",
    "outputs_teacher = model_teacher.forward(inputs_teacher)\n",
    "inputs_student  = {\"x\": torch.rand(1,1,2048).float(), \"y\": torch.ge(torch.rand(1,3,2048),0.5).float()}\n",
    "outputs_student = model_student.forward(inputs_student)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config                 = copy.deepcopy(execution)\n",
    "dataloader_labeled     = loader_train\n",
    "dataloader_unlabeled   = loader_mpl\n",
    "criterion              = sak.from_dict(execution['loss'])\n",
    "criterion_crossentropy = sak.from_dict(execution['loss_crossentropy'])\n",
    "# criterion_UDA        = sak.from_dict(execution['loss_UDA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Train) Epoch   1/100, Loss inf: 0it [00:00, ?it/s]/home/guille/VirtEnv/DeepLearning/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "(Train) Epoch   1/100, Loss      3.000,      3.397: : 4111it [16:21,  4.19it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Nan loss value encountered. Stopping...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9c12f3c070cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# Break early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nan loss value encountered. Stopping...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_student\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mstudent_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Nan loss value encountered. Stopping..."
     ]
    }
   ],
   "source": [
    "##### 1. Load configuration file #####\n",
    "with open(config_file, \"r\") as f:\n",
    "    execution = json.load(f)\n",
    "\n",
    "execution[\"root_directory\"] = os.path.expanduser(execution[\"root_directory\"])\n",
    "execution[\"save_directory\"] = os.path.expanduser(execution[\"save_directory\"])\n",
    "config = copy.deepcopy(execution)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Log progress\n",
    "max_length = max(len(dataloader_labeled),len(dataloader_unlabeled))\n",
    "batch_loss = np.zeros((max_length,2),dtype='float16')\n",
    "generator  = src.train.get_batch_ssl(dataloader_labeled,dataloader_unlabeled)\n",
    "state[\"moving_dot_product\"] = state.get(\"moving_dot_product\",0)\n",
    "\n",
    "# Create transforms\n",
    "if ('data_pre' in config):\n",
    "    data_pre     = sak.from_dict(config[\"data_pre\"])\n",
    "if ('augmentation' in config) and model_student.training:\n",
    "    augmentation = sak.from_dict(config[\"augmentation\"])\n",
    "if ('data_post' in config):\n",
    "    data_post    = sak.from_dict(config[\"data_post\"])\n",
    "\n",
    "# Select iterator decorator\n",
    "train_type = 'Train' if model_student.training else 'Valid'\n",
    "iterator = sak.get_tqdm(generator, config.get('iterator',''), \n",
    "                        desc=\"({}) Epoch {:>3d}/{:>3d}, Loss {:0.3f}\".format(train_type, \n",
    "                                                                             state['epoch']+1, \n",
    "                                                                             config['epochs'], np.inf))\n",
    "\n",
    "\n",
    "for i,(inputs_sup,inputs_unsup) in enumerate(iterator):\n",
    "    # Apply data transforms\n",
    "    if ('data_pre' in config):\n",
    "        data_pre(inputs=inputs_sup)\n",
    "        data_pre(inputs=inputs_unsup)\n",
    "    if ('augmentation' in config) and model_student.training:\n",
    "        augmentation(inputs=inputs_sup)\n",
    "        augmentation(inputs=inputs_unsup)\n",
    "    if ('data_post' in config):\n",
    "        data_post(inputs=inputs_sup)\n",
    "        data_post(inputs=inputs_unsup)\n",
    "\n",
    "    # UDA\n",
    "    inputs_aug = copy.deepcopy(inputs_unsup)\n",
    "    if ('augmentation' in config) and model_student.training:\n",
    "        for _ in range(random.randint(0,config[\"UDA\"].get(\"max_N\",5)-2)):\n",
    "            augmentation(inputs=inputs_aug)\n",
    "            \n",
    "    # Map models to device\n",
    "    model_teacher = model_teacher.to(state['device'], non_blocking=True)\n",
    "    model_student = model_student.to(state['device'], non_blocking=True)\n",
    "    \n",
    "    # Set gradients to zero\n",
    "    if model_teacher.training: state['optimizer_teacher'].zero_grad()\n",
    "    if model_student.training: state['optimizer_student'].zero_grad()\n",
    "    \n",
    "    # Predict supervised data\n",
    "    inputs_sup            = {k: v.to(state[\"device\"], non_blocking=True) for k,v in inputs_sup.items()}\n",
    "    outputs_sup_teacher   = {k: v.cpu() for k,v in model_teacher(inputs_sup).items()}\n",
    "    outputs_sup_student   = {k: v.cpu() for k,v in model_student(inputs_sup).items()}\n",
    "    inputs_sup            = {k: v.cpu() for k,v in inputs_sup.items()}\n",
    "    \n",
    "    # Predict unsupervised data\n",
    "    inputs_unsup          = {k: v.to(state[\"device\"], non_blocking=True) for k,v in inputs_unsup.items()}\n",
    "    outputs_unsup_teacher = {k: v.cpu() for k,v in model_teacher(inputs_unsup).items()}\n",
    "    inputs_unsup          = {k: v.cpu() for k,v in inputs_unsup.items()}\n",
    "\n",
    "    # Predict augmented data\n",
    "    inputs_aug            = {k: v.to(state[\"device\"], non_blocking=True) for k,v in inputs_aug.items()}\n",
    "    outputs_aug_teacher   = {k: v.cpu() for k,v in model_teacher(inputs_aug).items()}\n",
    "    outputs_aug_student   = {k: v.cpu() for k,v in model_student(inputs_aug).items()}\n",
    "    inputs_aug            = {k: v.cpu() for k,v in inputs_aug.items()}\n",
    "    \n",
    "    # Supervised loss UDA\n",
    "    uda_sup_loss          = criterion(inputs=inputs_sup,outputs=outputs_sup_teacher,state=state)\n",
    "    \n",
    "    # Temperature on unsupervised for UDA\n",
    "    outputs_unsup_teacher_temp = {k: v.clone().detach() for k,v in outputs_unsup_teacher.items()}\n",
    "    outputs_unsup_teacher_temp[\"logits\"] = outputs_unsup_teacher_temp[\"logits\"]/config[\"UDA\"].get(\"temperature\",0.7)\n",
    "    outputs_unsup_teacher_temp[\"probas\"] = torch.sigmoid(outputs_unsup_teacher_temp[\"logits\"])\n",
    "    \n",
    "    # Mask outputs for unsupervised loss\n",
    "    largest_probs,_       = torch.max(outputs_unsup_teacher_temp[\"probas\"].detach().flatten(start_dim=1),dim=-1)\n",
    "    mask_elements         = torch.ge(largest_probs,config[\"UDA\"].get(\"mask_threshold\",0.6)).float() # GOOD ONE\n",
    "    for _ in range(outputs_unsup_teacher_temp[\"probas\"].ndim-1):\n",
    "        mask_elements     = mask_elements.unsqueeze(-1)\n",
    "    outputs_aug_teacher_mask   = {k: v.clone()*mask_elements for k,v in outputs_aug_teacher.items()}\n",
    "    outputs_unsup_teacher_temp = {k: v.clone()*mask_elements for k,v in outputs_unsup_teacher_temp.items()}\n",
    "    outputs_unsup_teacher_temp[\"y\"] = outputs_unsup_teacher_temp[\"probas\"]\n",
    "        \n",
    "    # Unsupervised loss UDA\n",
    "    uda_unsup_loss        = criterion(inputs=outputs_unsup_teacher_temp,outputs=outputs_aug_teacher_mask,state=state)\n",
    "\n",
    "    # Total loss UDA\n",
    "    step                  = max_length*state[\"epoch\"] + i\n",
    "    uda_weight_decay      = config[\"UDA\"].get(\"weight_decay\",0) # compute weight decay\n",
    "    uda_weight            = config[\"UDA\"].get(\"weight\",10)*min(step/config[\"UDA\"].get(\"steps\",1),1)\n",
    "    uda_total_loss        = (uda_weight*uda_unsup_loss) + uda_sup_loss + uda_weight_decay\n",
    "    \n",
    "\n",
    "    #########################\n",
    "    # Loss on UDA labels, \"for backprop\" and \"For Taylor\"\n",
    "    outputs_aug_teacher_detached = {k: v.clone().detach() for k,v in outputs_aug_teacher.items()}\n",
    "    outputs_aug_teacher_detached[\"y\"] = outputs_aug_teacher_detached[\"probas\"]\n",
    "    student_loss          = criterion(inputs=outputs_aug_teacher_detached, outputs=outputs_aug_student,state=state)\n",
    "    student_loss_cce_prev = criterion(inputs=inputs_sup, outputs=outputs_sup_student,state=state)\n",
    "\n",
    "    # Optimize network's weights\n",
    "    # Break early\n",
    "    if torch.isnan(student_loss):\n",
    "        raise ValueError(\"Nan loss value encountered. Stopping...\")\n",
    "    if model_student.training:\n",
    "        student_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_student.parameters(),config[\"UDA\"].get(\"grad_clip\",5.0))\n",
    "        state[\"optimizer_student\"].step()\n",
    "        if \"lr_scheduler_student\" in state:\n",
    "            state[\"lr_scheduler_student\"].step()\n",
    "\n",
    "    # Apply Exponential Moving Average of student model\n",
    "    if config[\"UDA\"].get(\"ema\",0) > 0: \n",
    "        state[\"ema_model\"].update_parameters(model_student)\n",
    "\n",
    "    # 2nd call to student: get logits & cross-entropy\n",
    "    # Predict supervised data\n",
    "    inputs_sup               = {k: v.to(state[\"device\"], non_blocking=True) for k,v in inputs_sup.items()}\n",
    "    outputs_sup_student_next = {k: v.cpu() for k,v in model_student(inputs_sup).items()}\n",
    "    inputs_sup               = {k: v.cpu() for k,v in inputs_sup.items()}\n",
    "    \n",
    "    # Get supervised loss\n",
    "    student_loss_cce_next    = criterion(inputs=inputs_sup, outputs=outputs_sup_student_next,state=state)\n",
    "\n",
    "    # Get dot product, moving average of dot product & correct dot product with moving average\n",
    "    dot_product               = (student_loss_cce_next - student_loss_cce_prev).detach()\n",
    "    moving_dot_product        = state[\"moving_dot_product\"] + 0.01*(dot_product-state[\"moving_dot_product\"])\n",
    "    dot_product              -= state[\"moving_dot_product\"]\n",
    "\n",
    "    # Final teacher loss\n",
    "    crossentropy_loss         = criterion_crossentropy(inputs=outputs_aug_teacher_detached,outputs=outputs_aug_teacher,state=state)\n",
    "    teacher_loss              = uda_total_loss + dot_product*crossentropy_loss\n",
    "\n",
    "    # Optimize teacher\n",
    "    if torch.isnan(teacher_loss):\n",
    "        raise ValueError(\"Nan loss value encountered. Stopping...\")\n",
    "    if model_teacher.training:\n",
    "        teacher_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_teacher.parameters(),config[\"UDA\"].get(\"grad_clip\",5.0))\n",
    "        state[\"optimizer_teacher\"].step()\n",
    "        if \"lr_scheduler_teacher\" in state:\n",
    "            state[\"lr_scheduler_teacher\"].step()\n",
    "    \n",
    "    # Retrieve for printing purposes\n",
    "    print_loss = (student_loss.item(),teacher_loss.item())\n",
    "\n",
    "    # Accumulate losses\n",
    "    batch_loss[i] = print_loss\n",
    "    \n",
    "    # Change iterator description\n",
    "    if isinstance(iterator,tqdm.tqdm):\n",
    "        if i == max_length-1: batch_print = np.mean(batch_loss,axis=0)\n",
    "        else:                    batch_print = print_loss\n",
    "        iterator.set_description(\"({}) Epoch {:>3d}/{:>3d}, Loss {:10.3f}, {:10.3f}\".format(train_type, state['epoch']+1, config['epochs'], batch_print[0], batch_print[1]))\n",
    "        iterator.refresh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Challenge/E04603/I###0',\n",
       " 'Challenge/E04603/II###0',\n",
       " 'Challenge/E06072/V2###0',\n",
       " 'Challenge/E06072/V3###0',\n",
       " 'Challenge/E06909/V2###0',\n",
       " 'Challenge/E07675/V3###0',\n",
       " 'Challenge/E07941/V6###0',\n",
       " 'Challenge/E08321/V6###0',\n",
       " 'Challenge/I0002/V6###0',\n",
       " 'Challenge/I0002/V6###1',\n",
       " 'Challenge/I0002/V6###2',\n",
       " 'Challenge/I0002/V6###3',\n",
       " 'Challenge/I0002/V6###4',\n",
       " 'Challenge/I0002/V6###5',\n",
       " 'Challenge/I0002/V6###6',\n",
       " 'Challenge/I0002/V6###7',\n",
       " 'Challenge/I0002/V6###8',\n",
       " 'Challenge/I0069/V4###7',\n",
       " 'Challenge/I0069/V4###8',\n",
       " 'Challenge/I0069/V4###9',\n",
       " 'Challenge/I0069/V4###10',\n",
       " 'Challenge/I0069/V4###11',\n",
       " 'Challenge/I0069/V4###12',\n",
       " 'Challenge/I0069/V4###13',\n",
       " 'Challenge/I0069/V4###14',\n",
       " 'Challenge/I0069/V4###15',\n",
       " 'Challenge/I0069/V4###16',\n",
       " 'Challenge/I0069/V4###17',\n",
       " 'Challenge/I0069/V4###18',\n",
       " 'Challenge/I0069/V4###19',\n",
       " 'Challenge/I0069/V4###20',\n",
       " 'Challenge/I0069/V4###21']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arenan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O FILES\n",
    "\n",
    "    aaa = pd.DataFrame(np.random.rand(10000,10000))\n",
    "    \n",
    "    ~~~ FEATHER ~~~\n",
    "    767M\ttest.fth\n",
    "    299 ms ± 21 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "    ~~~ HDF5 ~~~\n",
    "    764M\ttest.hdf5\n",
    "    451 ms ± 92.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "    ~~~ CSV ~~~\n",
    "    1,8G\ttest.csv\n",
    "    29 s ± 291 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "    ~~~ WFDN ~~~\n",
    "    382M\ttest.dat\n",
    "    732K\ttest.hea\n",
    "    3.96 s ± 198 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "    ~~~ FLEET ~~~\n",
    "    771M\ttest.fleet # With pyarrow\n",
    "    765M\ttest.fleet # With pickle protocol 5\n",
    "    # 315 ms ± 10.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) # With pyarrow\n",
    "    161 ms ± 446 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)  # With pickle protocol 5\n",
    "\n",
    "    https://towardsdatascience.com/what-format-should-i-store-my-ecg-data-in-for-dl-training-bc808eb64981"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing numpy content to a new Fleet file.\n",
      "Done.\n",
      "766M\ttest.fleet\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import fleetfmt\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage\n",
    "import skimage.util\n",
    "\n",
    "aaa = np.random.rand(10000,10000)\n",
    "print(\"Writing numpy content to a new Fleet file.\")\n",
    "with pathlib.Path(\"test.fleet\").open('wb') as fhandle, fleetfmt.FileWriter(fhandle) as writer:\n",
    "    for i,value in enumerate(aaa):\n",
    "        writer.append(f\"jakhgkjahgkjhak {i}\", value)\n",
    "print(\"Done.\")\n",
    "!du -sh test.fleet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 ms ± 381 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with pathlib.Path(\"test.fleet\").open('rb') as fhandle, fleetfmt.FileReader(fhandle) as reader:\n",
    "    # get keys\n",
    "    dkeys = list(reader.keys())\n",
    "\n",
    "    # get values\n",
    "    for key in dkeys:\n",
    "        value = reader.read(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279 ns ± 1.02 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n",
      "18.4 µs ± 27.2 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "with pathlib.Path(\"test.fleet\").open('rb') as fhandle, fleetfmt.FileReader(fhandle) as reader:\n",
    "    # get keys\n",
    "    dkeys = list(reader.keys())\n",
    "\n",
    "    # get random value\n",
    "    %timeit key = random.choice(dkeys)\n",
    "    %timeit key = random.choice(dkeys); reader.read(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST I/O FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing numpy content to a new Fleet file.\n",
      "MUSE dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10646/10646 [01:45<00:00, 100.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brugada dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [01:24<00:00, 12.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallot dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1941/1941 [00:17<00:00, 112.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HCM dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:00<00:00, 171.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenge dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41704/41704 [05:48<00:00, 119.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongQT dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 807/807 [00:07<00:00, 109.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THEW dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1700/1700 [2:58:24<00:00,  6.30s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepFake dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150000/150000 [12:54<00:00, 193.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoO dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [00:30<00:00, 17.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDB dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:27<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVDB dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictAF dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [06:30<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zhejiang dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 336/336 [00:08<00:00, 39.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUVR_CARTO dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20618/20618 [37:29<00:00,  9.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "116G\t/media/guille/DADES/DADES/ECG/unsupervised.fleet\n"
     ]
    }
   ],
   "source": [
    "import ishneholterlib\n",
    "import struct\n",
    "\n",
    "window = 2048\n",
    "max_size = window*10\n",
    "window_step = max_size-window//4\n",
    "target_fs = 250\n",
    "target_dtype = \"float32\"\n",
    "\n",
    "print(\"Writing numpy content to a new Fleet file.\")\n",
    "with pathlib.Path(f\"/media/guille/DADES/DADES/ECG/unsupervised_{target_dtype}.fleet\").open('wb') as fhandle, fleetfmt.FileWriter(fhandle) as writer:\n",
    "    print(\"MUSE dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/ECGData/*.csv\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            tmp = pd.read_csv(file)\n",
    "            tmp = tmp.round(3)\n",
    "            tmp.columns = map(lambda x: str(x).upper(), tmp.columns)\n",
    "            fs = 500\n",
    "            hea = list(tmp.columns)\n",
    "            tmp = sak.signal.interpolate.interp1d(tmp.values,target_fs*tmp.shape[0]//fs,axis=0).T\n",
    "            for k,value in zip(hea,tmp): \n",
    "                if value.size < window: continue\n",
    "                writer.append(f\"MUSE/{fname[5:]}/{k}###0\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Brugada dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/Brugada/Databases/HUVR/*.ecg\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg = ishneholterlib.Holter(file)\n",
    "            ecg.load_data()\n",
    "            fs = ecg.sr\n",
    "            for lead in ecg.lead:\n",
    "                k = lead.spec_str().upper()\n",
    "                data = lead.data.copy()\n",
    "                data = sak.signal.interpolate.interp1d(data,target_fs*data.size//fs)\n",
    "                if data.size > max_size:\n",
    "                    data = skimage.util.view_as_windows(data,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    data = data[None,]\n",
    "                for i,value in enumerate(data):\n",
    "                    if i == 0:\n",
    "                        continue\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"Brugada/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Fallot dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/Fallot/ECGs/*.csv\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            tmp = pd.read_csv(file)\n",
    "            tmp = tmp.round(3)\n",
    "            tmp.columns = map(lambda x: str(x).upper(), tmp.columns)\n",
    "            hea = map(lambda x: str(x).upper(), tmp.columns)\n",
    "            fs = 1/np.unique(np.round(np.diff(tmp[\"TIME IN SEC.\"].values),5))\n",
    "            assert fs.size == 1, \"check fs!!!\"\n",
    "            fs = int(fs[0])\n",
    "            hea = list(tmp.columns)\n",
    "            tmp = sak.signal.interpolate.interp1d(tmp.values,target_fs*tmp.shape[0]//fs,axis=0).T\n",
    "            for k,value in zip(hea,tmp): \n",
    "                if k == \"TIME IN SEC.\": continue\n",
    "                if value.size < window: continue\n",
    "                writer.append(f\"Fallot/{fname}/{k}###0\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"HCM dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/HCMData/CSV/*.csv\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            tmp = pd.read_csv(file,header=None)[:-500]/1000\n",
    "            tmp.columns = map(lambda x: str(x).upper(), StandardHeader)\n",
    "            fs = 500\n",
    "            tmp = sak.signal.interpolate.interp1d(tmp.values,target_fs*tmp.shape[0]//fs,axis=0).T\n",
    "            for k,value in zip(StandardHeader,tmp): \n",
    "                if value.size < window: continue\n",
    "                writer.append(f\"HCM/{fname}/{k}###0\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Challenge dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/Challenge/*.mat\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            sig,hea = wfdb.rdsamp(os.path.join(root,fname),return_res=16)\n",
    "            hea[\"sig_name\"] = list(map(lambda x: str(x).upper(), hea[\"sig_name\"]))\n",
    "            fs = hea[\"fs\"]\n",
    "            sig = sak.signal.interpolate.interp1d(sig,target_fs*sig.shape[0]//fs,axis=0).T\n",
    "            for k,lead in zip(hea[\"sig_name\"],sig):\n",
    "                if lead.size > max_size:\n",
    "                    data = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    data = lead[None,]\n",
    "                for i,value in enumerate(data):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"Challenge/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"LongQT dataset\")\n",
    "    time.sleep(0.5)\n",
    "    import xml.etree.ElementTree as ET\n",
    "    ET.register_namespace(\"\",\"http://www3.medical.philips.com\")\n",
    "    all_files = (glob.glob(\"/home/guille/DADES/DADES/ECG/LONG_QT/LongQT_*/*.XML\") + \n",
    "                 glob.glob(\"/home/guille/DADES/DADES/ECG/LONG_QT/LongQT_*/*/*.XML\"))\n",
    "    head = \"{http://www3.medical.philips.com}\"\n",
    "    namespace = {\"xmlns:ns0\": \"http://www3.medical.philips.com\",\n",
    "                 \"xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\",\n",
    "                 \"xsi:schemaLocation\": \"http://www3.medical.philips.com PhilipsECG.xsd\"}\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            tree = ET.parse(file)\n",
    "            if len(tree.findall(\"FullDisclosure\")) != 0:\n",
    "                continue\n",
    "            fs = float(tree.findall(\"StripData/SampleRate\")[0].text)\n",
    "            ecg = {}\n",
    "            for lead in tree.findall(\"StripData/WaveformData\"):\n",
    "                ecg[lead.get(\"lead\").upper()] = np.array(lead.text.strip().split(\",\"),dtype=int)\n",
    "            ecg = np.array([ecg[k] for k in StandardHeader])\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg,round(ecg.shape[1]*250/fs))\n",
    "            for k,lead in zip(StandardHeader,ecg):\n",
    "                if lead.size > max_size:\n",
    "                    data = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    data = lead[None,]\n",
    "                for i,value in enumerate(data):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"LongQT/{fname}/{k}###{i}\", (value/1000).astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"THEW dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/THEWProject/*/*.ecg\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            rt,stu_id = os.path.split(root)\n",
    "            ecg = ishneholterlib.Holter(file,check_valid=False)\n",
    "            ecg.load_data()\n",
    "            fs = ecg.sr\n",
    "            for lead in ecg.lead:\n",
    "                k = lead.spec_str().upper()\n",
    "                data = lead.data.copy()\n",
    "                data = sak.signal.interpolate.interp1d(data,target_fs*data.size//fs)\n",
    "                if data.size > max_size:\n",
    "                    data = skimage.util.view_as_windows(data,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    data = data[None,]\n",
    "                for i,value in enumerate(data):\n",
    "                    if value.size < window: continue\n",
    "                    if np.all(np.abs(np.diff(value)) < 1e-6): continue\n",
    "                    writer.append(f\"THEW/{stu_id}_{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"DeepFake dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/DeepFake/*.asc\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg = pd.read_csv(file,header=None,sep=\" \")\n",
    "            fs = 500\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs)\n",
    "            for k,value in zip(StandardHeader,ecg):\n",
    "                k = k.upper()\n",
    "                if value.size < window: continue\n",
    "                writer.append(f\"DeepFake/{fname}/{k}###0\", (value/1000).astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"SoO dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/Delineator/SoO/RETAG/*.txt\")\n",
    "    db = pd.read_csv('/home/guille/DADES/DADES/ECG/Delineator/SoO/DATABASE_MANUAL.csv')\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg = pd.read_csv(file,header=0,sep=\",\",index_col=0)\n",
    "            hea = list(map(lambda x: str(x).upper(), ecg.columns))\n",
    "            filt_id = (db[\"ID\"] == int(fname.split(\"-\")[0]))\n",
    "            fs = db[filt_id][\"Sampling_Freq\"].values[0]\n",
    "            ecg = np.round(sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs))/(2**15)\n",
    "            for k,lead in zip(hea,ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"SoO/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"EDB dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/PhysioNet/EDB/*.dat\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg,hea = wfdb.rdsamp(file[:-4])\n",
    "            fs = hea[\"fs\"]\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs)\n",
    "            for k,lead in zip(hea[\"sig_name\"],ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"EDB/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"SVDB dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/PhysioNet/SVDB/*.dat\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg,hea = wfdb.rdsamp(file[:-4])\n",
    "            fs = hea[\"fs\"]\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs)\n",
    "            for k,lead in zip(hea[\"sig_name\"],ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"SVDB/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"PredictAF dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/PredictAF/*/*/*.NHS\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            rt,pat_id = os.path.split(root)\n",
    "            rt,stu_id = os.path.split(rt)\n",
    "            ecg,fs = src.reader.readNHS(file)\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg.T,int(target_fs*ecg.shape[0]//fs)).T\n",
    "            for k,lead in zip(StandardHeader,ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if i < 5:\n",
    "                        continue\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"PredictAF/{stu_id}_{pat_id}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Zhejiang dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/RubenDoste/ZhejiangDatabase/PVCVTRawECGData/*.csv\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            if fname in [\"onsets\",\"offsets\"]: continue\n",
    "            ecg = pd.read_csv(file,header=0)\n",
    "            hea = list(ecg.columns)\n",
    "            fs = 2000\n",
    "            ecg = np.round(sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs))/(2**13)\n",
    "            for k,lead in zip(hea,ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"Zhejiang/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"HUVR_CARTO dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/EGMDelineator/Databases/CARTOEXPORT/*/*ECG_Export.txt\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            rt,pat_id = os.path.split(root)\n",
    "            fs = 2000\n",
    "            signal,header = src.reader.Read_CARTO3_ECG(file)\n",
    "            channels = np.array(sak.map_upper([c.split(\"(\")[0].replace(\" \",\"\") for c in header[\"Channels\"]]))\n",
    "            idx_header = np.array([np.where(channels == c)[0][0] for c in StandardHeader])\n",
    "            signal = np.round(sak.signal.interpolate.interp1d(signal[idx_header],target_fs*signal.shape[1]//fs))/(2**10)\n",
    "            for k,lead in zip(StandardHeader,signal):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"HUVR_CARTO/{pat_id}_{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "time.sleep(0.5)\n",
    "print(\"Done.\")\n",
    "time.sleep(0.5)\n",
    "!du -sh /media/guille/DADES/DADES/ECG/unsupervised.fleet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422 ns ± 0.346 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n",
      "1.64 ms ± 32 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "with pathlib.Path(\"/media/guille/DADES/DADES/ECG/unsupervised_16.fleet\").open('rb') as fhandle, fleetfmt.FileReader(fhandle) as reader:\n",
    "    # get keys\n",
    "    dkeys = list(reader.keys())\n",
    "\n",
    "    # get random value\n",
    "    %timeit key = random.choice(dkeys)\n",
    "    %timeit key = random.choice(dkeys); reader.read(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MUSE': 127752,\n",
       " 'Brugada': 92364,\n",
       " 'Fallot': 23292,\n",
       " 'HCM': 1836,\n",
       " 'Challenge': 519984,\n",
       " 'LongQT': 9624,\n",
       " 'THEW': 2194152,\n",
       " 'DeepFake': 1200000,\n",
       " 'SoO': 6492,\n",
       " 'EDB': 16560,\n",
       " 'SVDB': 3588,\n",
       " 'PredictAF': 210738,\n",
       " 'Zhejiang': 4008,\n",
       " 'HUVR_CARTO': 247416}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_keys = {}\n",
    "for key in dkeys:\n",
    "    DB = key.split(\"/\")[0]\n",
    "    \n",
    "    if DB not in all_keys:\n",
    "        all_keys[DB] = 0\n",
    "        \n",
    "    all_keys[DB] += 1\n",
    "all_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
