{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import skimage\n",
    "import skimage.segmentation\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import math\n",
    "import shutil\n",
    "import pathlib\n",
    "import glob\n",
    "import shutil\n",
    "import uuid\n",
    "import random\n",
    "import platform\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import wfdb\n",
    "import copy\n",
    "import fleetfmt\n",
    "import json\n",
    "import tqdm\n",
    "import dill\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import src.data\n",
    "import src.reader\n",
    "\n",
    "import sak\n",
    "import sak.signal.wavelet\n",
    "import sak.data\n",
    "import sak.data.augmentation\n",
    "import sak.visualization\n",
    "import sak.visualization.signal\n",
    "import sak.torch\n",
    "import sak.torch.nn\n",
    "import sak.torch.nn as nn\n",
    "import sak.torch.train\n",
    "import sak.torch.data\n",
    "import sak.data.preprocessing\n",
    "import sak.torch.models\n",
    "import sak.torch.models.lego\n",
    "import sak.torch.models.variational\n",
    "import sak.torch.models.classification\n",
    "\n",
    "from sak.signal import StandardHeader\n",
    "\n",
    "def smooth(x: np.ndarray, window_size: int, conv_mode: str = 'same'):\n",
    "    x = np.pad(np.copy(x),(window_size,window_size),'edge')\n",
    "    window = np.hamming(window_size)/(window_size//2)\n",
    "    x = np.convolve(x, window, mode=conv_mode)\n",
    "    x = x[window_size:-window_size]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From train_multi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:00<00:00, 2559.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue with file sel35_0, continuing...\n",
      "Issue with file sel35_1, continuing...\n",
      "################# FOLD 1 #################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guille/GitHub/DelineatorSwitchAndCompose/Notebooks/sak/torch/nn/modules/composers.py:58: UserWarning: Nodes set() are leafs but not marked as outputs. Check the provided config file\n",
      "  warnings.warn(\"Nodes {} are leafs but not marked as outputs. Check the provided config file\".format(terminal_nodes-terminal_outputs))\n"
     ]
    }
   ],
   "source": [
    "bool_hpc    = False\n",
    "model_name  = \"TestSemiSupervisedLearning\"\n",
    "config_file = './configurations/MPLWNet5LevelsSelfAttention.json'\n",
    "input_files = './pickle/'\n",
    "\n",
    "##### 1. Load configuration file #####\n",
    "with open(config_file, \"r\") as f:\n",
    "    execution = json.load(f)\n",
    "\n",
    "execution[\"root_directory\"] = os.path.expanduser(execution[\"root_directory\"])\n",
    "execution[\"save_directory\"] = os.path.expanduser(execution[\"save_directory\"])\n",
    "\n",
    "# NO ITERATOR FOR HPC, WASTE OF MEMORY\n",
    "if bool_hpc:\n",
    "    execution[\"iterator\"] = \"none\"\n",
    "\n",
    "##### 2. Load synthetic dataset #####\n",
    "# 2.1. Load individual segments\n",
    "P = sak.pickleload(os.path.join(input_files,\"Psignal_new.pkl\"))\n",
    "PQ = sak.pickleload(os.path.join(input_files,\"PQsignal_new.pkl\"))\n",
    "QRS = sak.pickleload(os.path.join(input_files,\"QRSsignal_new.pkl\"))\n",
    "ST = sak.pickleload(os.path.join(input_files,\"STsignal_new.pkl\"))\n",
    "T = sak.pickleload(os.path.join(input_files,\"Tsignal_new.pkl\"))\n",
    "TP = sak.pickleload(os.path.join(input_files,\"TPsignal_new.pkl\"))\n",
    "\n",
    "Pamplitudes = sak.pickleload(os.path.join(input_files,\"Pamplitudes_new.pkl\"))\n",
    "PQamplitudes = sak.pickleload(os.path.join(input_files,\"PQamplitudes_new.pkl\"))\n",
    "QRSamplitudes = sak.pickleload(os.path.join(input_files,\"QRSamplitudes_new.pkl\"))\n",
    "STamplitudes = sak.pickleload(os.path.join(input_files,\"STamplitudes_new.pkl\"))\n",
    "Tamplitudes = sak.pickleload(os.path.join(input_files,\"Tamplitudes_new.pkl\"))\n",
    "TPamplitudes = sak.pickleload(os.path.join(input_files,\"TPamplitudes_new.pkl\"))\n",
    "\n",
    "# 2.2. Get amplitude distribution\n",
    "Pdistribution   = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(Pamplitudes.values()))))\n",
    "PQdistribution  = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(PQamplitudes.values()))))\n",
    "QRSdistribution = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.hstack((np.array(list(QRSamplitudes.values())), 2-np.array(list(QRSamplitudes.values()))))))\n",
    "STdistribution  = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(STamplitudes.values()))))\n",
    "Tdistribution   = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(Tamplitudes.values()))))\n",
    "TPdistribution  = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(TPamplitudes.values()))))\n",
    "\n",
    "# 2.3. Smooth all\n",
    "window = 5\n",
    "P   = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth(  P[k],window)),metric=sak.signal.abs_max) for k in   P}\n",
    "PQ  = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth( PQ[k],window)),metric=sak.signal.abs_max) for k in  PQ}\n",
    "QRS = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth(QRS[k],window)),metric=sak.signal.abs_max) for k in QRS}\n",
    "ST  = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth( ST[k],window)),metric=sak.signal.abs_max) for k in  ST}\n",
    "T   = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth(  T[k],window)),metric=sak.signal.abs_max) for k in   T}\n",
    "TP  = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth( TP[k],window)),metric=sak.signal.abs_max) for k in  TP}\n",
    "\n",
    "\n",
    "##### 3. Load QTDB #####\n",
    "dataset             = pd.read_csv(os.path.join(input_files,'QTDB','Dataset.csv'), index_col=0)\n",
    "dataset             = dataset.sort_index(axis=1)\n",
    "labels              = np.asarray(list(dataset)) # In case no data augmentation is applied\n",
    "description         = dataset.describe()\n",
    "group               = {k: '_'.join(k.split('_')[:-1]) for k in dataset}\n",
    "unique_ids          = list(set([k.split('_')[0] for k in dataset]))\n",
    "\n",
    "# Load validity\n",
    "validity            = sak.load_data(os.path.join(input_files,'QTDB','validity.csv'))\n",
    "\n",
    "# Load fiducials\n",
    "Pon_QTDB            = sak.load_data(os.path.join(input_files,'QTDB','PonNew.csv'))\n",
    "Poff_QTDB           = sak.load_data(os.path.join(input_files,'QTDB','PoffNew.csv'))\n",
    "QRSon_QTDB          = sak.load_data(os.path.join(input_files,'QTDB','QRSonNew.csv'))\n",
    "QRSoff_QTDB         = sak.load_data(os.path.join(input_files,'QTDB','QRSoffNew.csv'))\n",
    "Ton_QTDB            = sak.load_data(os.path.join(input_files,'QTDB','TonNew.csv'))\n",
    "Toff_QTDB           = sak.load_data(os.path.join(input_files,'QTDB','ToffNew.csv'))\n",
    "\n",
    "# Generate masks & signals\n",
    "signal_QTDB = {}\n",
    "segmentation_QTDB = {}\n",
    "for k in tqdm.tqdm(QRSon_QTDB):\n",
    "    # Check file exists and all that\n",
    "    if k not in validity:\n",
    "        print(\"Issue with file {}, continuing...\".format(k))\n",
    "        continue\n",
    "\n",
    "    # Store signal\n",
    "    signal = dataset[k][validity[k][0]:validity[k][1]].values\n",
    "    signal = sak.signal.on_off_correction(signal)\n",
    "    amplitude = np.median(sak.signal.moving_lambda(signal,200,sak.signal.abs_max))\n",
    "    signal = signal/amplitude\n",
    "    signal_QTDB[k] = signal[None,]\n",
    "\n",
    "    # Generate boolean mask\n",
    "    segmentation = np.zeros((3,dataset.shape[0]),dtype=bool)\n",
    "    if k in Pon_QTDB:\n",
    "        for on,off in zip(Pon_QTDB[k],Poff_QTDB[k]):\n",
    "            segmentation[0,on:off] = True\n",
    "    if k in QRSon_QTDB:\n",
    "        for on,off in zip(QRSon_QTDB[k],QRSoff_QTDB[k]):\n",
    "            segmentation[1,on:off] = True\n",
    "    if k in Ton_QTDB:\n",
    "        for on,off in zip(Ton_QTDB[k],Toff_QTDB[k]):\n",
    "            segmentation[2,on:off] = True\n",
    "\n",
    "    segmentation_QTDB[k] = segmentation[:,validity[k][0]:validity[k][1]]\n",
    "\n",
    "\n",
    "##### 4. Generate random splits #####\n",
    "# 4.1. Split into train and test\n",
    "all_keys_synthetic = {}\n",
    "for k in list(P) + list(PQ) + list(QRS) + list(ST) + list(T) + list(TP):\n",
    "    uid = k.split(\"###\")[0].split(\"_\")[0].split(\"-\")[0]\n",
    "    if uid not in all_keys_synthetic:\n",
    "        all_keys_synthetic[uid] = [k]\n",
    "    else:\n",
    "        all_keys_synthetic[uid].append(k)\n",
    "\n",
    "all_keys_real = {}\n",
    "for k in list(signal_QTDB) + list(segmentation_QTDB):\n",
    "    uid = k.split(\"###\")[0].split(\"_\")[0].split(\"-\")[0]\n",
    "    if uid not in all_keys_real:\n",
    "        all_keys_real[uid] = [k]\n",
    "    else:\n",
    "        all_keys_real[uid].append(k)\n",
    "\n",
    "# 4.2. Get database and file\n",
    "filenames = []\n",
    "database = []\n",
    "for k in all_keys_synthetic:\n",
    "    filenames.append(k)\n",
    "    if k.startswith(\"SOO\"):\n",
    "        database.append(0)\n",
    "    elif k.startswith(\"sel\"):\n",
    "        database.append(1)\n",
    "    else:\n",
    "        database.append(2)\n",
    "filenames = np.array(filenames)\n",
    "database = np.array(database)\n",
    "\n",
    "# Set random seed for the execution and perform train/test splitting\n",
    "random.seed(execution[\"seed\"])\n",
    "np.random.seed(execution[\"seed\"])\n",
    "torch.random.manual_seed(execution[\"seed\"])\n",
    "splitter = sklearn.model_selection.StratifiedKFold(5).split(filenames,database)\n",
    "splits = list(splitter)\n",
    "indices_train = [s[0] for s in splits]\n",
    "indices_valid = [s[1] for s in splits]\n",
    "\n",
    "##### 5. Train folds #####\n",
    "# 5.1. Save model-generating files\n",
    "target_path = execution[\"save_directory\"] # Store original output path for future usage\n",
    "original_length = execution[\"dataset\"][\"length\"]\n",
    "if not os.path.isdir(os.path.join(target_path,model_name)):\n",
    "    pathlib.Path(os.path.join(target_path,model_name)).mkdir(parents=True, exist_ok=True)\n",
    "# shutil.copyfile(\"./train_multi.py\",os.path.join(target_path,model_name,\"train_multi.py\"))\n",
    "shutil.copyfile(\"./src/data.py\",os.path.join(target_path,model_name,\"data.py\"))\n",
    "shutil.copyfile(\"./src/metrics.py\",os.path.join(target_path,model_name,\"metrics.py\"))\n",
    "# shutil.copyfile(\"./sak/torch/nn/modules/loss.py\",os.path.join(target_path,model_name,\"loss.py\"))\n",
    "shutil.copyfile(config_file,os.path.join(target_path,model_name,os.path.split(config_file)[1]))\n",
    "\n",
    "# 5.2. Save folds of valid files\n",
    "all_folds_test = {\"fold_{}\".format(i+1): np.array(filenames)[ix_valid] for i,ix_valid in enumerate(indices_valid)}\n",
    "sak.save_data(all_folds_test,os.path.join(target_path,model_name,\"validation_files.csv\"))\n",
    "\n",
    "# 5.3. Iterate over folds\n",
    "for i,(ix_train,ix_valid) in enumerate(zip(indices_train,indices_valid)):\n",
    "    print(\"################# FOLD {} #################\".format(i+1))\n",
    "    # Synthetic keys\n",
    "    train_keys_synthetic, valid_keys_synthetic = ([],[])\n",
    "    for k in np.array(filenames)[ix_train]: \n",
    "        train_keys_synthetic += all_keys_synthetic[k]\n",
    "    for k in np.array(filenames)[ix_valid]: \n",
    "        valid_keys_synthetic += all_keys_synthetic[k]\n",
    "\n",
    "    # Real keys\n",
    "    train_keys_real, valid_keys_real = ([],[])\n",
    "    for k in np.array(filenames)[ix_train]: \n",
    "        if k in all_keys_real: train_keys_real += all_keys_real[k]\n",
    "    for k in np.array(filenames)[ix_valid]: \n",
    "        if k in all_keys_real: valid_keys_real += all_keys_real[k]\n",
    "\n",
    "    # Avoid repetitions\n",
    "    train_keys_synthetic = list(set(train_keys_synthetic))\n",
    "    valid_keys_synthetic = list(set(valid_keys_synthetic))\n",
    "    train_keys_real = list(set(train_keys_real))\n",
    "    valid_keys_real = list(set(valid_keys_real))\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~ Refine synthetic set ~~~~~~~~~~~~~~~~~~~~\n",
    "    # Divide train/valid segments\n",
    "    Ptrain   = {k:   P[k] for k in   P if k in train_keys_synthetic}\n",
    "    PQtrain  = {k:  PQ[k] for k in  PQ if k in train_keys_synthetic}\n",
    "    QRStrain = {k: QRS[k] for k in QRS if k in train_keys_synthetic}\n",
    "    STtrain  = {k:  ST[k] for k in  ST if k in train_keys_synthetic}\n",
    "    Ttrain   = {k:   T[k] for k in   T if k in train_keys_synthetic}\n",
    "    TPtrain  = {k:  TP[k] for k in  TP if k in train_keys_synthetic}\n",
    "\n",
    "    Pvalid   = {k:   P[k] for k in   P if k in valid_keys_synthetic}\n",
    "    PQvalid  = {k:  PQ[k] for k in  PQ if k in valid_keys_synthetic}\n",
    "    QRSvalid = {k: QRS[k] for k in QRS if k in valid_keys_synthetic}\n",
    "    STvalid  = {k:  ST[k] for k in  ST if k in valid_keys_synthetic}\n",
    "    Tvalid   = {k:   T[k] for k in   T if k in valid_keys_synthetic}\n",
    "    TPvalid  = {k:  TP[k] for k in  TP if k in valid_keys_synthetic}\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~ Refine real set ~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    signal_QTDB_train       = {k:       signal_QTDB[k] for k in       signal_QTDB if k in train_keys_real}\n",
    "    signal_QTDB_valid       = {k:       signal_QTDB[k] for k in       signal_QTDB if k in valid_keys_real}\n",
    "    segmentation_QTDB_train = {k: segmentation_QTDB[k] for k in segmentation_QTDB if k in train_keys_real}\n",
    "    segmentation_QTDB_valid = {k: segmentation_QTDB[k] for k in segmentation_QTDB if k in valid_keys_real}\n",
    "\n",
    "\n",
    "    # Prepare folders\n",
    "    execution[\"save_directory\"] = os.path.join(target_path, model_name, \"fold_{}\".format(i+1))\n",
    "    if not os.path.isdir(execution[\"save_directory\"]):\n",
    "        pathlib.Path(execution[\"save_directory\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define synthetic datasets\n",
    "    dataset_train_synthetic = src.data.Dataset(Ptrain, QRStrain, Ttrain, PQtrain, STtrain, TPtrain, \n",
    "                                               Pdistribution, QRSdistribution, Tdistribution, PQdistribution, \n",
    "                                               STdistribution, TPdistribution, **execution[\"dataset\"])\n",
    "    execution[\"dataset\"][\"length\"] = execution[\"dataset\"][\"length\"]//4 # On synthetic data, not so useful to do intensive validation\n",
    "    dataset_valid_synthetic = src.data.Dataset(Pvalid, QRSvalid, Tvalid, PQvalid, STvalid, TPvalid, \n",
    "                                               Pdistribution, QRSdistribution, Tdistribution, PQdistribution, \n",
    "                                               STdistribution, TPdistribution, **execution[\"dataset\"])\n",
    "    execution[\"dataset\"][\"length\"] = original_length # On synthetic data, not so useful to do intensive validation\n",
    "\n",
    "    # Define real datasets\n",
    "    dataset_train_real = src.data.DatasetQTDB(signal_QTDB_train,segmentation_QTDB_train,execution[\"dataset\"][\"N\"],128)\n",
    "    dataset_valid_real = src.data.DatasetQTDB(signal_QTDB_valid,segmentation_QTDB_valid,execution[\"dataset\"][\"N\"],128)\n",
    "\n",
    "    # Define merging dataset\n",
    "    dataset_train = sak.torch.data.UniformMultiDataset((dataset_train_synthetic,dataset_train_real),[10,1],[1,10],return_weights=True)\n",
    "    sampler_train = sak.torch.data.UniformMultiSampler(dataset_train)\n",
    "    dataset_valid = sak.torch.data.UniformMultiDataset((dataset_valid_synthetic,dataset_valid_real),[10,1],[1,10],return_weights=True)\n",
    "    sampler_valid = sak.torch.data.UniformMultiSampler(dataset_valid)\n",
    "\n",
    "    # Create dataloaders\n",
    "    loader_train = torch.utils.data.DataLoader(dataset_train, sampler=sampler_train, **execution[\"loader\"])\n",
    "    loader_valid = torch.utils.data.DataLoader(dataset_valid, sampler=sampler_valid, **execution[\"loader\"])\n",
    "\n",
    "    # Define model\n",
    "    model_teacher = sak.from_dict(execution[\"model\"]).float().cuda()\n",
    "    model_student = sak.from_dict(execution[\"model\"]).float().cuda()\n",
    "\n",
    "    # Train model\n",
    "    state = {\n",
    "        \"epoch\"             : 0,\n",
    "        \"device\"            : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        \"optimizer_teacher\" : sak.class_selector(execution[\"optimizer\"][\"class\"])(model_teacher.parameters(), **execution[\"optimizer\"][\"arguments\"]),\n",
    "        \"optimizer_student\" : sak.class_selector(execution[\"optimizer\"][\"class\"])(model_student.parameters(), **execution[\"optimizer\"][\"arguments\"]),\n",
    "        \"root_dir\"          : \"./\"\n",
    "    }\n",
    "    if \"scheduler\" in execution:\n",
    "        state[\"scheduler_teacher\"] = sak.class_selector(execution[\"scheduler\"][\"class\"])(state[\"optimizer_teacher\"], **execution[\"scheduler\"][\"arguments\"])\n",
    "    if \"scheduler\" in execution:\n",
    "        state[\"scheduler_student\"] = sak.class_selector(execution[\"scheduler\"][\"class\"])(state[\"optimizer_student\"], **execution[\"scheduler\"][\"arguments\"])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 1. Load configuration file #####\n",
    "with open(config_file, \"r\") as f:\n",
    "    execution = json.load(f)\n",
    "\n",
    "execution[\"root_directory\"] = os.path.expanduser(execution[\"root_directory\"])\n",
    "execution[\"save_directory\"] = os.path.expanduser(execution[\"save_directory\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9078, 15887)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import src.train\n",
    "\n",
    "dataset_mpl = sak.from_dict(execution[\"dataset_unsupervised\"])\n",
    "loader_mpl  = torch.utils.data.DataLoader(dataset_mpl, **execution[\"loader_unsupervised\"])\n",
    "generator   = src.train.get_batch_ssl(loader_train,loader_mpl)\n",
    "\n",
    "len(loader_train),len(loader_mpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_teacher = sak.from_dict(execution[\"model\"]).float()\n",
    "model_student = sak.from_dict(execution[\"model\"]).float()\n",
    "\n",
    "inputs_teacher  = {\"x\": torch.rand(1,1,2048).float(), \"y\": torch.ge(torch.rand(1,3,2048),0.5).float()}\n",
    "outputs_teacher = model_teacher.forward(inputs_teacher)\n",
    "inputs_student  = {\"x\": torch.rand(1,1,2048).float(), \"y\": torch.ge(torch.rand(1,3,2048),0.5).float()}\n",
    "outputs_student = model_student.forward(inputs_student)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config                 = copy.deepcopy(execution)\n",
    "dataloader_labeled     = loader_train\n",
    "dataloader_unlabeled   = loader_mpl\n",
    "criterion              = sak.from_dict(execution['loss'])\n",
    "criterion_crossentropy = sak.from_dict(execution['loss_crossentropy'])\n",
    "# criterion_UDA        = sak.from_dict(execution['loss_UDA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Train) Epoch   1/100, Loss      3.000,      3.272: : 108it [00:26,  4.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9c12f3c070cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_sup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs_unsup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Apply data transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'data_pre'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/DelineatorSwitchAndCompose/Notebooks/src/train.py\u001b[0m in \u001b[0;36mget_batch_ssl\u001b[0;34m(loader_labeled, loader_unlabeled)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Return next element in iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_labeled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_unlabeled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/DelineatorSwitchAndCompose/Notebooks/src/data.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Read fragment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mfragment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Generate onset randomly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/fleetfmt/reader.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/fleetfmt/reader.py\u001b[0m in \u001b[0;36m_read_record\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# read record size (bytes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mrsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRECORD_HEAD_SERDES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# deserialize using pyarrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### 1. Load configuration file #####\n",
    "with open(config_file, \"r\") as f:\n",
    "    execution = json.load(f)\n",
    "\n",
    "execution[\"root_directory\"] = os.path.expanduser(execution[\"root_directory\"])\n",
    "execution[\"save_directory\"] = os.path.expanduser(execution[\"save_directory\"])\n",
    "config = copy.deepcopy(execution)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Log progress\n",
    "max_length = max(len(dataloader_labeled),len(dataloader_unlabeled))\n",
    "batch_loss = np.zeros((max_length,2),dtype='float16')\n",
    "generator  = src.train.get_batch_ssl(dataloader_labeled,dataloader_unlabeled)\n",
    "state[\"moving_dot_product\"] = state.get(\"moving_dot_product\",0)\n",
    "\n",
    "# Create transforms\n",
    "if ('data_pre' in config):\n",
    "    data_pre     = sak.from_dict(config[\"data_pre\"])\n",
    "if ('augmentation' in config) and model_student.training:\n",
    "    augmentation = sak.from_dict(config[\"augmentation\"])\n",
    "if ('data_post' in config):\n",
    "    data_post    = sak.from_dict(config[\"data_post\"])\n",
    "\n",
    "# Select iterator decorator\n",
    "train_type = 'Train' if model_student.training else 'Valid'\n",
    "iterator = sak.get_tqdm(generator, config.get('iterator',''), \n",
    "                        desc=\"({}) Epoch {:>3d}/{:>3d}, Loss {:0.3f}\".format(train_type, \n",
    "                                                                             state['epoch']+1, \n",
    "                                                                             config['epochs'], np.inf))\n",
    "\n",
    "\n",
    "for i,(inputs_sup,inputs_unsup) in enumerate(iterator):\n",
    "    # Apply data transforms\n",
    "    if ('data_pre' in config):\n",
    "        data_pre(inputs=inputs_sup)\n",
    "        data_pre(inputs=inputs_unsup)\n",
    "    if ('augmentation' in config) and model_student.training:\n",
    "        augmentation(inputs=inputs_sup)\n",
    "        augmentation(inputs=inputs_unsup)\n",
    "    if ('data_post' in config):\n",
    "        data_post(inputs=inputs_sup)\n",
    "        data_post(inputs=inputs_unsup)\n",
    "\n",
    "    # UDA\n",
    "    inputs_aug = copy.deepcopy(inputs_unsup)\n",
    "    if ('augmentation' in config) and model_student.training:\n",
    "        for _ in range(random.randint(0,config[\"UDA\"].get(\"max_N\",5)-2)):\n",
    "            augmentation(inputs=inputs_aug)\n",
    "            \n",
    "    # Map models to device\n",
    "    model_teacher = model_teacher.to(state['device'], non_blocking=True)\n",
    "    model_student = model_student.to(state['device'], non_blocking=True)\n",
    "    \n",
    "    # Set gradients to zero\n",
    "    if model_teacher.training: state['optimizer_teacher'].zero_grad()\n",
    "    if model_student.training: state['optimizer_student'].zero_grad()\n",
    "    \n",
    "    # Predict supervised data\n",
    "    inputs_sup            = {k: v.to(state[\"device\"], non_blocking=True) for k,v in inputs_sup.items()}\n",
    "    outputs_sup_teacher   = {k: v.cpu() for k,v in model_teacher(inputs_sup).items()}\n",
    "    outputs_sup_student   = {k: v.cpu() for k,v in model_student(inputs_sup).items()}\n",
    "    inputs_sup            = {k: v.cpu() for k,v in inputs_sup.items()}\n",
    "    \n",
    "    # Predict unsupervised data\n",
    "    inputs_unsup          = {k: v.to(state[\"device\"], non_blocking=True) for k,v in inputs_unsup.items()}\n",
    "    outputs_unsup_teacher = {k: v.cpu() for k,v in model_teacher(inputs_unsup).items()}\n",
    "    inputs_unsup          = {k: v.cpu() for k,v in inputs_unsup.items()}\n",
    "\n",
    "    # Predict augmented data\n",
    "    inputs_aug            = {k: v.to(state[\"device\"], non_blocking=True) for k,v in inputs_aug.items()}\n",
    "    outputs_aug_teacher   = {k: v.cpu() for k,v in model_teacher(inputs_aug).items()}\n",
    "    outputs_aug_student   = {k: v.cpu() for k,v in model_student(inputs_aug).items()}\n",
    "    inputs_aug            = {k: v.cpu() for k,v in inputs_aug.items()}\n",
    "    \n",
    "    # Supervised loss UDA\n",
    "    uda_sup_loss          = criterion(inputs=inputs_sup,outputs=outputs_sup_teacher,state=state)\n",
    "    \n",
    "    # Temperature on unsupervised for UDA\n",
    "    outputs_unsup_teacher_temp = {k: v.clone().detach() for k,v in outputs_unsup_teacher.items()}\n",
    "    outputs_unsup_teacher_temp[\"logits\"] = outputs_unsup_teacher_temp[\"logits\"]/config[\"UDA\"].get(\"temperature\",0.7)\n",
    "    outputs_unsup_teacher_temp[\"probas\"] = torch.sigmoid(outputs_unsup_teacher_temp[\"logits\"])\n",
    "    \n",
    "    # Mask outputs for unsupervised loss\n",
    "    largest_probs,_       = torch.max(outputs_unsup_teacher_temp[\"probas\"].detach().flatten(start_dim=1),dim=-1)\n",
    "    mask_elements         = torch.ge(largest_probs,config[\"UDA\"].get(\"mask_threshold\",0.6)).float() # GOOD ONE\n",
    "    for _ in range(outputs_unsup_teacher_temp[\"probas\"].ndim-1):\n",
    "        mask_elements     = mask_elements.unsqueeze(-1)\n",
    "    outputs_aug_teacher_mask   = {k: v.clone()*mask_elements for k,v in outputs_aug_teacher.items()}\n",
    "    outputs_unsup_teacher_temp = {k: v.clone()*mask_elements for k,v in outputs_unsup_teacher_temp.items()}\n",
    "    outputs_unsup_teacher_temp[\"y\"] = outputs_unsup_teacher_temp[\"probas\"]\n",
    "        \n",
    "    # Unsupervised loss UDA\n",
    "    uda_unsup_loss        = criterion(inputs=outputs_unsup_teacher_temp,outputs=outputs_aug_teacher_mask,state=state)\n",
    "\n",
    "    # Total loss UDA\n",
    "    step                  = max_length*state[\"epoch\"] + i\n",
    "    uda_weight_decay      = config[\"UDA\"].get(\"weight_decay\",0) # compute weight decay\n",
    "    uda_weight            = config[\"UDA\"].get(\"weight\",10)*min(step/config[\"UDA\"].get(\"steps\",1),1)\n",
    "    uda_total_loss        = (uda_weight*uda_unsup_loss) + uda_sup_loss + uda_weight_decay\n",
    "    \n",
    "\n",
    "    #########################\n",
    "    # Loss on UDA labels, \"for backprop\" and \"For Taylor\"\n",
    "    outputs_aug_teacher_detached = {k: v.clone().detach() for k,v in outputs_aug_teacher.items()}\n",
    "    outputs_aug_teacher_detached[\"y\"] = outputs_aug_teacher_detached[\"probas\"]\n",
    "    student_loss          = criterion(inputs=outputs_aug_teacher_detached, outputs=outputs_aug_student,state=state)\n",
    "    student_loss_cce_prev = criterion(inputs=inputs_sup, outputs=outputs_sup_student,state=state)\n",
    "\n",
    "    # Optimize network's weights\n",
    "    # Break early\n",
    "    if torch.isnan(student_loss):\n",
    "        raise ValueError(\"Nan loss value encountered. Stopping...\")\n",
    "    if model_student.training:\n",
    "        student_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_student.parameters(),config[\"UDA\"].get(\"grad_clip\",5.0))\n",
    "        state[\"optimizer_student\"].step()\n",
    "        if \"lr_scheduler_student\" in state:\n",
    "            state[\"lr_scheduler_student\"].step()\n",
    "\n",
    "    # Apply Exponential Moving Average of student model\n",
    "    if config[\"UDA\"].get(\"ema\",0) > 0: \n",
    "        state[\"ema_model\"].update_parameters(model_student)\n",
    "\n",
    "    # 2nd call to student: get logits & cross-entropy\n",
    "    # Predict supervised data\n",
    "    inputs_sup               = {k: v.to(state[\"device\"], non_blocking=True) for k,v in inputs_sup.items()}\n",
    "    outputs_sup_student_next = {k: v.cpu() for k,v in model_student(inputs_sup).items()}\n",
    "    inputs_sup               = {k: v.cpu() for k,v in inputs_sup.items()}\n",
    "    \n",
    "    # Get supervised loss\n",
    "    student_loss_cce_next    = criterion(inputs=inputs_sup, outputs=outputs_sup_student_next,state=state)\n",
    "\n",
    "    # Get dot product, moving average of dot product & correct dot product with moving average\n",
    "    dot_product               = (student_loss_cce_next - student_loss_cce_prev).detach()\n",
    "    moving_dot_product        = state[\"moving_dot_product\"] + 0.01*(dot_product-state[\"moving_dot_product\"])\n",
    "    dot_product              -= state[\"moving_dot_product\"]\n",
    "\n",
    "    # Final teacher loss\n",
    "    crossentropy_loss         = criterion_crossentropy(inputs=outputs_aug_teacher_detached,outputs=outputs_aug_teacher,state=state)\n",
    "    teacher_loss              = uda_total_loss + dot_product*crossentropy_loss\n",
    "\n",
    "    # Optimize teacher\n",
    "    if torch.isnan(teacher_loss):\n",
    "        raise ValueError(\"Nan loss value encountered. Stopping...\")\n",
    "    if model_teacher.training:\n",
    "        teacher_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_teacher.parameters(),config[\"UDA\"].get(\"grad_clip\",5.0))\n",
    "        state[\"optimizer_teacher\"].step()\n",
    "        if \"lr_scheduler_teacher\" in state:\n",
    "            state[\"lr_scheduler_teacher\"].step()\n",
    "    \n",
    "    # Retrieve for printing purposes\n",
    "    print_loss = (student_loss.item(),teacher_loss.item())\n",
    "\n",
    "    # Accumulate losses\n",
    "    batch_loss[i] = print_loss\n",
    "    \n",
    "    # Change iterator description\n",
    "    if isinstance(iterator,tqdm.tqdm):\n",
    "        if i == max_length-1: batch_print = np.mean(batch_loss,axis=0)\n",
    "        else:                    batch_print = print_loss\n",
    "        iterator.set_description(\"({}) Epoch {:>3d}/{:>3d}, Loss {:10.3f}, {:10.3f}\".format(train_type, state['epoch']+1, config['epochs'], batch_print[0], batch_print[1]))\n",
    "        iterator.refresh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 678/9078 [00:12<02:31, 55.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-840f201fd66b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/guille/VirtEnv/DeepLearning/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in _pin_memory_loop\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/guille/VirtEnv/DeepLearning/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 289, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 508, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 752, in answer_challenge\n",
      "    message = connection.recv_bytes(256)         # reject large message\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 383, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm.tqdm(loader_train):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 334/15887 [00:08<06:54, 37.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d1e12ba7cbeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_mpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/DelineatorSwitchAndCompose/Notebooks/src/data.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Read fragment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mfragment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Generate onset randomly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/fleetfmt/reader.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtEnv/DeepLearning/lib/python3.8/site-packages/fleetfmt/reader.py\u001b[0m in \u001b[0;36m_read_record\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# read record size (bytes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mrsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRECORD_HEAD_SERDES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# deserialize using pyarrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in tqdm.tqdm(loader_mpl):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Temperature on unsupervised for UDA\n",
    "    outputs_unsup_teacher_temp = {k: v.clone() for k,v in outputs_unsup_teacher.items()}\n",
    "    outputs_unsup_teacher_temp[\"logits\"] = outputs_unsup_teacher_temp[\"logits\"]/config.get(\"UDA_temp\",0.7)\n",
    "    outputs_unsup_teacher_temp[\"probas\"] = torch.sigmoid(outputs_unsup_teacher_temp[\"logits\"])\n",
    "    \n",
    "    # Mask outputs for unsupervised loss\n",
    "    largest_probs,_     = torch.max(outputs_unsup_teacher_temp[\"probas\"].detach().flatten(start_dim=1),dim=-1)\n",
    "    mask_elements       = torch.ge(largest_probs,config.get(\"UDA_threshold\",0.6)).float() # GOOD ONE\n",
    "    for _ in range(outputs_unsup_teacher_temp[\"probas\"].ndim-1):\n",
    "        mask_elements   = mask_elements.unsqueeze(-1)\n",
    "    outputs_aug_teacher_mask   = {k: v.clone()*mask_elements for k,v in outputs_aug_teacher.items()}\n",
    "    outputs_unsup_teacher_temp = {k: v.clone()*mask_elements for k,v in outputs_unsup_teacher_temp.items()}\n",
    "    outputs_unsup_teacher_temp[\"y\"] = outputs_unsup_teacher_temp[\"probas\"]\n",
    "        \n",
    "    # Unsupervised loss\n",
    "    uda_unsup_loss      = criterion(inputs=outputs_unsup_teacher_temp,outputs=outputs_aug_teacher_mask,state=state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[[ 0.4213,  0.3591,  0.1528,  ...,  0.3275,  0.4264,  0.2644],\n",
       "          [-0.6373, -0.6299, -0.5987,  ..., -0.7278, -0.7710, -0.5929],\n",
       "          [-0.4923, -0.5203, -0.4258,  ..., -0.5306, -0.5816, -0.5087]],\n",
       " \n",
       "         [[ 0.3647,  0.4907,  0.4822,  ...,  0.3387,  0.5196,  0.4550],\n",
       "          [-0.6635, -0.7594, -0.8056,  ..., -0.6614, -0.7185, -0.6619],\n",
       "          [-0.5309, -0.6178, -0.6667,  ..., -0.3976, -0.4569, -0.4806]],\n",
       " \n",
       "         [[-0.5509, -0.4088, -0.3397,  ...,  0.1107,  0.1051,  0.2511],\n",
       "          [ 0.2466,  0.4957,  0.4311,  ..., -0.4860, -0.4935, -0.4588],\n",
       "          [ 0.1968,  0.0679,  0.3894,  ..., -0.4295, -0.4527, -0.4319]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.3239, -0.0707, -0.0020,  ...,  0.4040, -0.2579,  0.3074],\n",
       "          [-0.3870, -0.5109, -0.4724,  ..., -0.4211, -0.4171, -0.4190],\n",
       "          [-0.5572, -0.2215, -0.2177,  ..., -0.3862, -0.1200, -0.3987]],\n",
       " \n",
       "         [[ 0.1849,  0.1121,  0.4905,  ...,  0.7113,  0.5749,  0.4791],\n",
       "          [-0.5869, -0.4274, -0.7901,  ..., -1.0152, -0.9105, -0.7129],\n",
       "          [-0.4395, -0.3435, -0.5811,  ..., -0.7305, -0.6465, -0.5227]],\n",
       " \n",
       "         [[ 0.0982,  0.0659, -0.0628,  ..., -0.0345,  0.0618,  0.1654],\n",
       "          [-0.3201, -0.2210, -0.1371,  ..., -0.1092, -0.1749, -0.2029],\n",
       "          [-0.2889, -0.3319, -0.1883,  ..., -0.0284, -0.1582, -0.2954]]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " 'probas': tensor([[[0.6038, 0.5888, 0.5381,  ..., 0.5812, 0.6050, 0.5657],\n",
       "          [0.3458, 0.3475, 0.3546,  ..., 0.3257, 0.3163, 0.3560],\n",
       "          [0.3793, 0.3728, 0.3951,  ..., 0.3704, 0.3586, 0.3755]],\n",
       " \n",
       "         [[0.5902, 0.6203, 0.6183,  ..., 0.5839, 0.6271, 0.6118],\n",
       "          [0.3399, 0.3188, 0.3088,  ..., 0.3404, 0.3277, 0.3403],\n",
       "          [0.3703, 0.3503, 0.3392,  ..., 0.4019, 0.3877, 0.3821]],\n",
       " \n",
       "         [[0.3657, 0.3992, 0.4159,  ..., 0.5276, 0.5263, 0.5624],\n",
       "          [0.5613, 0.6214, 0.6061,  ..., 0.3808, 0.3791, 0.3873],\n",
       "          [0.5490, 0.5170, 0.5961,  ..., 0.3942, 0.3887, 0.3937]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.5803, 0.4823, 0.4995,  ..., 0.5996, 0.4359, 0.5762],\n",
       "          [0.4044, 0.3750, 0.3841,  ..., 0.3963, 0.3972, 0.3968],\n",
       "          [0.3642, 0.4449, 0.4458,  ..., 0.4046, 0.4700, 0.4016]],\n",
       " \n",
       "         [[0.5461, 0.5280, 0.6202,  ..., 0.6707, 0.6399, 0.6175],\n",
       "          [0.3574, 0.3948, 0.3121,  ..., 0.2660, 0.2869, 0.3289],\n",
       "          [0.3919, 0.4150, 0.3587,  ..., 0.3251, 0.3438, 0.3722]],\n",
       " \n",
       "         [[0.5245, 0.5165, 0.4843,  ..., 0.4914, 0.5154, 0.5413],\n",
       "          [0.4207, 0.4450, 0.4658,  ..., 0.4727, 0.4564, 0.4495],\n",
       "          [0.4283, 0.4178, 0.4531,  ..., 0.4929, 0.4605, 0.4267]]],\n",
       "        grad_fn=<MulBackward0>)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_unsup_teacher_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2598, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uda_supervised_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = outputs_unsup_teacher[\"probas\"].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guille/VirtEnv/DeepLearning/lib/python3.8/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "    # Temperature on unsupervised for UDA\n",
    "    outputs_unsup_teacher[\"logits\"] = outputs_unsup_teacher[\"logits\"]/config.get(\"UDA_temp\",0.7)\n",
    "    outputs_unsup_teacher[\"probas\"] = torch.sigmoid(outputs_unsup_teacher[\"logits\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5218, 0.5179, 0.5046,  ..., 0.5064, 0.5045, 0.5128],\n",
       "         [0.5520, 0.5527, 0.5322,  ..., 0.5676, 0.5411, 0.5445],\n",
       "         [0.4818, 0.4912, 0.4943,  ..., 0.4907, 0.4946, 0.4925]],\n",
       "\n",
       "        [[0.5140, 0.5148, 0.5087,  ..., 0.5228, 0.5107, 0.5061],\n",
       "         [0.5593, 0.5571, 0.5471,  ..., 0.5613, 0.5537, 0.5421],\n",
       "         [0.4828, 0.4924, 0.4896,  ..., 0.4848, 0.4934, 0.4921]],\n",
       "\n",
       "        [[0.5104, 0.5295, 0.5084,  ..., 0.5249, 0.5211, 0.5147],\n",
       "         [0.5489, 0.5667, 0.5495,  ..., 0.5593, 0.5507, 0.5585],\n",
       "         [0.4872, 0.4854, 0.4803,  ..., 0.4842, 0.4919, 0.4910]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5066, 0.5197, 0.5123,  ..., 0.5215, 0.5231, 0.5143],\n",
       "         [0.5526, 0.5505, 0.5535,  ..., 0.5580, 0.5580, 0.5511],\n",
       "         [0.4875, 0.4867, 0.4860,  ..., 0.4874, 0.4847, 0.4874]],\n",
       "\n",
       "        [[0.5195, 0.5196, 0.5184,  ..., 0.4377, 0.5231, 0.5185],\n",
       "         [0.5537, 0.5532, 0.5568,  ..., 0.5181, 0.5649, 0.5181],\n",
       "         [0.4734, 0.4847, 0.4819,  ..., 0.4026, 0.4632, 0.4559]],\n",
       "\n",
       "        [[0.5037, 0.5213, 0.5149,  ..., 0.5117, 0.5199, 0.5172],\n",
       "         [0.5486, 0.5606, 0.5637,  ..., 0.5663, 0.5548, 0.5547],\n",
       "         [0.4884, 0.4919, 0.4891,  ..., 0.4910, 0.4868, 0.4886]]],\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_aug_student[\"probas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ajghakjsghkjgh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f90334661f58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0majghakjsghkjgh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ajghakjsghkjgh' is not defined"
     ]
    }
   ],
   "source": [
    "ajghakjsghkjgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Train) Epoch   1/100, Loss inf: 0it [00:01, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'agkhkjasghkjas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-3bbcb625eec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0magkhkjasghkjas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Apply data transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'data_pre'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agkhkjasghkjas' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimum do_epoch example\n",
    "1. Select device to send tensors\n",
    "2. Initialize loss function\n",
    "3. Predict + optimize batch\n",
    "4. Save loss per batch (useful given size of dataset)\n",
    "\"\"\"\n",
    "\n",
    "# Record progress\n",
    "max_length = max(len(dataloader_labeled),len(dataloader_unlabeled))\n",
    "batch_loss = np.zeros((max_length,),dtype='float16')\n",
    "generator  = src.train.get_batch_ssl(dataloader_labeled,dataloader_unlabeled)\n",
    "\n",
    "# Create transforms\n",
    "if ('data_pre' in config):\n",
    "    data_pre     = sak.from_dict(config[\"data_pre\"])\n",
    "if ('augmentation' in config) and model_student.training:\n",
    "    augmentation = sak.from_dict(config[\"augmentation\"])\n",
    "if ('data_post' in config):\n",
    "    data_post    = sak.from_dict(config[\"data_post\"])\n",
    "\n",
    "# Select iterator decorator\n",
    "train_type = 'Train' if model_student.training else 'Valid'\n",
    "iterator = sak.get_tqdm(generator, config.get('iterator',''), \n",
    "                        desc=\"({}) Epoch {:>3d}/{:>3d}, Loss {:0.3f}\".format(train_type, \n",
    "                                                                             state['epoch']+1, \n",
    "                                                                             config['epochs'], np.inf))\n",
    "\n",
    "# Iterate over all data in train/validation/test dataloader:\n",
    "print_loss = np.inf\n",
    "for i,inputs in enumerate(iterator):\n",
    "    agkhkjasghkjas\n",
    "    # Apply data transforms\n",
    "    if ('data_pre' in config):\n",
    "        data_pre(inputs=inputs)\n",
    "    if ('augmentation' in config) and model_student.training:\n",
    "        augmentation(inputs=inputs)\n",
    "    if ('data_post' in config):\n",
    "        data_post(inputs=inputs)\n",
    "        \n",
    "    hakjghakjghj\n",
    "\n",
    "    # Map all inputs to device\n",
    "    for k in inputs:\n",
    "        inputs[k] = inputs[k].to(state['device'], non_blocking=True)\n",
    "\n",
    "    # Set gradient to zero\n",
    "    if model.training: \n",
    "        state['optimizer'].zero_grad()\n",
    "\n",
    "    # Predict input data\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = criterion(inputs=inputs,outputs=outputs,state=state)\n",
    "\n",
    "    # Break early\n",
    "    if torch.isnan(loss):\n",
    "        raise ValueError(\"Nan loss value encountered. Stopping...\")\n",
    "\n",
    "    # Retrieve for printing purposes\n",
    "    print_loss = loss.item()\n",
    "\n",
    "    # Optimize network's weights\n",
    "    if model.training:\n",
    "        loss.backward()\n",
    "        state['optimizer'].step()\n",
    "\n",
    "    # Accumulate losses\n",
    "    batch_loss[i] = print_loss\n",
    "\n",
    "    # Change iterator description\n",
    "    if isinstance(iterator,tqdm.tqdm):\n",
    "        if i == len(iterator)-1: batch_print = np.mean(batch_loss)\n",
    "        else:                    batch_print = print_loss\n",
    "        iterator.set_description(\"({}) Epoch {:>3d}/{:>3d}, Loss {:10.3f}\".format(train_type, state['epoch']+1, config['epochs'], batch_print))\n",
    "        iterator.refresh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_batch_ssl at 0x7fbcd2aa35f0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch_ssl(model: torch.nn.Module, state: dict, config: dict, \n",
    "                 dataloader_labeled:   torch.utils.data.DataLoader, \n",
    "                 dataloader_unlabeled: torch.utils.data.DataLoader, \n",
    "                 criterion: Callable, \n",
    "                 criterion_crossentropy: Callable) -> list:\n",
    "    \"\"\"\n",
    "    Minimum do_epoch example\n",
    "    1. Select device to send tensors\n",
    "    2. Initialize loss function\n",
    "    3. Predict + optimize batch\n",
    "    4. Save loss per batch (useful given size of dataset)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Record progress\n",
    "    max_length = max(len(dataloader_labeled),len(dataloader_unlabeled))\n",
    "    batch_loss = np.zeros((max_length,),dtype='float16')\n",
    "\n",
    "    # Create transforms\n",
    "    if ('data_pre' in config):\n",
    "        data_pre     = sak.from_dict(config[\"data_pre\"])\n",
    "    if ('augmentation' in config) and model.training:\n",
    "        augmentation = sak.from_dict(config[\"augmentation\"])\n",
    "    if ('data_post' in config):\n",
    "        data_post    = sak.from_dict(config[\"data_post\"])\n",
    "\n",
    "    # Select iterator decorator\n",
    "    train_type = 'Train' if model.training else 'Valid'\n",
    "    iterator = sak.get_tqdm(dataloader, config.get('iterator',''), \n",
    "                            desc=\"({}) Epoch {:>3d}/{:>3d}, Loss {:0.3f}\".format(train_type, \n",
    "                                                                                 state['epoch']+1, \n",
    "                                                                                 config['epochs'], np.inf))\n",
    "\n",
    "    # Iterate over all data in train/validation/test dataloader:\n",
    "    print_loss = np.inf\n",
    "    for i, inputs in enumerate(iterator):\n",
    "        # Apply data transforms\n",
    "        if ('data_pre' in config):\n",
    "            data_pre(inputs=inputs)\n",
    "        if ('augmentation' in config) and model.training:\n",
    "            augmentation(inputs=inputs)\n",
    "        if ('data_post' in config):\n",
    "            data_post(inputs=inputs)\n",
    "\n",
    "        # Map all inputs to device\n",
    "        for k in inputs:\n",
    "            inputs[k] = inputs[k].to(state['device'], non_blocking=True)\n",
    "\n",
    "        # Set gradient to zero\n",
    "        if model.training: \n",
    "            state['optimizer'].zero_grad()\n",
    "\n",
    "        # Predict input data\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(inputs=inputs,outputs=outputs,state=state)\n",
    "\n",
    "        # Break early\n",
    "        if torch.isnan(loss):\n",
    "            raise ValueError(\"Nan loss value encountered. Stopping...\")\n",
    "\n",
    "        # Retrieve for printing purposes\n",
    "        print_loss = loss.item()\n",
    "        \n",
    "        # Optimize network's weights\n",
    "        if model.training:\n",
    "            loss.backward()\n",
    "            state['optimizer'].step()\n",
    "\n",
    "        # Accumulate losses\n",
    "        batch_loss[i] = print_loss\n",
    "\n",
    "        # Change iterator description\n",
    "        if isinstance(iterator,tqdm.tqdm):\n",
    "            if i == len(iterator)-1: batch_print = np.mean(batch_loss)\n",
    "            else:                    batch_print = print_loss\n",
    "            iterator.set_description(\"({}) Epoch {:>3d}/{:>3d}, Loss {:10.3f}\".format(train_type, state['epoch']+1, config['epochs'], batch_print))\n",
    "            iterator.refresh()\n",
    "\n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "def train_model_ssl(model, state: dict, config: dict, loader: torch.utils.data.DataLoader):\n",
    "    # Send model to device\n",
    "    model = model.to(state['device'])\n",
    "\n",
    "    # Instantiate criterion\n",
    "    criterion = sak.from_dict(config['loss'])\n",
    "    \n",
    "    # Initialize best loss for early stopping\n",
    "    if 'best_loss' not in state:\n",
    "        state['best_loss'] = np.inf\n",
    "\n",
    "    # Get savedir string\n",
    "    if \"savedir\" in config:          str_savedir = 'savedir'\n",
    "    elif \"save_directory\" in config: str_savedir = 'save_directory'\n",
    "    else: raise ValueError(\"Configuration file should include either the 'savedir' or 'save_directory' fields [case-sensitive]\")\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(state['epoch'], config['epochs']):\n",
    "        try:\n",
    "            # Store current epoch\n",
    "            state['epoch'] = epoch\n",
    "            \n",
    "            # Train model\n",
    "            loss_train = do_epoch(model.train(), state, config, loader, criterion)\n",
    "            state['loss_train'] = np.mean(loss_train)\n",
    "\n",
    "            # Save model/state info\n",
    "            model = model.cpu().eval()\n",
    "            torch.save(model,              os.path.join(config[str_savedir],'checkpoint.model'),      pickle_module=dill)\n",
    "            torch.save(model.state_dict(), os.path.join(config[str_savedir],'checkpoint.state_dict'), pickle_module=dill)\n",
    "            sak.pickledump(state, os.path.join(config[str_savedir],'checkpoint.state'), mode='wb')\n",
    "            model = model.to(state['device'])\n",
    "            \n",
    "            # Log train loss\n",
    "            with open(os.path.join(config[str_savedir],'log.csv'),'a') as f:\n",
    "                csvwriter = csv.writer(f)\n",
    "                csvwriter.writerow([\"(Train) Epoch {:>3d}/{:>3d}, Loss {:10.3f}, Time {}\".format(state['epoch']+1, config['epochs'], state['loss_train'], time.ctime())])\n",
    "\n",
    "            # Check if loss is best loss\n",
    "            if state['loss_train'] < state['best_loss']:\n",
    "                state['best_loss'] = state['loss_train']\n",
    "                state['best_epoch'] = epoch\n",
    "                \n",
    "                # Copy checkpoint and mark as best\n",
    "                shutil.copyfile(os.path.join(config[str_savedir],'checkpoint.model'), os.path.join(config[str_savedir],'model_best.model'))\n",
    "                shutil.copyfile(os.path.join(config[str_savedir],'checkpoint.state'), os.path.join(config[str_savedir],'model_best.state'))\n",
    "        except KeyboardInterrupt:\n",
    "            model = model.cpu().eval()\n",
    "            torch.save(model,              os.path.join(config[str_savedir],'keyboard_interrupt.model'),      pickle_module=dill)\n",
    "            torch.save(model.state_dict(), os.path.join(config[str_savedir],'keyboard_interrupt.state_dict'), pickle_module=dill)\n",
    "            sak.pickledump(state, os.path.join(config[str_savedir],'keyboard_interrupt.state'), mode='wb')\n",
    "            raise\n",
    "        except:\n",
    "            model = model.cpu().eval()\n",
    "            torch.save(model,              os.path.join(config[str_savedir],'error.model'),      pickle_module=dill)\n",
    "            torch.save(model.state_dict(), os.path.join(config[str_savedir],'error.state_dict'), pickle_module=dill)\n",
    "            sak.pickledump(state, os.path.join(config[str_savedir],'error.state'), mode='wb')\n",
    "            raise\n",
    "\n",
    "\n",
    "def train_valid_model_ssl(model, state: dict, config: dict, \n",
    "                          loader_train: torch.utils.data.DataLoader, \n",
    "                          loader_valid: torch.utils.data.DataLoader):\n",
    "    # Send model to device\n",
    "    model = model.to(state['device'])\n",
    "\n",
    "    # Instantiate criterion\n",
    "    criterion = sak.from_dict(config['loss'])\n",
    "    \n",
    "    # Initialize best loss for early stopping\n",
    "    if 'best_loss' not in state:\n",
    "        state['best_loss'] = np.inf\n",
    "\n",
    "    # Get savedir string\n",
    "    if \"savedir\" in config:          str_savedir = 'savedir'\n",
    "    elif \"save_directory\" in config: str_savedir = 'save_directory'\n",
    "    else: raise ValueError(\"Configuration file should include either the 'savedir' or 'save_directory' fields [case-sensitive]\")\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(state['epoch'], config['epochs']):\n",
    "        try:\n",
    "            # Store current epoch\n",
    "            state['epoch'] = epoch\n",
    "            \n",
    "            # Training model\n",
    "            loss_train = do_epoch(model.train(), state, config, loader_train, criterion)\n",
    "            state['loss_train'] = np.mean(loss_train)\n",
    "\n",
    "            # Validate results\n",
    "            with torch.no_grad():\n",
    "                loss_valid = do_epoch(model.eval(), state, config, loader_valid, criterion)\n",
    "            state['loss_validation'] = np.mean(loss_valid)\n",
    "\n",
    "            # Update learning rate scheduler\n",
    "            if 'scheduler' in state:\n",
    "                state['scheduler'].step(state['loss_validation'])\n",
    "\n",
    "            # Save model/state info\n",
    "            model = model.cpu().eval()\n",
    "            torch.save(model,              os.path.join(config[str_savedir],'checkpoint.model'),      pickle_module=dill)\n",
    "            torch.save(model.state_dict(), os.path.join(config[str_savedir],'checkpoint.state_dict'), pickle_module=dill)\n",
    "            sak.pickledump(state, os.path.join(config[str_savedir],'checkpoint.state'), mode='wb')\n",
    "            model = model.to(state['device'])\n",
    "            \n",
    "            # Log train/valid losses\n",
    "            with open(os.path.join(config[str_savedir],'log.csv'),'a') as f:\n",
    "                csvwriter = csv.writer(f)\n",
    "                csvwriter.writerow([\"(Train) Epoch {:>3d}/{:>3d}, Loss {:10.3f}, Time {}\".format(state['epoch']+1, config['epochs'], state['loss_train'], time.ctime())])\n",
    "                csvwriter.writerow([\"(Valid) Epoch {:>3d}/{:>3d}, Loss {:10.3f}, Time {}\".format(state['epoch']+1, config['epochs'], state['loss_validation'], time.ctime())])\n",
    "\n",
    "            # Check if loss is best loss\n",
    "            compound_loss = 2*state['loss_train']*state['loss_validation']/(state['loss_train']+state['loss_validation'])\n",
    "            if compound_loss < state['best_loss']:\n",
    "                state['best_loss'] = compound_loss\n",
    "                state['best_epoch'] = epoch\n",
    "                \n",
    "                # Copy checkpoint and mark as best\n",
    "                shutil.copyfile(os.path.join(config[str_savedir],'checkpoint.model'), os.path.join(config[str_savedir],'model_best.model'))\n",
    "                shutil.copyfile(os.path.join(config[str_savedir],'checkpoint.state'), os.path.join(config[str_savedir],'model_best.state'))\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            model = model.cpu().eval()\n",
    "            torch.save(model,              os.path.join(config[str_savedir],'keyboard_interrupt.model'),      pickle_module=dill)\n",
    "            torch.save(model.state_dict(), os.path.join(config[str_savedir],'keyboard_interrupt.state_dict'), pickle_module=dill)\n",
    "            sak.pickledump(state, os.path.join(config[str_savedir],'keyboard_interrupt.state'), mode='wb')\n",
    "            raise\n",
    "        except:\n",
    "            model = model.cpu().eval()\n",
    "            torch.save(model,              os.path.join(config[str_savedir],'error.model'),      pickle_module=dill)\n",
    "            torch.save(model.state_dict(), os.path.join(config[str_savedir],'error.state_dict'), pickle_module=dill)\n",
    "            sak.pickledump(state, os.path.join(config[str_savedir],'error.state'), mode='wb')\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O FILES\n",
    "\n",
    "    aaa = pd.DataFrame(np.random.rand(10000,10000))\n",
    "    \n",
    "    ~~~ FEATHER ~~~\n",
    "    767M\ttest.fth\n",
    "    299 ms ± 21 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "    ~~~ HDF5 ~~~\n",
    "    764M\ttest.hdf5\n",
    "    451 ms ± 92.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "    ~~~ CSV ~~~\n",
    "    1,8G\ttest.csv\n",
    "    29 s ± 291 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "    ~~~ WFDN ~~~\n",
    "    382M\ttest.dat\n",
    "    732K\ttest.hea\n",
    "    3.96 s ± 198 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "    ~~~ FLEET ~~~\n",
    "    771M\ttest.fleet # With pyarrow\n",
    "    765M\ttest.fleet # With pickle protocol 5\n",
    "    # 315 ms ± 10.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) # With pyarrow\n",
    "    161 ms ± 446 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)  # With pickle protocol 5\n",
    "\n",
    "    https://towardsdatascience.com/what-format-should-i-store-my-ecg-data-in-for-dl-training-bc808eb64981"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing numpy content to a new Fleet file.\n",
      "Done.\n",
      "766M\ttest.fleet\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import fleetfmt\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage\n",
    "import skimage.util\n",
    "\n",
    "aaa = np.random.rand(10000,10000)\n",
    "print(\"Writing numpy content to a new Fleet file.\")\n",
    "with pathlib.Path(\"test.fleet\").open('wb') as fhandle, fleetfmt.FileWriter(fhandle) as writer:\n",
    "    for i,value in enumerate(aaa):\n",
    "        writer.append(f\"jakhgkjahgkjhak {i}\", value)\n",
    "print(\"Done.\")\n",
    "!du -sh test.fleet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 ms ± 381 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with pathlib.Path(\"test.fleet\").open('rb') as fhandle, fleetfmt.FileReader(fhandle) as reader:\n",
    "    # get keys\n",
    "    dkeys = list(reader.keys())\n",
    "\n",
    "    # get values\n",
    "    for key in dkeys:\n",
    "        value = reader.read(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279 ns ± 1.02 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n",
      "18.4 µs ± 27.2 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "with pathlib.Path(\"test.fleet\").open('rb') as fhandle, fleetfmt.FileReader(fhandle) as reader:\n",
    "    # get keys\n",
    "    dkeys = list(reader.keys())\n",
    "\n",
    "    # get random value\n",
    "    %timeit key = random.choice(dkeys)\n",
    "    %timeit key = random.choice(dkeys); reader.read(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST I/O FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing numpy content to a new Fleet file.\n",
      "MUSE dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10646/10646 [01:45<00:00, 100.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brugada dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [01:24<00:00, 12.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallot dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1941/1941 [00:17<00:00, 112.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HCM dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:00<00:00, 171.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenge dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41704/41704 [05:48<00:00, 119.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongQT dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 807/807 [00:07<00:00, 109.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THEW dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1700/1700 [2:58:24<00:00,  6.30s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepFake dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150000/150000 [12:54<00:00, 193.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoO dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [00:30<00:00, 17.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDB dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:27<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVDB dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictAF dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [06:30<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zhejiang dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 336/336 [00:08<00:00, 39.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUVR_CARTO dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20618/20618 [37:29<00:00,  9.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "116G\t/media/guille/DADES/DADES/ECG/unsupervised.fleet\n"
     ]
    }
   ],
   "source": [
    "import ishneholterlib\n",
    "import struct\n",
    "\n",
    "window = 2048\n",
    "max_size = window*10\n",
    "window_step = max_size-window//4\n",
    "target_fs = 250\n",
    "target_dtype = \"float32\"\n",
    "\n",
    "print(\"Writing numpy content to a new Fleet file.\")\n",
    "with pathlib.Path(f\"/media/guille/DADES/DADES/ECG/unsupervised_{target_dtype}.fleet\").open('wb') as fhandle, fleetfmt.FileWriter(fhandle) as writer:\n",
    "    print(\"MUSE dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/ECGData/*.csv\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            tmp = pd.read_csv(file)\n",
    "            tmp = tmp.round(3)\n",
    "            tmp.columns = map(lambda x: str(x).upper(), tmp.columns)\n",
    "            fs = 500\n",
    "            hea = list(tmp.columns)\n",
    "            tmp = sak.signal.interpolate.interp1d(tmp.values,target_fs*tmp.shape[0]//fs,axis=0).T\n",
    "            for k,value in zip(hea,tmp): \n",
    "                if value.size < window: continue\n",
    "                writer.append(f\"MUSE/{fname[5:]}/{k}###0\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Brugada dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/Brugada/Databases/HUVR/*.ecg\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg = ishneholterlib.Holter(file)\n",
    "            ecg.load_data()\n",
    "            fs = ecg.sr\n",
    "            for lead in ecg.lead:\n",
    "                k = lead.spec_str().upper()\n",
    "                data = lead.data.copy()\n",
    "                data = sak.signal.interpolate.interp1d(data,target_fs*data.size//fs)\n",
    "                if data.size > max_size:\n",
    "                    data = skimage.util.view_as_windows(data,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    data = data[None,]\n",
    "                for i,value in enumerate(data):\n",
    "                    if i == 0:\n",
    "                        continue\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"Brugada/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Fallot dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/Fallot/ECGs/*.csv\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            tmp = pd.read_csv(file)\n",
    "            tmp = tmp.round(3)\n",
    "            tmp.columns = map(lambda x: str(x).upper(), tmp.columns)\n",
    "            hea = map(lambda x: str(x).upper(), tmp.columns)\n",
    "            fs = 1/np.unique(np.round(np.diff(tmp[\"TIME IN SEC.\"].values),5))\n",
    "            assert fs.size == 1, \"check fs!!!\"\n",
    "            fs = int(fs[0])\n",
    "            hea = list(tmp.columns)\n",
    "            tmp = sak.signal.interpolate.interp1d(tmp.values,target_fs*tmp.shape[0]//fs,axis=0).T\n",
    "            for k,value in zip(hea,tmp): \n",
    "                if k == \"TIME IN SEC.\": continue\n",
    "                if value.size < window: continue\n",
    "                writer.append(f\"Fallot/{fname}/{k}###0\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"HCM dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/HCMData/CSV/*.csv\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            tmp = pd.read_csv(file,header=None)[:-500]/1000\n",
    "            tmp.columns = map(lambda x: str(x).upper(), StandardHeader)\n",
    "            fs = 500\n",
    "            tmp = sak.signal.interpolate.interp1d(tmp.values,target_fs*tmp.shape[0]//fs,axis=0).T\n",
    "            for k,value in zip(StandardHeader,tmp): \n",
    "                if value.size < window: continue\n",
    "                writer.append(f\"HCM/{fname}/{k}###0\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Challenge dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/Challenge/*.mat\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            sig,hea = wfdb.rdsamp(os.path.join(root,fname),return_res=16)\n",
    "            hea[\"sig_name\"] = list(map(lambda x: str(x).upper(), hea[\"sig_name\"]))\n",
    "            fs = hea[\"fs\"]\n",
    "            sig = sak.signal.interpolate.interp1d(sig,target_fs*sig.shape[0]//fs,axis=0).T\n",
    "            for k,lead in zip(hea[\"sig_name\"],sig):\n",
    "                if lead.size > max_size:\n",
    "                    data = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    data = lead[None,]\n",
    "                for i,value in enumerate(data):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"Challenge/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"LongQT dataset\")\n",
    "    time.sleep(0.5)\n",
    "    import xml.etree.ElementTree as ET\n",
    "    ET.register_namespace(\"\",\"http://www3.medical.philips.com\")\n",
    "    all_files = (glob.glob(\"/home/guille/DADES/DADES/ECG/LONG_QT/LongQT_*/*.XML\") + \n",
    "                 glob.glob(\"/home/guille/DADES/DADES/ECG/LONG_QT/LongQT_*/*/*.XML\"))\n",
    "    head = \"{http://www3.medical.philips.com}\"\n",
    "    namespace = {\"xmlns:ns0\": \"http://www3.medical.philips.com\",\n",
    "                 \"xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\",\n",
    "                 \"xsi:schemaLocation\": \"http://www3.medical.philips.com PhilipsECG.xsd\"}\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            tree = ET.parse(file)\n",
    "            if len(tree.findall(\"FullDisclosure\")) != 0:\n",
    "                continue\n",
    "            fs = float(tree.findall(\"StripData/SampleRate\")[0].text)\n",
    "            ecg = {}\n",
    "            for lead in tree.findall(\"StripData/WaveformData\"):\n",
    "                ecg[lead.get(\"lead\").upper()] = np.array(lead.text.strip().split(\",\"),dtype=int)\n",
    "            ecg = np.array([ecg[k] for k in StandardHeader])\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg,round(ecg.shape[1]*250/fs))\n",
    "            for k,lead in zip(StandardHeader,ecg):\n",
    "                if lead.size > max_size:\n",
    "                    data = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    data = lead[None,]\n",
    "                for i,value in enumerate(data):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"LongQT/{fname}/{k}###{i}\", (value/1000).astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"THEW dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/THEWProject/*/*.ecg\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            rt,stu_id = os.path.split(root)\n",
    "            ecg = ishneholterlib.Holter(file,check_valid=False)\n",
    "            ecg.load_data()\n",
    "            fs = ecg.sr\n",
    "            for lead in ecg.lead:\n",
    "                k = lead.spec_str().upper()\n",
    "                data = lead.data.copy()\n",
    "                data = sak.signal.interpolate.interp1d(data,target_fs*data.size//fs)\n",
    "                if data.size > max_size:\n",
    "                    data = skimage.util.view_as_windows(data,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    data = data[None,]\n",
    "                for i,value in enumerate(data):\n",
    "                    if value.size < window: continue\n",
    "                    if np.all(np.abs(np.diff(value)) < 1e-6): continue\n",
    "                    writer.append(f\"THEW/{stu_id}_{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"DeepFake dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/DeepFake/*.asc\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg = pd.read_csv(file,header=None,sep=\" \")\n",
    "            fs = 500\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs)\n",
    "            for k,value in zip(StandardHeader,ecg):\n",
    "                k = k.upper()\n",
    "                if value.size < window: continue\n",
    "                writer.append(f\"DeepFake/{fname}/{k}###0\", (value/1000).astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"SoO dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/Delineator/SoO/RETAG/*.txt\")\n",
    "    db = pd.read_csv('/home/guille/DADES/DADES/ECG/Delineator/SoO/DATABASE_MANUAL.csv')\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg = pd.read_csv(file,header=0,sep=\",\",index_col=0)\n",
    "            hea = list(map(lambda x: str(x).upper(), ecg.columns))\n",
    "            filt_id = (db[\"ID\"] == int(fname.split(\"-\")[0]))\n",
    "            fs = db[filt_id][\"Sampling_Freq\"].values[0]\n",
    "            ecg = np.round(sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs))/(2**15)\n",
    "            for k,lead in zip(hea,ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"SoO/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"EDB dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/PhysioNet/EDB/*.dat\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg,hea = wfdb.rdsamp(file[:-4])\n",
    "            fs = hea[\"fs\"]\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs)\n",
    "            for k,lead in zip(hea[\"sig_name\"],ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"EDB/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"SVDB dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/PhysioNet/SVDB/*.dat\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            ecg,hea = wfdb.rdsamp(file[:-4])\n",
    "            fs = hea[\"fs\"]\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs)\n",
    "            for k,lead in zip(hea[\"sig_name\"],ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"SVDB/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"PredictAF dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/PredictAF/*/*/*.NHS\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            rt,pat_id = os.path.split(root)\n",
    "            rt,stu_id = os.path.split(rt)\n",
    "            ecg,fs = src.reader.readNHS(file)\n",
    "            ecg = sak.signal.interpolate.interp1d(ecg.T,int(target_fs*ecg.shape[0]//fs)).T\n",
    "            for k,lead in zip(StandardHeader,ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if i < 5:\n",
    "                        continue\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"PredictAF/{stu_id}_{pat_id}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Zhejiang dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/ECG/RubenDoste/ZhejiangDatabase/PVCVTRawECGData/*.csv\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            if fname in [\"onsets\",\"offsets\"]: continue\n",
    "            ecg = pd.read_csv(file,header=0)\n",
    "            hea = list(ecg.columns)\n",
    "            fs = 2000\n",
    "            ecg = np.round(sak.signal.interpolate.interp1d(ecg.T,target_fs*ecg.shape[0]//fs))/(2**13)\n",
    "            for k,lead in zip(hea,ecg):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"Zhejiang/{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"HUVR_CARTO dataset\")\n",
    "    time.sleep(0.5)\n",
    "    all_files = glob.glob(\"/home/guille/DADES/DADES/EGMDelineator/Databases/CARTOEXPORT/*/*ECG_Export.txt\")\n",
    "    for file in tqdm.tqdm(all_files):\n",
    "        try:\n",
    "            root,fname,ext = sak.splitrfe(file)\n",
    "            rt,pat_id = os.path.split(root)\n",
    "            fs = 2000\n",
    "            signal,header = src.reader.Read_CARTO3_ECG(file)\n",
    "            channels = np.array(sak.map_upper([c.split(\"(\")[0].replace(\" \",\"\") for c in header[\"Channels\"]]))\n",
    "            idx_header = np.array([np.where(channels == c)[0][0] for c in StandardHeader])\n",
    "            signal = np.round(sak.signal.interpolate.interp1d(signal[idx_header],target_fs*signal.shape[1]//fs))/(2**10)\n",
    "            for k,lead in zip(StandardHeader,signal):\n",
    "                k = k.upper()\n",
    "                if lead.size > max_size:\n",
    "                    lead = skimage.util.view_as_windows(lead,(max_size,),step=window_step)\n",
    "                else:\n",
    "                    lead = lead[None,]\n",
    "                for i,value in enumerate(lead):\n",
    "                    if value.size < window: continue\n",
    "                    writer.append(f\"HUVR_CARTO/{pat_id}_{fname}/{k}###{i}\", value.astype(target_dtype))\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "time.sleep(0.5)\n",
    "print(\"Done.\")\n",
    "time.sleep(0.5)\n",
    "!du -sh /media/guille/DADES/DADES/ECG/unsupervised.fleet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422 ns ± 0.346 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n",
      "1.64 ms ± 32 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "with pathlib.Path(\"/media/guille/DADES/DADES/ECG/unsupervised_16.fleet\").open('rb') as fhandle, fleetfmt.FileReader(fhandle) as reader:\n",
    "    # get keys\n",
    "    dkeys = list(reader.keys())\n",
    "\n",
    "    # get random value\n",
    "    %timeit key = random.choice(dkeys)\n",
    "    %timeit key = random.choice(dkeys); reader.read(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MUSE': 127752,\n",
       " 'Brugada': 92364,\n",
       " 'Fallot': 23292,\n",
       " 'HCM': 1836,\n",
       " 'Challenge': 519984,\n",
       " 'LongQT': 9624,\n",
       " 'THEW': 2194152,\n",
       " 'DeepFake': 1200000,\n",
       " 'SoO': 6492,\n",
       " 'EDB': 16560,\n",
       " 'SVDB': 3588,\n",
       " 'PredictAF': 210738,\n",
       " 'Zhejiang': 4008,\n",
       " 'HUVR_CARTO': 247416}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_keys = {}\n",
    "for key in dkeys:\n",
    "    DB = key.split(\"/\")[0]\n",
    "    \n",
    "    if DB not in all_keys:\n",
    "        all_keys[DB] = 0\n",
    "        \n",
    "    all_keys[DB] += 1\n",
    "all_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
