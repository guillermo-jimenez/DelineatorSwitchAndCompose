{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import skimage\n",
    "import skimage.segmentation\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import math\n",
    "import pathlib\n",
    "import glob\n",
    "import shutil\n",
    "import uuid\n",
    "import random\n",
    "import platform\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import wfdb\n",
    "import json\n",
    "import tqdm\n",
    "import dill\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import src.data\n",
    "import sak\n",
    "import sak.wavelet\n",
    "import sak.data\n",
    "import sak.data.augmentation\n",
    "import sak.data.preprocessing\n",
    "import sak.visualization\n",
    "import sak.visualization.plot\n",
    "import sak.torch\n",
    "import sak.torch.nn\n",
    "import sak.torch.nn as nn\n",
    "import sak.torch.train\n",
    "import sak.torch.data\n",
    "import sak.torch.models\n",
    "import sak.torch.models.lego\n",
    "import sak.torch.models.variational\n",
    "import sak.torch.models.classification\n",
    "\n",
    "from sak.signal import StandardHeader\n",
    "\n",
    "def smooth(x: np.ndarray, window_size: int, conv_mode: str = \"same\"):\n",
    "    x = np.pad(np.copy(x),(window_size,window_size),\"edge\")\n",
    "    window = np.hamming(window_size)/(window_size//2)\n",
    "    x = np.convolve(x, window, mode=conv_mode)\n",
    "    x = x[window_size:-window_size]\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = './configurations/MultiScaleUNet5Levels.json' \n",
    "input_files  = './pickle/' \n",
    "model_name = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:00<00:00, 1586.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue with file sel35_0, continuing...\n",
      "Issue with file sel35_1, continuing...\n",
      "################# FOLD 1 #################\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'agkjdklsjagklsjgkl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7738e89bc91b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mloader_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampler_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexecution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loader\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mloader_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampler_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexecution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loader\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0magkjdklsjagklsjgkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;31m# Define model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agkjdklsjagklsjgkl' is not defined"
     ]
    }
   ],
   "source": [
    "##### 1. Load configuration file #####\n",
    "with open(config_file, \"r\") as f:\n",
    "    execution = json.load(f)\n",
    "\n",
    "execution[\"root_directory\"] = os.path.expanduser(execution[\"root_directory\"])\n",
    "execution[\"save_directory\"] = os.path.expanduser(execution[\"save_directory\"])\n",
    "\n",
    "##### 2. Load synthetic dataset #####\n",
    "# 2.1. Load individual segments\n",
    "P = sak.pickleload(os.path.join(input_files,\"Psignal_new.pkl\"))\n",
    "PQ = sak.pickleload(os.path.join(input_files,\"PQsignal_new.pkl\"))\n",
    "QRS = sak.pickleload(os.path.join(input_files,\"QRSsignal_new.pkl\"))\n",
    "ST = sak.pickleload(os.path.join(input_files,\"STsignal_new.pkl\"))\n",
    "T = sak.pickleload(os.path.join(input_files,\"Tsignal_new.pkl\"))\n",
    "TP = sak.pickleload(os.path.join(input_files,\"TPsignal_new.pkl\"))\n",
    "\n",
    "Pamplitudes = sak.pickleload(os.path.join(input_files,\"Pamplitudes_new.pkl\"))\n",
    "PQamplitudes = sak.pickleload(os.path.join(input_files,\"PQamplitudes_new.pkl\"))\n",
    "QRSamplitudes = sak.pickleload(os.path.join(input_files,\"QRSamplitudes_new.pkl\"))\n",
    "STamplitudes = sak.pickleload(os.path.join(input_files,\"STamplitudes_new.pkl\"))\n",
    "Tamplitudes = sak.pickleload(os.path.join(input_files,\"Tamplitudes_new.pkl\"))\n",
    "TPamplitudes = sak.pickleload(os.path.join(input_files,\"TPamplitudes_new.pkl\"))\n",
    "\n",
    "# 2.2. Get amplitude distribution\n",
    "Pdistribution   = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(Pamplitudes.values()))))\n",
    "PQdistribution  = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(PQamplitudes.values()))))\n",
    "QRSdistribution = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.hstack((np.array(list(QRSamplitudes.values())), 2-np.array(list(QRSamplitudes.values()))))))\n",
    "STdistribution  = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(STamplitudes.values()))))\n",
    "Tdistribution   = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(Tamplitudes.values()))))\n",
    "TPdistribution  = scipy.stats.lognorm(*scipy.stats.lognorm.fit(np.array(list(TPamplitudes.values()))))\n",
    "\n",
    "# 2.3. Smooth all\n",
    "window = 5\n",
    "P   = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth(  P[k],window)),metric=sak.signal.abs_max) for k in   P}\n",
    "PQ  = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth( PQ[k],window)),metric=sak.signal.abs_max) for k in  PQ}\n",
    "QRS = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth(QRS[k],window)),metric=sak.signal.abs_max) for k in QRS}\n",
    "ST  = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth( ST[k],window)),metric=sak.signal.abs_max) for k in  ST}\n",
    "T   = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth(  T[k],window)),metric=sak.signal.abs_max) for k in   T}\n",
    "TP  = {k: sak.data.ball_scaling(sak.signal.on_off_correction(smooth( TP[k],window)),metric=sak.signal.abs_max) for k in  TP}\n",
    "\n",
    "\n",
    "##### 3. Load QTDB #####\n",
    "dataset             = pd.read_csv(os.path.join(input_files,'QTDB','Dataset.csv'), index_col=0)\n",
    "dataset             = dataset.sort_index(axis=1)\n",
    "labels              = np.asarray(list(dataset)) # In case no data augmentation is applied\n",
    "description         = dataset.describe()\n",
    "group               = {k: '_'.join(k.split('_')[:-1]) for k in dataset}\n",
    "unique_ids          = list(set([k.split('_')[0] for k in dataset]))\n",
    "\n",
    "# Load validity\n",
    "validity            = sak.load_data(os.path.join(input_files,'QTDB','validity.csv'))\n",
    "\n",
    "# Load fiducials\n",
    "Pon_QTDB            = sak.load_data(os.path.join(input_files,'QTDB','PonNew.csv'))\n",
    "Poff_QTDB           = sak.load_data(os.path.join(input_files,'QTDB','PoffNew.csv'))\n",
    "QRSon_QTDB          = sak.load_data(os.path.join(input_files,'QTDB','QRSonNew.csv'))\n",
    "QRSoff_QTDB         = sak.load_data(os.path.join(input_files,'QTDB','QRSoffNew.csv'))\n",
    "Ton_QTDB            = sak.load_data(os.path.join(input_files,'QTDB','TonNew.csv'))\n",
    "Toff_QTDB           = sak.load_data(os.path.join(input_files,'QTDB','ToffNew.csv'))\n",
    "\n",
    "# Generate masks & signals\n",
    "signal_QTDB = {}\n",
    "segmentation_QTDB = {}\n",
    "for k in tqdm.tqdm(QRSon_QTDB):\n",
    "    # Check file exists and all that\n",
    "    if k not in validity:\n",
    "        print(\"Issue with file {}, continuing...\".format(k))\n",
    "        continue\n",
    "\n",
    "    # Store signal\n",
    "    signal = dataset[k][validity[k][0]:validity[k][1]].values\n",
    "    signal = sak.signal.on_off_correction(signal)\n",
    "    amplitude = np.median(sak.signal.moving_lambda(signal,200,sak.signal.abs_max))\n",
    "    signal = signal/amplitude\n",
    "    signal_QTDB[k] = signal[None,]\n",
    "\n",
    "    # Generate boolean mask\n",
    "    segmentation = np.zeros((3,dataset.shape[0]),dtype=bool)\n",
    "    if k in Pon_QTDB:\n",
    "        for on,off in zip(Pon_QTDB[k],Poff_QTDB[k]):\n",
    "            segmentation[0,on:off] = True\n",
    "    if k in QRSon_QTDB:\n",
    "        for on,off in zip(QRSon_QTDB[k],QRSoff_QTDB[k]):\n",
    "            segmentation[1,on:off] = True\n",
    "    if k in Ton_QTDB:\n",
    "        for on,off in zip(Ton_QTDB[k],Toff_QTDB[k]):\n",
    "            segmentation[2,on:off] = True\n",
    "\n",
    "    segmentation_QTDB[k] = segmentation[:,validity[k][0]:validity[k][1]]\n",
    "\n",
    "\n",
    "##### 4. Generate random splits #####\n",
    "# 4.1. Split into train and test\n",
    "all_keys_synthetic = {}\n",
    "for k in list(P) + list(PQ) + list(QRS) + list(ST) + list(T) + list(TP):\n",
    "    uid = k.split(\"###\")[0].split(\"_\")[0].split(\"-\")[0]\n",
    "    if uid not in all_keys_synthetic:\n",
    "        all_keys_synthetic[uid] = [k]\n",
    "    else:\n",
    "        all_keys_synthetic[uid].append(k)\n",
    "\n",
    "all_keys_real = {}\n",
    "for k in list(signal_QTDB) + list(segmentation_QTDB):\n",
    "    uid = k.split(\"###\")[0].split(\"_\")[0].split(\"-\")[0]\n",
    "    if uid not in all_keys_real:\n",
    "        all_keys_real[uid] = [k]\n",
    "    else:\n",
    "        all_keys_real[uid].append(k)\n",
    "\n",
    "# 4.2. Get database and file\n",
    "filenames = []\n",
    "database = []\n",
    "for k in all_keys_synthetic:\n",
    "    filenames.append(k)\n",
    "    if k.startswith(\"SOO\"):\n",
    "        database.append(0)\n",
    "    elif k.startswith(\"sel\"):\n",
    "        database.append(1)\n",
    "    else:\n",
    "        database.append(2)\n",
    "filenames = np.array(filenames)\n",
    "database = np.array(database)\n",
    "\n",
    "# Set random seed for the execution and perform train/test splitting\n",
    "random.seed(execution[\"seed\"])\n",
    "np.random.seed(execution[\"seed\"])\n",
    "torch.random.manual_seed(execution[\"seed\"])\n",
    "splitter = sklearn.model_selection.StratifiedKFold(5).split(filenames,database)\n",
    "\n",
    "##### 5. Train folds #####\n",
    "# 5.1. Define loss\n",
    "criterion = lambda X,y,y_pred,sample_weight: sak.torch.nn.DiceLoss()(y_pred, y, sample_weight=sample_weight)\n",
    "metric    = lambda X,y,y_pred,sample_weight: sak.torch.nn.DiceLoss()(y_pred, y, sample_weight=sample_weight)\n",
    "\n",
    "# 5.2. Save model-generating files\n",
    "target_path = execution[\"save_directory\"] # Store original output path for future usage\n",
    "original_length = execution[\"dataset\"][\"length\"]\n",
    "if not os.path.isdir(os.path.join(target_path,model_name)):\n",
    "    pathlib.Path(os.path.join(target_path,model_name)).mkdir(parents=True, exist_ok=True)\n",
    "# shutil.copyfile(\"./train_multi.py\",os.path.join(target_path,model_name,\"train_multi.py\"))\n",
    "# shutil.copyfile(\"./src/data.py\",os.path.join(target_path,model_name,\"data.py\"))\n",
    "# shutil.copyfile(\"./src/metrics.py\",os.path.join(target_path,model_name,\"metrics.py\"))\n",
    "# shutil.copyfile(config_file,os.path.join(target_path,model_name,os.path.split(config_file)[1]))\n",
    "\n",
    "# 5.3. Structure to save splitter\n",
    "all_folds_test = {}\n",
    "\n",
    "# 5.4. Iterate over folds\n",
    "for i,(ix_train,ix_valid) in enumerate(splitter):\n",
    "    print(\"################# FOLD {} #################\".format(i+1))\n",
    "    # Synthetic keys\n",
    "    train_keys_synthetic, valid_keys_synthetic = ([],[])\n",
    "    for k in np.array(filenames)[ix_train]: \n",
    "        train_keys_synthetic += all_keys_synthetic[k]\n",
    "    for k in np.array(filenames)[ix_valid]: \n",
    "        valid_keys_synthetic += all_keys_synthetic[k]\n",
    "\n",
    "    # Real keys\n",
    "    train_keys_real, valid_keys_real = ([],[])\n",
    "    for k in np.array(filenames)[ix_train]: \n",
    "        if k in all_keys_real: train_keys_real += all_keys_real[k]\n",
    "    for k in np.array(filenames)[ix_valid]: \n",
    "        if k in all_keys_real: valid_keys_real += all_keys_real[k]\n",
    "\n",
    "    # Avoid repetitions\n",
    "    train_keys_synthetic = list(set(train_keys_synthetic))\n",
    "    valid_keys_synthetic = list(set(valid_keys_synthetic))\n",
    "    train_keys_real = list(set(train_keys_real))\n",
    "    valid_keys_real = list(set(valid_keys_real))\n",
    "\n",
    "    # Save fold\"s validation files for later usage\n",
    "    all_folds_test[\"fold_{}\".format(i+1)] = np.array(filenames)[ix_valid]\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~ Refine synthetic set ~~~~~~~~~~~~~~~~~~~~\n",
    "    # Divide train/valid segments\n",
    "    Ptrain   = {k:   P[k] for k in   P if k in train_keys_synthetic}\n",
    "    PQtrain  = {k:  PQ[k] for k in  PQ if k in train_keys_synthetic}\n",
    "    QRStrain = {k: QRS[k] for k in QRS if k in train_keys_synthetic}\n",
    "    STtrain  = {k:  ST[k] for k in  ST if k in train_keys_synthetic}\n",
    "    Ttrain   = {k:   T[k] for k in   T if k in train_keys_synthetic}\n",
    "    TPtrain  = {k:  TP[k] for k in  TP if k in train_keys_synthetic}\n",
    "\n",
    "    Pvalid   = {k:   P[k] for k in   P if k in valid_keys_synthetic}\n",
    "    PQvalid  = {k:  PQ[k] for k in  PQ if k in valid_keys_synthetic}\n",
    "    QRSvalid = {k: QRS[k] for k in QRS if k in valid_keys_synthetic}\n",
    "    STvalid  = {k:  ST[k] for k in  ST if k in valid_keys_synthetic}\n",
    "    Tvalid   = {k:   T[k] for k in   T if k in valid_keys_synthetic}\n",
    "    TPvalid  = {k:  TP[k] for k in  TP if k in valid_keys_synthetic}\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~ Refine real set ~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    signal_QTDB_train       = {k:       signal_QTDB[k] for k in       signal_QTDB if k in train_keys_real}\n",
    "    signal_QTDB_valid       = {k:       signal_QTDB[k] for k in       signal_QTDB if k in valid_keys_real}\n",
    "    segmentation_QTDB_train = {k: segmentation_QTDB[k] for k in segmentation_QTDB if k in train_keys_real}\n",
    "    segmentation_QTDB_valid = {k: segmentation_QTDB[k] for k in segmentation_QTDB if k in valid_keys_real}\n",
    "\n",
    "\n",
    "    # Prepare folders\n",
    "    execution[\"save_directory\"] = os.path.join(target_path, model_name, \"fold_{}\".format(i+1))\n",
    "    if not os.path.isdir(execution[\"save_directory\"]):\n",
    "        pathlib.Path(execution[\"save_directory\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define synthetic datasets\n",
    "    dataset_train_synthetic = src.data.Dataset(Ptrain, QRStrain, Ttrain, PQtrain, STtrain, TPtrain, \n",
    "                                               Pdistribution, QRSdistribution, Tdistribution, PQdistribution, \n",
    "                                               STdistribution, TPdistribution, **execution[\"dataset\"])\n",
    "    execution[\"dataset\"][\"length\"] = execution[\"dataset\"][\"length\"]//4 # On synthetic data, not so useful to do intensive validation\n",
    "    dataset_valid_synthetic = src.data.Dataset(Pvalid, QRSvalid, Tvalid, PQvalid, STvalid, TPvalid, \n",
    "                                               Pdistribution, QRSdistribution, Tdistribution, PQdistribution, \n",
    "                                               STdistribution, TPdistribution, **execution[\"dataset\"])\n",
    "    execution[\"dataset\"][\"length\"] = original_length # On synthetic data, not so useful to do intensive validation\n",
    "\n",
    "    # Define real datasets\n",
    "    dataset_train_real = src.data.DatasetQTDB(signal_QTDB_train,segmentation_QTDB_train,execution[\"dataset\"][\"N\"],128)\n",
    "    dataset_valid_real = src.data.DatasetQTDB(signal_QTDB_valid,segmentation_QTDB_valid,execution[\"dataset\"][\"N\"],128)\n",
    "\n",
    "    # Define merging dataset\n",
    "    dataset_train = sak.torch.data.UniformMultiDataset((dataset_train_synthetic,dataset_train_real),[10,1],[1,10],return_weights=True)\n",
    "    sampler_train = sak.torch.data.UniformMultiSampler(dataset_train)\n",
    "    dataset_valid = sak.torch.data.UniformMultiDataset((dataset_valid_synthetic,dataset_valid_real),[10,1],[1,10],return_weights=True)\n",
    "    sampler_valid = sak.torch.data.UniformMultiSampler(dataset_valid)\n",
    "\n",
    "    # Create dataloaders\n",
    "    loader_train = torch.utils.data.DataLoader(dataset_train, sampler=sampler_train, **execution[\"loader\"])\n",
    "    loader_valid = torch.utils.data.DataLoader(dataset_valid, sampler=sampler_valid, **execution[\"loader\"])\n",
    "    agkjdklsjagklsjgkl\n",
    "    # Define model\n",
    "    model = nn.ModelGraph(execution[\"model\"]).float().cuda()\n",
    "\n",
    "    # Train model\n",
    "    state = {\n",
    "        \"epoch\"         : 0,\n",
    "        \"device\"        : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        \"optimizer\"     : sak.class_selector(execution[\"optimizer\"][\"class\"])(model.parameters(), **execution[\"optimizer\"][\"arguments\"]),\n",
    "        \"root_dir\"      : \"./\"\n",
    "    }\n",
    "    if \"scheduler\" in execution:\n",
    "        state[\"scheduler\"] = sak.class_selector(execution[\"scheduler\"][\"class\"])(state[\"optimizer\"], **execution[\"scheduler\"][\"arguments\"])\n",
    "\n",
    "    # Train model (auto-saves to same location as above)\n",
    "    sak.torch.train.train_model(model,state,execution,loader_train,loader_valid,criterion,metric,smaller=True)\n",
    "\n",
    "sak.save_data(all_folds_test,os.path.join(target_path,model_name,\"validation_files.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(x,y,z) in enumerate(tqdm.tqdm(loader_train)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1128"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65536, 6602]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler_train.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6602"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler_train.iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72622/72622 [00:00<00:00, 1575000.23it/s]\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "for i,g in tqdm.tqdm(sampler_train):\n",
    "    indices.append((i,g))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.ceil(max([len(d)/dataset_train.draws[i] for i,d in enumerate(dataset_train.datasets)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6602.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(d)/dataset_train.draws[i] for i,d in enumerate(dataset_train.datasets)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6602"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(max([len(d)/dataset_train.draws[i] for i,d in enumerate(dataset_train.datasets)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning3",
   "language": "python",
   "name": "deeplearning3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
